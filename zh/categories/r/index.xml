<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R on </title>
    <link>https://chengjun.github.io/zh/categories/r/index.xml</link>
    <description>Recent content in R on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <copyright>&amp;copy; 2016 Cheng-Jun Wang</copyright>
    <atom:link href="/zh/categories/r/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>文本挖掘基础：使用openNLP进行词性标注</title>
      <link>https://chengjun.github.io/zh/post/cn/2013-09-07-part-of-speech-analysis-with-opennlp/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chengjun.github.io/zh/post/cn/2013-09-07-part-of-speech-analysis-with-opennlp/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://upload.wikimedia.org/wikipedia/en/thumb/6/62/LF_OpenNLP_Parser.jpg/800px-LF_OpenNLP_Parser.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;使用R软件进行自然语言处理（文本挖掘）是比较方便的。其中一个比较基础的分析是采用part-of-speech tagging的思路进行词性标注。在本文中，我将简单地介绍使用R软件及其子包openNLP进行词性标注。这里的测试语料依然是英文，采用的算法主要是最大熵的方法。&lt;/p&gt;

&lt;p&gt;openNLP的发展开始回归到底层的基本功能，之前搭建起来的比较方便实用的函数被取消了，比如tagPOS命令消失了。所以，需要自己来重新写这个方程。这也不太难，根据R文档对Maxent_POS_Tag_Annotator的介绍中的例子重新组合一下，就可以得到。&lt;/p&gt;

&lt;p&gt;动词和名词的使用在文本挖掘中异常重要，单纯的名词语料可以用于进一步的文本挖掘，如我要做的是采用它们继续做主题挖掘（topic modeling）。&lt;/p&gt;

&lt;p&gt;第一步，当然是重组这个tagPOS命令。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;library(openNLP)
library(tm)
require(NLP)

# Compose the tagPOS function
tagPOS &amp;lt;-  function(text.var, pos_tag_annotator, ...) {
  s &amp;lt;- as.String(text.var)  
  ## Set up the POS annotator if missing (for parallel)
  PTA &amp;lt;- Maxent_POS_Tag_Annotator() 
  ## Need sentence and word token annotations.
  word_token_annotator &amp;lt;- Maxent_Word_Token_Annotator()
  a2 &amp;lt;- Annotation(1L, &amp;quot;sentence&amp;quot;, 1L, nchar(s))
  a2 &amp;lt;- annotate(s, word_token_annotator, a2)
  a3 &amp;lt;- annotate(s, PTA, a2) 
  ## Determine the distribution of POS tags for word tokens.
  a3w &amp;lt;- a3[a3$type == &amp;quot;word&amp;quot;]
  pos_tag &amp;lt;- unlist(lapply(a3w$features, &amp;quot;[[&amp;quot;, &amp;quot;POS&amp;quot;)) 
  ## Extract token/POS pairs (all of them): easy.
  pos_term &amp;lt;- list(term = s[a3w], tag = pos_tag)
  return (pos_term)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{:lang=&amp;ldquo;ruby&amp;rdquo;}&lt;/p&gt;

&lt;p&gt;第二步，为了保留文本序列，这里还需要一个函数。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# run tagPOS function for a list of texts
# do so to facilitate the conversion to generate corpus for topic modeling
run_pos = function(n){
  cat(&amp;quot;R is running for part-of-speech tagging&amp;quot;, n, 
      as.character(as.POSIXlt(Sys.time(), &amp;quot;Asia/Shanghai&amp;quot;)), sep = &amp;quot;\n&amp;quot;) 
  df = tagPOS(text[n])
  nn = (df$term[which(df$tag == &amp;quot;NN&amp;quot;)])
  vb = (df$term[which(df$tag == &amp;quot;VB&amp;quot;)])
  nnvb = (c(nn, vb))
  result = list(nn, vb, nnvb)
  return(result)
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{:lang=&amp;ldquo;ruby&amp;rdquo;}&lt;/p&gt;

&lt;p&gt;第三步，开始测试结果（这里用一个很短的语料），并将其转化为三个tm下的Corpus。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# test with a simple collection of text
text &amp;lt;- c(&amp;quot;I like it.&amp;quot;, &amp;quot;This is outstanding soup!&amp;quot;,  
          &amp;quot;I really must get the recipe.&amp;quot;)

# run the functions
df = lapply(c(1:3), run_pos)

data = data.frame(do.call(rbind, df))
names(data) = c(&amp;quot;nn&amp;quot;, &amp;quot;vb&amp;quot;, &amp;quot;nnvb&amp;quot;)

# make three corpus of nouns, verbs, and both of them
corpus_nn &amp;lt;- Corpus(   VectorSource( data$nn )  )  
corpus_vb &amp;lt;- Corpus(   VectorSource( data$vb )  )  
corpus_nnvb &amp;lt;- Corpus(   VectorSource( data$nnvb )  )   
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{:lang=&amp;ldquo;ruby&amp;rdquo;}&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
