<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Topic Models on Academic</title>
    <link>https://chengjunwang.com/zh/categories/topic-models/</link>
    <description>Recent content in Topic Models on Academic</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <copyright>&amp;copy; 2016 Cheng-Jun Wang</copyright>
    <lastBuildDate>Fri, 27 Sep 2013 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://chengjunwang.com/zh/categories/topic-models/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>东风夜放花千树：对宋词进行主题分析初探</title>
      <link>https://chengjunwang.com/zh/post/cn/2013-09-27-topic-modeling-of-song-peom/</link>
      <pubDate>Fri, 27 Sep 2013 00:00:00 +0000</pubDate>
      
      <guid>https://chengjunwang.com/zh/post/cn/2013-09-27-topic-modeling-of-song-peom/</guid>
      <description>邱怡轩在统计之都中展示了对宋词进行的分析（参见http://cos.name/tag/%E5%AE%8B%E8%AF%8D/），因为当时缺乏中文分词的工具，他独辟蹊径，假设宋词中任意两个相邻的汉字构成一个词语，进而找到了宋词当中的高频词。本文则尝试使用他所提供的宋词语料（http://cos.name/wp-content/uploads/2011/03/SongPoem.tar.gz），分析一下使用R进行中文分词、构建词云、高频词语聚类以及主题模型分析。
首先要载入使用的R包并读入数据。
library(Rwordseg) require(rJava) library(tm) library(slam) library(topicmodels) library(wordcloud) library(igraph) setwd(&amp;quot;D:/github/text mining/song&amp;quot;) # 更改为你的工作路径，并存放数据在此。 txt=read.csv(&amp;quot;SongPoem.csv&amp;quot;,colClasses=&amp;quot;character&amp;quot;)  {:lang=&amp;ldquo;ruby&amp;rdquo;}
然后进行对数据的操作。当然，第一步是进行中文分词，主要使用Rwordseg这个R包，其分词效果不错。分词的过程可以自动去掉标点符号。
poem_words &amp;lt;- lapply(1:length(txt$Sentence), function(i) segmentCN(txt$Sentence[i], nature = TRUE))  {:lang=&amp;ldquo;ruby&amp;rdquo;}
然后，我们将数据通过tm这个R包转化为文本-词矩阵（DocumentTermMatrix）。 wordcorpus &amp;lt;- Corpus(VectorSource(poem_words), encoding = &amp;ldquo;UTF-8&amp;rdquo;) # 组成语料库格式
Sys.setlocale(locale=&amp;quot;Chinese&amp;quot;) dtm1 &amp;lt;- DocumentTermMatrix(wordcorpus, control = list( wordLengths=c(1, Inf), # to allow long words bounds = list(global = c(5,Inf)), # each term appears in at least 5 docs removeNumbers = TRUE, # removePunctuation = list(preserve_intra_word_dashes = FALSE), weighting = weightTf, encoding = &amp;quot;UTF-8&amp;quot;) ) colnames(dtm1) findFreqTerms(dtm1, 1000) # 看一下高频词  {:lang=&amp;ldquo;ruby&amp;rdquo;}</description>
    </item>
    
    <item>
      <title>使用聚类分析为主题模型划分主题类型</title>
      <link>https://chengjunwang.com/zh/post/cn/2013-09-08-using-cluster-analysis-to-classify-topics-generated-by-topic-modeling/</link>
      <pubDate>Sun, 08 Sep 2013 00:00:00 +0000</pubDate>
      
      <guid>https://chengjunwang.com/zh/post/cn/2013-09-08-using-cluster-analysis-to-classify-topics-generated-by-topic-modeling/</guid>
      <description>使用主题模型（topic models）可以较为高效地划分文本的主题，但一个不得不面对的问题是有时候主题的划分过细，使得解读和归类成为困难。其实，聚类分析作为一个“古老”的分析方法可以较为简洁的解决这个问题。
举一个小例子，我们主要使用tm这个R包来完成文本挖掘的前期任务。在得到DocumentTermMatrix之后，可以通过计算cosine 相似度的方法来计算文本之间的不一致性（dissimilarity）。
# Using cluster analysis to classify topics generated by topic modeling # 2013 Sep 08 # Cheng-Jun Wang library(tm) library(topicmodels) require(proxy) data(acq) data(crude) m &amp;lt;- c(acq, crude) dtm &amp;lt;- DocumentTermMatrix(m) dtm &amp;lt;- removeSparseTerms(dtm, 0.8) inspect(dtm[1:5, 1:5]) # cluster analysis of documents based on DocumentTermMatrix dist_dtm &amp;lt;- dissimilarity(mtd, method = &#39;cosine&#39;) hc &amp;lt;- hclust(dist_dtm, method = &#39;ave&#39;) plot(hc, xlab = &#39;&#39;)  {:lang=&amp;ldquo;ruby&amp;rdquo;}
我在做RA的时候，面临的一个问题就是在做主体模型的时候出现的：模型拟合得到的主题数量太多。我们用下面这个例子进行简单的介绍。
# topic modeling topic_num = 50 for (k in c(topic_num)) { # k &amp;lt;- 10 SEED &amp;lt;- 2010 jss_TM &amp;lt;- list( VEM = LDA(dtm, k = k, control = list(seed = SEED)), VEM_fixed = LDA(dtm, k = k, control = list(estimate.</description>
    </item>
    
    <item>
      <title>使用R做主题模型：词语筛选和主题数量确定</title>
      <link>https://chengjunwang.com/zh/post/cn/2013-08-31-topic-modeling-with-r/</link>
      <pubDate>Sat, 31 Aug 2013 00:00:00 +0000</pubDate>
      
      <guid>https://chengjunwang.com/zh/post/cn/2013-08-31-topic-modeling-with-r/</guid>
      <description>1. 筛选单词 在数据清理（pre-processing）之后，需要对数据进行适当筛选。对数据的筛选包括至少两个步骤：
第一步，在DocumentTermMatrix中设定
使用R的topicmodels发现设定在DocumentTermMatrix里的约束条件失效，解决方法在此，其实在topicmodels的包里也粗略提及，只是用习惯了tm包的人觉得二者是无缝对接的。其实还很多差异，比如在tm里相似功能称之为TermDocumentMatrix
dtm &amp;lt;- DocumentTermMatrix(corpus, control = list(stemming = TRUE, stopwords = TRUE, wordLengths=c(4, 15), bounds = list(global = c(5,Inf)), # each term appears in at least 5 docs removeNumbers = TRUE, removePunctuation = list(preserve_intra_word_dashes = FALSE) #,encoding = &amp;quot;UTF-8&amp;quot; ) ) colnames(dtm) ## inspect all the words for errors dim(dtm)  {:lang=&amp;ldquo;ruby&amp;rdquo;}
第二步，通过tf-idf和col_sums选择高频词
这背后的逻辑在于主题模型是要对文本进行分类，频次较少的词的贡献并不大。但会显著的占用计算资源。
term_tfidf &amp;lt;-tapply(dtm$v/row_sums(dtm)[dtm$i], dtm$j, mean) * log2(nDocs(dtm)/col_sums(dtm &amp;gt; 0)) l1=term_tfidf &amp;gt;= quantile(term_tfidf, 0.</description>
    </item>
    
    <item>
      <title>使用R做主题模型：举一个处理Encoding问题的例子</title>
      <link>https://chengjunwang.com/zh/post/cn/2013-08-29-encoding-in-r-for-text-mining/</link>
      <pubDate>Thu, 29 Aug 2013 00:00:00 +0000</pubDate>
      
      <guid>https://chengjunwang.com/zh/post/cn/2013-08-29-encoding-in-r-for-text-mining/</guid>
      <description>引言 遭遇“邪恶的拉丁引号” 我遇到的问题比较复杂，因为原文里混合了latin1和UTF-8两种encoding的字形，最初我统一再读入text数据的时候采用encoding =&amp;ldquo;UTF-8&amp;rdquo;的方法，结果发现了很多奇诡的单引号和双引号错误。在生成的DocumentTermMatrix里出现了很多以引号开始或结束的terms，例如：“grandfather， “deputy with the constitution” 。用Encoding命令看一下它的原形是：
&amp;gt; Encoding(&amp;quot;“&amp;quot;) [1] &amp;quot;latin1&amp;quot;  只所以说是原形，是因为它们可以变形！&amp;rdquo;â€œ&amp;rdquo;， &amp;ldquo;â€™&amp;rdquo;， &amp;ldquo;â€\u009d&amp;rdquo;， &amp;ldquo;â€&amp;rdquo;都是它在不设定Encoding的环境下的形状。但我觉得不足以刻画我对它的厌恶，特别附图一张：
直到最后，我也没彻底搞定这些邪恶的拉丁引号，但我使用了一些tricks解决的我的问题。
1. 读入数据不设定encoding！ 因为邪恶的拉丁引号在UTF-8格式下根本就无法对付，在不设encoding方法的时候，它们现身为â€“, â€™, â€œ等形式，还可以对付。
dat1 = read.csv(&amp;quot;D:/chengjun/Crystal/Schwab_data_cleaningSep.csv&amp;quot;, header = F, sep = &amp;quot;|&amp;quot;, quote = &amp;quot;&amp;quot;, stringsAsFactors=F, fileEncoding = &amp;quot;&amp;quot;) # , encoding =&amp;quot;UTF-8&amp;quot;); dim(dat1) names(dat1) = c(&#39;name&#39;, &#39;organization&#39;, &#39;year&#39;, &#39;country&#39;, &#39;website&#39;, &#39;shortIntro&#39;, &#39;focus&#39;, &#39;geo&#39;, &#39;model&#39;, &#39;benefit&#39;, &#39;budget&#39;, &#39;revenue&#39;, &#39;recognization&#39;, &#39;background&#39;, &#39;innovation&#39;, &#39;entrepreneur&#39;)  2. 文本数据清理第一步：载入R包，选取变量 library(tm) library(topicmodels) text = dat1$entrepreneur  3.</description>
    </item>
    
  </channel>
</rss>