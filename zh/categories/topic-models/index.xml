<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Topic Models on </title>
    <link>https://chengjun.github.io/zh/categories/topic-models/index.xml</link>
    <description>Recent content in Topic Models on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <copyright>&amp;copy; 2016 Cheng-Jun Wang</copyright>
    <atom:link href="/zh/categories/topic-models/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>东风夜放花千树：对宋词进行主题分析初探</title>
      <link>https://chengjun.github.io/zh/post/cn/2013-09-27-topic-modeling-of-song-peom/</link>
      <pubDate>Fri, 27 Sep 2013 00:00:00 +0000</pubDate>
      
      <guid>https://chengjun.github.io/zh/post/cn/2013-09-27-topic-modeling-of-song-peom/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://forum.eedu.org.cn/upload/2008/02/27/90815055.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;邱怡轩在统计之都中展示了对宋词进行的分析（参见&lt;a href=&#34;http://cos.name/tag/%E5%AE%8B%E8%AF%8D/），因为当时缺乏中文分词的工具，他独辟蹊径，假设宋词中任意两个相邻的汉字构成一个词语，进而找到了宋词当中的高频词。本文则尝试使用他所提供的宋词语料（http://cos.name/wp-content/uploads/2011/03/SongPoem.tar.gz），分析一下使用R进行中文分词、构建词云、高频词语聚类以及主题模型分析。&#34; target=&#34;_blank&#34;&gt;http://cos.name/tag/%E5%AE%8B%E8%AF%8D/），因为当时缺乏中文分词的工具，他独辟蹊径，假设宋词中任意两个相邻的汉字构成一个词语，进而找到了宋词当中的高频词。本文则尝试使用他所提供的宋词语料（http://cos.name/wp-content/uploads/2011/03/SongPoem.tar.gz），分析一下使用R进行中文分词、构建词云、高频词语聚类以及主题模型分析。&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;首先要载入使用的R包并读入数据。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;library(Rwordseg)
require(rJava)
library(tm)
library(slam)
library(topicmodels)
library(wordcloud)
library(igraph)
setwd(&amp;quot;D:/github/text mining/song&amp;quot;) # 更改为你的工作路径，并存放数据在此。
txt=read.csv(&amp;quot;SongPoem.csv&amp;quot;,colClasses=&amp;quot;character&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{:lang=&amp;ldquo;ruby&amp;rdquo;}&lt;/p&gt;

&lt;p&gt;然后进行对数据的操作。当然，第一步是进行中文分词，主要使用Rwordseg这个R包，其分词效果不错。分词的过程可以自动去掉标点符号。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;poem_words &amp;lt;- lapply(1:length(txt$Sentence), function(i) segmentCN(txt$Sentence[i], nature = TRUE))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{:lang=&amp;ldquo;ruby&amp;rdquo;}&lt;/p&gt;

&lt;p&gt;然后，我们将数据通过tm这个R包转化为文本-词矩阵（DocumentTermMatrix）。
    wordcorpus &amp;lt;- Corpus(VectorSource(poem_words), encoding = &amp;ldquo;UTF-8&amp;rdquo;) # 组成语料库格式&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Sys.setlocale(locale=&amp;quot;Chinese&amp;quot;)
dtm1 &amp;lt;- DocumentTermMatrix(wordcorpus,
                          control = list(
                            wordLengths=c(1, Inf), # to allow long words
                            bounds = list(global = c(5,Inf)), # each term appears in at least 5 docs
                            removeNumbers = TRUE,
                            # removePunctuation  = list(preserve_intra_word_dashes = FALSE),
                            weighting = weightTf,
                            encoding = &amp;quot;UTF-8&amp;quot;)
)

colnames(dtm1)
findFreqTerms(dtm1, 1000) # 看一下高频词
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{:lang=&amp;ldquo;ruby&amp;rdquo;}&lt;/p&gt;

&lt;p&gt;这里需要注意的是，这里我们默认词语的长度为1到无穷大，稍后，我们可以对其长度进行修改。例如，本文中，作者对改为长度为2以上以及长度为3以上，分别得到另外两个文本-词矩阵：dtm2和dtm3。随后，我们可以在文本-词矩阵进行一系列的分析。这里，先做一个简单的词云分析。为更好展示效果，最多只列出100个词。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;m &amp;lt;- as.matrix(dtm1)
v &amp;lt;- sort(colSums(m), decreasing=TRUE)
myNames &amp;lt;- names(v)
d &amp;lt;- data.frame(word=myNames, freq=v)
par(mar = rep(2, 4))

png(paste(getwd(), &amp;quot;/wordcloud50_&amp;quot;,  &amp;quot;.png&amp;quot;, sep = &#39;&#39;),
    width=10, height=10,
    units=&amp;quot;in&amp;quot;, res=700)

pal2 &amp;lt;- brewer.pal(8,&amp;quot;Dark2&amp;quot;)
wordcloud(d$word,d$freq, scale=c(5,.2), min.freq=mean(d$freq),
          max.words=100, random.order=FALSE, rot.per=.15, colors=pal2)
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{:lang=&amp;ldquo;ruby&amp;rdquo;}&lt;/p&gt;

&lt;p&gt;我们可以看一下效果，如下图所示，主要是一个字长度的词。最多的是“人”和“不”， 然后是“春”和“花”。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm6.staticflickr.com/5480/9953518693_004590d6e3_z.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;但我们也许对长度大于2的词以及长度大于3的词更感兴趣（同时这样也可以和邱怡轩做的结果做一下比较）。使用前面步骤中生成的dtm2重复构建词云的R程序，我们可以得到以下两个词云：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm3.staticflickr.com/2821/9953413634_9a0d9020f7_z.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;同样，对dtm3构建词云，效果如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm8.staticflickr.com/7442/9953417766_8e6f160461_z.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;以上结果和邱怡轩得到的结果类似，可见对高频词的处理方面，他的方法的确有创见。对词云分析之后，我们还可以尝试根据词与词之间共同出现的概率对词进行聚类。这里我们展示长度大于2的词的聚类结果。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dtm01 &amp;lt;- weightTfIdf(dtm2)
N = 0.9
dtm02 &amp;lt;- removeSparseTerms(dtm01, N);dtm02
# 注意，为展示方便，这里我调节N的大小，使得dtm02中的词语数量在50左右。
tdm = as.TermDocumentMatrix(dtm02)
tdm &amp;lt;- weightTfIdf(tdm)
# convert the sparse term-document matrix to a standard data frame
mydata.df &amp;lt;- as.data.frame(inspect(tdm))
mydata.df.scale &amp;lt;- scale(mydata.df)
d &amp;lt;- dist(mydata.df.scale, method = &amp;quot;euclidean&amp;quot;) # distance matrix
fit &amp;lt;- hclust(d, method=&amp;quot;ward&amp;quot;)

png(paste(&amp;quot;d:/chengjun/honglou/honglou_termcluster_50_&amp;quot;, &amp;quot;.png&amp;quot;, sep = &#39;&#39;),
    width=10, height=10,
    units=&amp;quot;in&amp;quot;, res=700)
plot(fit) # display dendogram?
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{:lang=&amp;ldquo;ruby&amp;rdquo;}&lt;/p&gt;

&lt;p&gt;对长度大于2的词的聚类结果如下图所示，可见宋词的确注重“风流倜傥”，连分类都和风向有关系。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm3.staticflickr.com/2875/9953397625_abdc6e9874_z.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;当然读者还可以尝试对长度大于1词和长度大于3的词聚类，对长度大于1词聚类的效果图如下所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm6.staticflickr.com/5548/9953397345_05a8944455_z.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;长度大于3的词聚类结果如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm8.staticflickr.com/7375/9953419256_f80a00e9e8_z.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;完成这一步之后，作者还尝试对宋词进行简单的主题模型分析，首先还是从长度大于2的词开始吧。第一步是确定主题的数量。先对文本-词矩阵进行简单处理，以消除高频词被高估和低频词被低估的问题。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dtm = dtm2
term_tfidf &amp;lt;-tapply(dtm$v/row_sums(dtm)[dtm$i], dtm$j, mean) * log2(nDocs(dtm)/col_sums(dtm &amp;gt; 0))
l1=term_tfidf &amp;gt;= quantile(term_tfidf, 0.5)       # second quantile, ie. median
dtm &amp;lt;- dtm[,l1]
dtm = dtm[row_sums(dtm)&amp;gt;0, ]; dim(dtm) # 2246 6210
summary(col_sums(dtm))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{:lang=&amp;ldquo;ruby&amp;rdquo;}&lt;/p&gt;

&lt;p&gt;之后就可以正式地开始确定主题数量的R程序了。这里，笔者主要参考了朱雪宁《微博名人那些事儿》一文中的R程序（&lt;a href=&#34;http://cos.name/2013/08/something_about_weibo/）。&#34; target=&#34;_blank&#34;&gt;http://cos.name/2013/08/something_about_weibo/）。&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fold_num = 10
kv_num =  c(5, 10*c(1:5, 10))
seed_num = 2003
try_num = 1

smp&amp;lt;-function(cross=fold_num,n,seed)
{
  set.seed(seed)
  dd=list()
  aa0=sample(rep(1:cross,ceiling(n/cross))[1:n],n)
  for (i in 1:cross) dd[[i]]=(1:n)[aa0==i]
  return(dd)
}

selectK&amp;lt;-function(dtm,kv=kv_num,SEED=seed_num,cross=fold_num,sp) # change 60 to 15
{
  per_ctm=NULL
  log_ctm=NULL
  for (k in kv)
  {
    per=NULL
    loglik=NULL
    for (i in 1:try_num)  #only run for 3 replications#
    {
      cat(&amp;quot;R is running for&amp;quot;, &amp;quot;topic&amp;quot;, k, &amp;quot;fold&amp;quot;, i,
          as.character(as.POSIXlt(Sys.time(), &amp;quot;Asia/Shanghai&amp;quot;)),&amp;quot;\n&amp;quot;)
      te=sp[[i]]
      tr=setdiff(1:dtm$nrow, te) # setdiff(nrow(dtm),te)  ## fix here when restart r session

      # VEM = LDA(dtm[tr, ], k = k, control = list(seed = SEED)),
      # VEM_fixed = LDA(dtm[tr,], k = k, control = list(estimate.alpha = FALSE, seed = SEED)),

      #       CTM = CTM(dtm[tr,], k = k,
      #                 control = list(seed = SEED, var = list(tol = 10^-4), em = list(tol = 10^-3)))  
      #       
      Gibbs = LDA(dtm[tr,], k = k, method = &amp;quot;Gibbs&amp;quot;,
                  control = list(seed = SEED, burnin = 1000,thin = 100, iter = 1000))

      per=c(per,perplexity(Gibbs,newdata=dtm[te,]))
      loglik=c(loglik,logLik(Gibbs,newdata=dtm[te,]))
    }
    per_ctm=rbind(per_ctm,per)
    log_ctm=rbind(log_ctm,loglik)
  }
  return(list(perplex=per_ctm,loglik=log_ctm))
}

sp=smp(n=dtm$nrow, seed=seed_num) # n = nrow(dtm)

system.time((ctmK=selectK(dtm=dtm,kv=kv_num,SEED=seed_num,cross=fold_num,sp=sp)))

## plot the perplexity

m_per=apply(ctmK[[1]],1,mean)
m_log=apply(ctmK[[2]],1,mean)

k=c(kv_num)
df = ctmK[[1]]  # perplexity matrix
logLik = ctmK[[2]]  # perplexity matrix


write.csv(data.frame(k, df, logLik), paste(getwd(), &amp;quot;/Perplexity2_&amp;quot;,&amp;quot;gibbs5_100&amp;quot;, &amp;quot;.csv&amp;quot;, sep = &amp;quot;&amp;quot;))

# save the figure
png(paste(getwd(), &amp;quot;/Perplexity2_&amp;quot;,try_num, &amp;quot;_gibbs5_100&amp;quot;,&amp;quot;.png&amp;quot;, sep = &#39;&#39;),
    width=5, height=5,
    units=&amp;quot;in&amp;quot;, res=700)


matplot(k, df, type = c(&amp;quot;b&amp;quot;), xlab = &amp;quot;Number of topics&amp;quot;,
        ylab = &amp;quot;Perplexity&amp;quot;, pch=1:try_num,col = 1, main = &#39;&#39;)       
legend(&amp;quot;topright&amp;quot;, legend = paste(&amp;quot;fold&amp;quot;, 1:try_num), col=1, pch=1:try_num)

dev.off()

png(paste(getwd(), &amp;quot;/LogLikelihood2_&amp;quot;, &amp;quot;gibbs5_100&amp;quot;,&amp;quot;.png&amp;quot;, sep = &#39;&#39;),
    width=5, height=5,
    units=&amp;quot;in&amp;quot;, res=700)
matplot(k, logLik, type = c(&amp;quot;b&amp;quot;), xlab = &amp;quot;Number of topics&amp;quot;,
        ylab = &amp;quot;Log-Likelihood&amp;quot;, pch=1:try_num,col = 1, main = &#39;&#39;)       
legend(&amp;quot;topright&amp;quot;, legend = paste(&amp;quot;fold&amp;quot;, 1:try_num), col=1, pch=1:try_num)
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{:lang=&amp;ldquo;ruby&amp;rdquo;}&lt;/p&gt;

&lt;p&gt;于是可以得到对数似然率和主题数量的关系图，如下所示。可见选择10作为主题数量是比较合适的。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm4.staticflickr.com/3727/9953452316_6518296960.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;以下，我们将主要对主题数量为10的主题模型进行估计。topicmodels这个R包是由Bettina Grun和
Johannes Kepler两个人贡献的，目前支持VEM, VEM (fixed alpha)，Gibbs和CTM四种主题模型，关于其详细介绍，可以阅读他们的论文，关于主题模型的更多背景知识可以阅读Blei的相关文章。闲话少叙，看一下R代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# &#39;Refer to http://cos.name/2013/08/something_about_weibo/&#39;
k = 10
SEED &amp;lt;- 2003
jss_TM2 &amp;lt;- list(
  VEM = LDA(dtm, k = k, control = list(seed = SEED)),
  VEM_fixed = LDA(dtm, k = k, control = list(estimate.alpha = FALSE, seed = SEED)),
  Gibbs = LDA(dtm, k = k, method = &amp;quot;Gibbs&amp;quot;,
              control = list(seed = SEED, burnin = 1000, thin = 100, iter = 1000)),
  CTM = CTM(dtm, k = k,
            control = list(seed = SEED, var = list(tol = 10^-4), em = list(tol = 10^-3))) )   
save(jss_TM2, file = paste(getwd(), &amp;quot;/jss_TM2.Rdata&amp;quot;, sep = &amp;quot;&amp;quot;))
save(jss_TM, file = paste(getwd(), &amp;quot;/jss_TM1.Rdata&amp;quot;, sep = &amp;quot;&amp;quot;))

termsForSave1&amp;lt;- terms(jss_TM2[[&amp;quot;VEM&amp;quot;]], 10)
termsForSave2&amp;lt;- terms(jss_TM2[[&amp;quot;VEM_fixed&amp;quot;]], 10)
termsForSave3&amp;lt;- terms(jss_TM2[[&amp;quot;Gibbs&amp;quot;]], 10)
termsForSave4&amp;lt;- terms(jss_TM2[[&amp;quot;CTM&amp;quot;]], 10)

write.csv(as.data.frame(t(termsForSave1)),
          paste(getwd(), &amp;quot;/topic-document_&amp;quot;, &amp;quot;_VEM_&amp;quot;, k, &amp;quot;_2.csv&amp;quot;, sep=&amp;quot;&amp;quot;),
          fileEncoding = &amp;quot;UTF-8&amp;quot;)

write.csv(as.data.frame(t(termsForSave2)),
          paste(getwd(), &amp;quot;/topic-document_&amp;quot;, &amp;quot;_VEM_fixed_&amp;quot;, k, &amp;quot;_2.csv&amp;quot;, sep=&amp;quot;&amp;quot;),
          fileEncoding = &amp;quot;UTF-8&amp;quot;)

write.csv(as.data.frame(t(termsForSave3)),
          paste(getwd(), &amp;quot;/topic-document_&amp;quot;, &amp;quot;_Gibbs_&amp;quot;, k, &amp;quot;_2.csv&amp;quot;, sep=&amp;quot;&amp;quot;),
          fileEncoding = &amp;quot;UTF-8&amp;quot;)
write.csv(as.data.frame(t(termsForSave4)),
          paste(getwd(), &amp;quot;/topic-document_&amp;quot;, &amp;quot;_CTM_&amp;quot;, k, &amp;quot;_2.csv&amp;quot;, sep=&amp;quot;&amp;quot;),
          fileEncoding = &amp;quot;UTF-8&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{:lang=&amp;ldquo;ruby&amp;rdquo;}&lt;/p&gt;

&lt;p&gt;对主题模型进行估计之后，一般选择展示每个主题的前10个词语。因为主题之间可以共享相同词语，所以构成网络关系，因此这里我选择用网络的方法展示其结果。首先看一下吉布斯抽样算法得到的主题网络图，其R程序如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#&#39;topic graphs&#39;


tfs = as.data.frame(termsForSave3, stringsAsFactors = F); tfs[,1]
adjacent_list = lapply(1:10, function(i) embed(tfs[,i], 2)[, 2:1])
edgelist = as.data.frame(do.call(rbind, adjacent_list), stringsAsFactors =F)
# topic = unlist(lapply(1:10, function(i) rep(i, 9)))
edgelist$topic = topic
g &amp;lt;-graph.data.frame(edgelist,directed=T )
l&amp;lt;-layout.fruchterman.reingold(g)
# edge.color=&amp;quot;black&amp;quot;
nodesize = centralization.degree(g)$res
V(g)$size = log( centralization.degree(g)$res )

nodeLabel = V(g)$name
E(g)$color =  unlist(lapply(sample(colors()[26:137], 10), function(i) rep(i, 9))); unique(E(g)$color)

# 保存图片格式
png(  paste(getwd(), &amp;quot;/topic_graph_gibbs.png&amp;quot;, sep=&amp;quot;&amp;quot;）,
    width=5, height=5,
    units=&amp;quot;in&amp;quot;, res=700)

plot(g, vertex.label= nodeLabel,  edge.curved=TRUE,
     vertex.label.cex =0.5,  edge.arrow.size=0.2, layout=l )

# 结束保存图片
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{:lang=&amp;ldquo;ruby&amp;rdquo;}&lt;/p&gt;

&lt;p&gt;得到的图形如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm8.staticflickr.com/7330/9953452256_fb1baaa16d_c.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;当然，我们还可以看一下其他算法得到的网络图，这里我们看一下根据VEM和CTM两个主题模型得到的网络图。&lt;/p&gt;

&lt;p&gt;CTM模型的主题网络图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm8.staticflickr.com/7425/9953448564_d24b8e348a_c.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;VEM模型的主题网络图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm4.staticflickr.com/3727/9953448704_ec8291a0e3_c.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>使用聚类分析为主题模型划分主题类型</title>
      <link>https://chengjun.github.io/zh/post/cn/2013-09-08-using-cluster-analysis-to-classify-topics-generated-by-topic-modeling/</link>
      <pubDate>Sun, 08 Sep 2013 00:00:00 +0000</pubDate>
      
      <guid>https://chengjun.github.io/zh/post/cn/2013-09-08-using-cluster-analysis-to-classify-topics-generated-by-topic-modeling/</guid>
      <description>&lt;p&gt;使用主题模型（topic models）可以较为高效地划分文本的主题，但一个不得不面对的问题是有时候主题的划分过细，使得解读和归类成为困难。其实，聚类分析作为一个“古老”的分析方法可以较为简洁的解决这个问题。&lt;/p&gt;

&lt;p&gt;举一个小例子，我们主要使用tm这个R包来完成文本挖掘的前期任务。在得到DocumentTermMatrix之后，可以通过计算cosine 相似度的方法来计算文本之间的不一致性（dissimilarity）。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Using cluster analysis to classify topics generated by topic modeling
# 2013 Sep 08
# Cheng-Jun Wang

library(tm)
library(topicmodels)
require(proxy)


data(acq)
data(crude)

m &amp;lt;- c(acq, crude)
dtm &amp;lt;- DocumentTermMatrix(m)
dtm &amp;lt;- removeSparseTerms(dtm, 0.8)
inspect(dtm[1:5, 1:5])

# cluster analysis of documents based on DocumentTermMatrix
dist_dtm &amp;lt;- dissimilarity(mtd, method = &#39;cosine&#39;)
hc &amp;lt;- hclust(dist_dtm, method = &#39;ave&#39;)
plot(hc, xlab = &#39;&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{:lang=&amp;ldquo;ruby&amp;rdquo;}&lt;/p&gt;

&lt;p&gt;我在做RA的时候，面临的一个问题就是在做主体模型的时候出现的：模型拟合得到的主题数量太多。我们用下面这个例子进行简单的介绍。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# topic modeling
topic_num = 50
for (k in c(topic_num))  {
  # k &amp;lt;- 10
  SEED &amp;lt;- 2010
  jss_TM &amp;lt;- list(
    VEM = LDA(dtm, k = k, control = list(seed = SEED)),
    VEM_fixed = LDA(dtm, k = k, control = list(estimate.alpha = FALSE, seed = SEED)),
    Gibbs = LDA(dtm, k = k, method = &amp;quot;Gibbs&amp;quot;,
                control = list(seed = SEED, burnin = 1000, thin = 100, iter = 1000))    )
}

rs = posterior(jss_TM$Gibbs, dtm)

mtd = t(rs$topics) # topics and documents
mtt = rs$terms  # topic and terms
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{:lang=&amp;ldquo;ruby&amp;rdquo;}&lt;/p&gt;

&lt;p&gt;使用k mean聚类方法的好处是可以较为方便地知道聚类的数量&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#########################################
#
# K means analysis for rownames of a matrix
#
#########################################


# Determine number of clusters
mydata = mtt
wss &amp;lt;- (nrow(mydata)-1)*sum(apply(mydata,2,var))
max_group = nrow(mydata)-1
for (i in 2:max_group) wss[i] &amp;lt;- sum(kmeans(mydata,
                                            centers=i)$withinss)
plot(1:max_group, wss, type=&amp;quot;b&amp;quot;, xlab=&amp;quot;Number of Clusters&amp;quot;,
     ylab=&amp;quot;Within groups sum of squares&amp;quot;)
##
# K-Means Cluster Analysis
fit &amp;lt;- kmeans(mydata, 5) # 5 cluster solution
# get cluster means
cluster_means = aggregate(mydata,by=list(fit$cluster),FUN=mean)
# append cluster assignment
mydata &amp;lt;- data.frame(rownames(mydata), fit$cluster)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{:lang=&amp;ldquo;ruby&amp;rdquo;}&lt;/p&gt;

&lt;p&gt;以下，我们对矩阵计算cosine similarity并使用阶层聚类方法得到结果。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;############################################
#
#  Hierarchical Clustering
#
############################################

cos.sim &amp;lt;- function(ix)
{
  A = X[ix[1],]
  B = X[ix[2],]
  return( sum(A*B)/sqrt(sum(A^2)*sum(B^2)) )
}   

mdt = as.matrix(dtm)
X = mtt  # whether to scale it


n &amp;lt;- nrow(X)
cmb &amp;lt;- expand.grid(i=1:n, j=1:n)
simdt &amp;lt;- matrix(apply(cmb,1,cos.sim),n,n)
rownames(simdt) = rownames(mtt)
hc &amp;lt;- hclust(dist(simdt, method = &amp;quot;euclidean&amp;quot;), method = &#39;ave&#39;)
# hc &amp;lt;- hclust(dist(simdt)^2, method = &#39;cen&#39;)

plot(hc, xlab = &#39;&#39;)
k = 10
groups &amp;lt;- cutree(hc, k=k)  # cut tree into 5 clusters
rect.hclust(hc, k=k, border=&amp;quot;red&amp;quot;) # draw dendogram with red borders
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{:lang=&amp;ldquo;ruby&amp;rdquo;}&lt;/p&gt;

&lt;p&gt;看一下效果吧：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm8.staticflickr.com/7302/9699627576_6744f02576_c.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;当然了，如果你觉得这个方法过于粗暴，还可以尝试构建主题网络并进行社区划分的方法。不再赘述。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>使用R做主题模型：词语筛选和主题数量确定</title>
      <link>https://chengjun.github.io/zh/post/cn/2013-08-31-topic-modeling-with-r/</link>
      <pubDate>Sat, 31 Aug 2013 00:00:00 +0000</pubDate>
      
      <guid>https://chengjun.github.io/zh/post/cn/2013-08-31-topic-modeling-with-r/</guid>
      <description>

&lt;h2 id=&#34;1-筛选单词&#34;&gt;1. 筛选单词&lt;/h2&gt;

&lt;p&gt;在数据清理（pre-processing）之后，需要对数据进行适当筛选。对数据的筛选包括至少两个步骤：&lt;/p&gt;

&lt;p&gt;第一步，在DocumentTermMatrix中设定&lt;/p&gt;

&lt;p&gt;使用R的topicmodels发现设定在DocumentTermMatrix里的约束条件失效，解决方法&lt;a href=&#34;http://stackoverflow.com/questions/13366897/r-documenttermmatrix-control-list-not-working&#34; target=&#34;_blank&#34;&gt;在此&lt;/a&gt;，其实在topicmodels的包里也粗略提及，只是用习惯了tm包的人觉得二者是无缝对接的。其实还很多差异，比如在tm里相似功能称之为TermDocumentMatrix&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dtm &amp;lt;- DocumentTermMatrix(corpus, control = list(stemming = TRUE,  
                                                 stopwords = TRUE,
                                                 wordLengths=c(4, 15),
                                                 bounds = list(global = c(5,Inf)), # each term appears in at least 5 docs
                                                 removeNumbers = TRUE,
                                                 removePunctuation  = list(preserve_intra_word_dashes = FALSE)
                                                 #,encoding = &amp;quot;UTF-8&amp;quot;
                                                 )
                         )


colnames(dtm)   ##  inspect all the words for errors
dim(dtm)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{:lang=&amp;ldquo;ruby&amp;rdquo;}&lt;/p&gt;

&lt;p&gt;第二步，通过tf-idf和col_sums选择高频词&lt;/p&gt;

&lt;p&gt;这背后的逻辑在于主题模型是要对文本进行分类，频次较少的词的贡献并不大。但会显著的占用计算资源。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;term_tfidf &amp;lt;-tapply(dtm$v/row_sums(dtm)[dtm$i], dtm$j, mean) * log2(nDocs(dtm)/col_sums(dtm &amp;gt; 0))

l1=term_tfidf &amp;gt;= quantile(term_tfidf, 0.1)       # fix this!
dtm &amp;lt;- dtm[,l1]
l2=col_sums(dtm) &amp;gt;= quantile(col_sums(dtm), 0.1)  # fix this!
dtm &amp;lt;- dtm[,l2]

dtm = dtm[row_sums(dtm)&amp;gt;0, ]; dim(dtm) # 2246 6210
range(col_sums(dtm))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{:lang=&amp;ldquo;ruby&amp;rdquo;}&lt;/p&gt;

&lt;h2 id=&#34;2-确定主题数量&#34;&gt;2. 确定主题数量&lt;/h2&gt;

&lt;p&gt;在对单个词进行筛选之后，就可以正式进行主题模型的设定了。这是一步非常耗时间的工作，但第一步当然是理解主题模型的基本思路。&lt;/p&gt;

&lt;p&gt;主题模型是在概率潜在语义分析（probabilistic latent semanticanalysis，PLSI）的基础上发展起来的。从对矩阵进行因子分解的角度而言，可以看做是对离散数据进行主成分分析。其具体内容可以参见Blei发表在Communication of ACM上的&lt;a href=&#34;http://cacm.acm.org/magazines/2012/4/147361-probabilistic-topic-models/fulltext&#34; target=&#34;_blank&#34;&gt;文献回顾文章&lt;/a&gt;（Blei(2012) Probalistic topic models. Communication of ACM, 55, 77-84）。&lt;/p&gt;

&lt;p&gt;简而言之，我们看到是单词在文本中的分布。1. 我们认为在单词和文本之间存在潜在的主题，并且每个主题$$β&lt;em&gt;{1:K}$$可以表达为一些词语在这个文本里的分布；2. 一篇文章可能对应多个主题，假设我们已经知道了存在哪些主题，那么一个主题在一个文本中的比例$$θ&lt;/em&gt;{d:k}$$也应该知道；3. 我们将主题和词语对应起来，建立一个映射$$z&lt;em&gt;{d:n}$$，这样我们就知道把文章d中的第n个词赋给哪个主题；4.但实际上前三步都是不能直接观察到的，之间看到的就是词语在文本中的分布$$w&lt;/em&gt;{d:n}$$ ，例如文本d当中第n个词语是什么，即词在文本中的分布。&lt;/p&gt;

&lt;p&gt;前三个隐变量和第四个先变量之间的联合分布（joint distribution）就是主题生成的过程，这个过程还可以使用概率图模型的方法进行建模。如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm4.staticflickr.com/3756/9634040055_1bdc1c8013.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;其主要逻辑是机器学习的思路：给定了可以观察到的词语在文本中的分布$$w_{d:n}$$，主题结构可以表达为一个条件分布：$$p(\beta _{1:K},\theta &lt;em&gt;{1:D},z&lt;/em&gt;{1:D} \mid w_{1:D})$$。这是后验分布的分析思路。因为可能的主题结构太多，这个后验分布无法计算出来，而主题模型的主要目的也只是逼近这个后验分布。&lt;/p&gt;

&lt;p&gt;通常逼近这个后验分布的方法可以分为两类：1. 变异算法（variational algorithms）,这是一种决定论式的方法。变异式算法假设一些参数分布，并根据这些理想中的分布与后验的数据相比较，并从中找到最接近的。由此，将一个估计问题转化为最优化问题。最主要的算法是变异式的期望最大化算法(variational expectation-maximization，VEM)。这个方法是最主要使用的方法。在R软件的tomicmodels包中被重点使用。 2. 基于抽样的算法。抽样的算法，如吉布斯抽样（gibbs sampling）主要是构造一个马尔科夫链，从后验的实证的分布中抽取一些样本，以之估计后验分布。吉布斯抽样的方法在R软件的lda包中广泛使用。&lt;/p&gt;

&lt;p&gt;常用的主题模型是LDA, 可以使用VEM和gibbs两种方法估计。之后的模型的发展，主要是要放松严格的模型假设，其中之一是允许主题之间存在相关。由此Blei等人提出了相关的主题模型（correalted topic model，CTM），可以使用VEM方法估计。在本文当中，我们采用CTM为例。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fold_num = 10
kv_num = c(5, 10*c(1:5, 10))
seed_num = 2003


smp&amp;lt;-function(cross=fold_num,n,seed)
{
  set.seed(seed)
  dd=list()
  aa0=sample(rep(1:cross,ceiling(n/cross))[1:n],n)
  for (i in 1:cross) dd[[i]]=(1:n)[aa0==i]
  return(dd)
}

selectK&amp;lt;-function(dtm,kv=kv_num,SEED=seed_num,cross=fold_num,sp) # change 60 to 15
{
  per_ctm=NULL
  log_ctm=NULL
  for (k in kv)
  {
    per=NULL
    loglik=NULL
    for (i in 1:3)  #only run for 3 replications#
    {
      cat(&amp;quot;R is running for&amp;quot;, &amp;quot;topic&amp;quot;, k, &amp;quot;fold&amp;quot;, i,
          as.character(as.POSIXlt(Sys.time(), &amp;quot;Asia/Shanghai&amp;quot;)),&amp;quot;\n&amp;quot;)
      te=sp[[i]]
      tr=setdiff(1:nrow(dtm),te)

      # VEM = LDA(dtm[tr, ], k = k, control = list(seed = SEED)),
      # VEM_fixed = LDA(dtm[tr,], k = k, control = list(estimate.alpha = FALSE, seed = SEED)),

      CTM = CTM(dtm[tr,], k = k,
                control = list(seed = SEED, var = list(tol = 10^-4), em = list(tol = 10^-3)))  

      # Gibbs = LDA(dtm[tr,], k = k, method = &amp;quot;Gibbs&amp;quot;,
      # control = list(seed = SEED, burnin = 1000,thin = 100, iter = 1000))

      per=c(per,perplexity(CTM,newdata=dtm[te,]))
      loglik=c(loglik,logLik(CTM,newdata=dtm[te,]))
    }
    per_ctm=rbind(per_ctm,per)
    log_ctm=rbind(log_ctm,loglik)
  }
  return(list(perplex=per_ctm,loglik=log_ctm))
}

sp=smp(n=nrow(dtm),seed=seed_num)

system.time((ctmK=selectK(dtm=dtm,kv=kv_num,SEED=seed_num,cross=fold_num,sp=sp)))

## plot the perplexity

m_per=apply(ctmK[[1]],1,mean)
m_log=apply(ctmK[[2]],1,mean)

k=c(kv_num)
df = ctmK[[1]]  # perplexity matrix
matplot(k, df, type = c(&amp;quot;b&amp;quot;), xlab = &amp;quot;Number of topics&amp;quot;,
        ylab = &amp;quot;Perplexity&amp;quot;, pch=1:5,col = 1, main = &#39;&#39;)       
legend(&amp;quot;bottomright&amp;quot;, legend = paste(&amp;quot;fold&amp;quot;, 1:5), col=1, pch=1:5)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{:lang=&amp;ldquo;ruby&amp;rdquo;}&lt;/p&gt;

&lt;p&gt;有趣的是计算时间：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; system.time((ctmK=selectK(dtm=dtm,kv=kv_num,SEED=seed_num,cross=fold_num,sp=sp)))
R is running for topic 5 fold 1 2013-08-31 18:26:32
R is running for topic 5 fold 2 2013-08-31 18:26:39
R is running for topic 5 fold 3 2013-08-31 18:26:45
R is running for topic 10 fold 1 2013-08-31 18:26:50
R is running for topic 10 fold 2 2013-08-31 18:27:14
R is running for topic 10 fold 3 2013-08-31 18:27:36
R is running for topic 20 fold 1 2013-08-31 18:27:57
R is running for topic 20 fold 2 2013-08-31 18:29:42
R is running for topic 20 fold 3 2013-08-31 18:32:00
R is running for topic 30 fold 1 2013-08-31 18:33:42
R is running for topic 30 fold 2 2013-08-31 18:37:39
R is running for topic 30 fold 3 2013-08-31 18:45:46
R is running for topic 40 fold 1 2013-08-31 18:52:52
R is running for topic 40 fold 2 2013-08-31 18:57:26
R is running for topic 40 fold 3 2013-08-31 19:00:31
R is running for topic 50 fold 1 2013-08-31 19:03:47
R is running for topic 50 fold 2 2013-08-31 19:04:02
R is running for topic 50 fold 3 2013-08-31 19:04:52
R is running for topic 100 fold 1 2013-08-31 19:05:42
R is running for topic 100 fold 2 2013-08-31 19:06:05
R is running for topic 100 fold 3 2013-08-31 19:06:28
   user  system elapsed
2417.801.13 2419.28
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{:lang=&amp;ldquo;ruby&amp;rdquo;}&lt;/p&gt;

&lt;p&gt;看一下最终绘制的perplexity的图，如下可见，在本例当中，当主题数量为30的时候，perplexity最小，模型的最大似然率最高，由此确定主题数量为30。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm8.staticflickr.com/7290/9633619385_13a1472d1b.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>使用R做主题模型：举一个处理Encoding问题的例子</title>
      <link>https://chengjun.github.io/zh/post/cn/2013-08-29-encoding-in-r-for-text-mining/</link>
      <pubDate>Thu, 29 Aug 2013 00:00:00 +0000</pubDate>
      
      <guid>https://chengjun.github.io/zh/post/cn/2013-08-29-encoding-in-r-for-text-mining/</guid>
      <description>

&lt;h2 id=&#34;引言&#34;&gt;引言&lt;/h2&gt;

&lt;h3 id=&#34;遭遇-邪恶的拉丁引号&#34;&gt;遭遇“邪恶的拉丁引号”&lt;/h3&gt;

&lt;p&gt;我遇到的问题比较复杂，因为原文里混合了latin1和UTF-8两种encoding的字形，最初我统一再读入text数据的时候采用encoding =&amp;ldquo;UTF-8&amp;rdquo;的方法，结果发现了很多奇诡的单引号和双引号错误。在生成的DocumentTermMatrix里出现了很多以引号开始或结束的terms，例如：“grandfather， “deputy with the constitution” 。用Encoding命令看一下它的原形是：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; Encoding(&amp;quot;“&amp;quot;)
[1] &amp;quot;latin1&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;只所以说是原形，是因为它们可以变形！&amp;rdquo;â€œ&amp;rdquo;， &amp;ldquo;â€™&amp;rdquo;， &amp;ldquo;â€\u009d&amp;rdquo;， &amp;ldquo;â€&amp;rdquo;都是它在不设定Encoding的环境下的形状。但我觉得不足以刻画我对它的厌恶，特别附图一张：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm6.staticflickr.com/5521/9621347348_c9b66db982_o.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;直到最后，我也没彻底搞定这些邪恶的拉丁引号，但我使用了一些tricks解决的我的问题。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm4.staticflickr.com/3703/9618226049_b87d57c266.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;1-读入数据不设定encoding&#34;&gt;1. 读入数据不设定encoding！&lt;/h2&gt;

&lt;p&gt;因为邪恶的拉丁引号在UTF-8格式下根本就无法对付，在不设encoding方法的时候，它们现身为â€“, â€™, â€œ等形式，还可以对付。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dat1 = read.csv(&amp;quot;D:/chengjun/Crystal/Schwab_data_cleaningSep.csv&amp;quot;,
               header = F, sep = &amp;quot;|&amp;quot;, quote = &amp;quot;&amp;quot;, stringsAsFactors=F,
           fileEncoding = &amp;quot;&amp;quot;) # , encoding =&amp;quot;UTF-8&amp;quot;); dim(dat1)

names(dat1) = c(&#39;name&#39;, &#39;organization&#39;, &#39;year&#39;, &#39;country&#39;, &#39;website&#39;,
               &#39;shortIntro&#39;, &#39;focus&#39;, &#39;geo&#39;, &#39;model&#39;, &#39;benefit&#39;, &#39;budget&#39;,
             &#39;revenue&#39;, &#39;recognization&#39;, &#39;background&#39;,
             &#39;innovation&#39;, &#39;entrepreneur&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;2-文本数据清理第一步-载入r包-选取变量&#34;&gt;2. 文本数据清理第一步：载入R包，选取变量&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;library(tm)
library(topicmodels)

text = dat1$entrepreneur
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;3-终于可以删除部分邪恶的拉丁引号&#34;&gt;3. 终于可以删除部分邪恶的拉丁引号&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;text = gsub(&amp;quot;.&amp;quot;, &amp;quot; &amp;quot;, text, fixed = TRUE )
text = gsub(&amp;quot;!&amp;quot;, &amp;quot; &amp;quot;, text, fixed = TRUE )
text = gsub(&amp;quot;?&amp;quot;, &amp;quot; &amp;quot;, text, fixed = TRUE )
text = gsub(&amp;quot;â€œ&amp;quot;, &amp;quot; &amp;quot;, text, fixed = TRUE )
text = gsub(&amp;quot;â€™&amp;quot;, &amp;quot; &amp;quot;, text, fixed = TRUE )
text = gsub(&amp;quot;â€\u009d&amp;quot;, &amp;quot; &amp;quot;, text, fixed = TRUE )
text = gsub(&amp;quot;â€&amp;quot;, &amp;quot; &amp;quot;, text,  fixed = TRUE )
text = gsub(&amp;quot;&amp;lt;/b&amp;gt;&amp;quot;, &amp;quot; &amp;quot;, text,  fixed = TRUE )
text = gsub(&amp;quot;&amp;lt;b&amp;gt;&amp;quot;, &amp;quot; &amp;quot;, text,  fixed = TRUE )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意：1. 这些不规则的latin表达在r的script里再次打开会改变。所以，每次使用都要来这个网页里粘贴复制，也算是一种state-of-art，markdown都比R script保存的持久啊。2.使用gsub的代价是corpus被再度转化为character。所以这段代码如放在下面使用还要用Corpus命令再度转回来。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;corpus &amp;lt;- Corpus(   VectorSource( text )  )  # corpus[[1]] ## inspect the first corpus
# make each letter lowercase
corpus &amp;lt;- tm_map(corpus, tolower)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;4-我这里根据研究需要-要剔除人名-地名-组织名&#34;&gt;4. 我这里根据研究需要，要剔除人名、地名、组织名。&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;# remove generic and custom stopwords
human_find_special_cases = c(&amp;quot;Fundação Pró-Cerrado&amp;quot;, &amp;quot;Fundação PróCerrado&amp;quot;, &amp;quot;FundaÃ§Ã£o PrÃ³-Cerrado&amp;quot;)
my_stopwords &amp;lt;- c(dat1$country,
              dat1$organization,
            dat1$name,
            human_find_special_cases)

my_stopwords &amp;lt;- Corpus(   VectorSource(my_stopwords)  )
my_stopwords &amp;lt;- tm_map(my_stopwords, tolower) # to lowercase my_stopwords here

# Finally we can delete the country/org/person names
corpus &amp;lt;- tm_map(corpus, removeWords, my_stopwords); corpus[[1]]

corpus &amp;lt;- tm_map(corpus, removePunctuation)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;5-将语料转化为documenttermmatrix&#34;&gt;5. 将语料转化为DocumentTermMatrix&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;# install.packages(&amp;quot;SnowballC&amp;quot;)
Sys.setlocale(&amp;quot;LC_COLLATE&amp;quot;, &amp;quot;C&amp;quot;) # set this for reproducible results

corpus &amp;lt;- Corpus(   VectorSource( corpus )  )  # corpus[[1]] ## inspect the first corpus
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外，到这里还是会有孤立的邪恶的拉丁单引号存在，但已经和其它term分开了，在以下DocumentTermMatrix的通过设置minWordLength = 3可以将其完全清理。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# corpus = tm_map(corpus, function(x) iconv(enc2utf8(x), sub = &amp;quot;byte&amp;quot;)) # very important to convert encoding
JSS_dtm &amp;lt;- DocumentTermMatrix(corpus, control = list(stemming = TRUE,  stopwords = TRUE,
                                     minWordLength = 3, removeNumbers = TRUE,
                                     removePunctuation = TRUE
                                     # weighting =
                                             #  function(x)
                                             #  weightTfIdf(x, normalize =FALSE),
                                     ,encoding = &amp;quot;UTF-8&amp;quot;
                               )  )

findFreqTerms(JSS_dtm, lowfreq=0) # 一定要看一下还有没有错误。inspect all the words for errors
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;6-你需要的一些背景知识&#34;&gt;6. 你需要的一些背景知识&lt;/h2&gt;

&lt;h3 id=&#34;6-1-如何识别和转化encoding的类型&#34;&gt;6.1 如何识别和转化encoding的类型?&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;iconvlist() # 看一下玲琅满目的encoding方法
iconv(x, from, to, sub=NA) # Convert Character Vector between Encodings

## convert from Latin-2 to UTF-8: two of the glibc iconv variants.
iconv(x, &amp;quot;ISO_8859-2&amp;quot;, &amp;quot;UTF-8&amp;quot;)
iconv(x, &amp;quot;LATIN2&amp;quot;, &amp;quot;UTF-8&amp;quot;)

## Both x below are in latin1 and will only display correctly in a
## latin1 locale.
(x &amp;lt;- &amp;quot;fa\xE7ile&amp;quot;)
charToRaw(xx &amp;lt;- iconv(x, &amp;quot;latin1&amp;quot;, &amp;quot;UTF-8&amp;quot;))
## in a UTF-8 locale, print(xx)

iconv(x, &amp;quot;latin1&amp;quot;, &amp;quot;ASCII&amp;quot;)          #   NA
iconv(x, &amp;quot;latin1&amp;quot;, &amp;quot;ASCII&amp;quot;, &amp;quot;?&amp;quot;)     # &amp;quot;fa?ile&amp;quot;
iconv(x, &amp;quot;latin1&amp;quot;, &amp;quot;ASCII&amp;quot;, &amp;quot;&amp;quot;)      # &amp;quot;faile&amp;quot;
iconv(x, &amp;quot;latin1&amp;quot;, &amp;quot;ASCII&amp;quot;, &amp;quot;byte&amp;quot;)  # &amp;quot;fa&amp;lt;e7&amp;gt;ile&amp;quot;

# Extracts from R help files
(x &amp;lt;- c(&amp;quot;Ekstr\xf8m&amp;quot;, &amp;quot;J\xf6reskog&amp;quot;, &amp;quot;bi\xdfchen Z\xfcrcher&amp;quot;))
iconv(x, &amp;quot;latin1&amp;quot;, &amp;quot;ASCII//TRANSLIT&amp;quot;)
iconv(x, &amp;quot;latin1&amp;quot;, &amp;quot;ASCII&amp;quot;, sub=&amp;quot;byte&amp;quot;)
## End(Not run)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;6-2-如何看documenttermmatrix的内容&#34;&gt;6.2 如何看DocumentTermMatrix的内容？&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;inspect(JSS_dtm)
colnames(JSS_dtm) # we can find that: [3206]   &amp;quot;works\u009d&amp;quot;    [3207]      &amp;quot;work\u009d&amp;quot;
inspect(JSS_dtm[,3206])
inspect(JSS_dtm[,&amp;quot;works\u009d&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;6-3-关于一些需要跳出的符号&#34;&gt;6.3 关于一些需要跳出的符号&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;unlist(strsplit(&amp;quot;a.b.c&amp;quot;, &amp;quot;.&amp;quot;))
## [1] &amp;quot;&amp;quot; &amp;quot;&amp;quot; &amp;quot;&amp;quot; &amp;quot;&amp;quot; &amp;quot;&amp;quot;
## Note that &#39;split&#39; is a regexp!
## If you really want to split on &#39;.&#39;, use
unlist(strsplit(&amp;quot;a.b.c&amp;quot;, &amp;quot;[.]&amp;quot;))
## [1] &amp;quot;a&amp;quot; &amp;quot;b&amp;quot; &amp;quot;c&amp;quot;
## or
unlist(strsplit(&amp;quot;a.b.c&amp;quot;, &amp;quot;.&amp;quot;, fixed = TRUE))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同理，gsub查找替换掉句点comma的时候也有类似的问题。我在使用python处理相邻多个段落的时候，直接使用了list的append的方法，导致两个自然段之间没有空格。这可要害苦我了。很多可以作为段落结尾的标点都要转化为空格。幸好老外写文章没有那么多问号和叹号结尾。&lt;/p&gt;

&lt;p&gt;fixed = TRUE意味着要use exact matching.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;text1 = gsub(&amp;quot;[.]&amp;quot;, &amp;quot; &amp;quot;, text）
text1 = gsub(&amp;quot;[.]&amp;quot;, &amp;quot; &amp;quot;, text, fixed = TRUE )
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
