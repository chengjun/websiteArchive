<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on </title>
    <link>https://chengjun.github.io/zh/post/index.xml</link>
    <description>Recent content in Posts on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <copyright>&amp;copy; 2016 Cheng-Jun Wang</copyright>
    <lastBuildDate>Thu, 28 Dec 2017 14:32:07 +0800</lastBuildDate>
    <atom:link href="/zh/post/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>2017年“大事记”</title>
      <link>https://chengjun.github.io/zh/post/news2017/</link>
      <pubDate>Thu, 28 Dec 2017 14:32:07 +0800</pubDate>
      
      <guid>https://chengjun.github.io/zh/post/news2017/</guid>
      <description>

&lt;h1 id=&#34;sunbelt2017社会网络分析国际研讨会&#34;&gt;SUNBELT2017社会网络分析国际研讨会&lt;/h1&gt;

&lt;p&gt;2017年6月4日上午，实验中心成员王成军在北京参加了2017年XXXV|| International Network for Social Network Analysis Conference 社会网络分析国际研讨会及第十三届中国社会学会社会网与社会资本研究专业委员会年会，发表论文Leveraging the Flow of Collective Attention for Computational Communication Research，并主持社会网络与计算传播学分论坛第二场。&lt;/p&gt;

&lt;h1 id=&#34;本科生论文答辩&#34;&gt;本科生论文答辩&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;5月27日 14:00&lt;/li&gt;
&lt;li&gt;费彝民楼A407&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;指导学术论文&#34;&gt;指导学术论文&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;专硕

&lt;ul&gt;
&lt;li&gt;周纬 《权威与中心:基于HITS算法的收视率分析》&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;本科生

&lt;ul&gt;
&lt;li&gt;黄浩 134056004 􏴃􏳤􏱼􏱽􏱾􏱿􏲀􏲄􏲁􏱴􏲅􏲆􏳓􏳔􏳕􏲄􏲯􏲰关于搭讪艺术家的行为动机与策略的调查报告&lt;/li&gt;
&lt;li&gt;沈越 &lt;a href=&#34;https://data-journalism.github.io/olympic/index.html&#34; target=&#34;_blank&#34;&gt;《“梦之队”统治奥运会？》&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;曾维靓 131050038 《大学生群体对于微博原生广告的态度及其影响因素》
􏴊􏴋􏴃􏳤􏱼􏱽􏱾􏱿􏲀􏲄􏲁􏱴􏲅􏲆􏳓􏳔􏳕􏲄􏲯􏲰􏴃􏳤􏱼􏱽􏱾􏱿􏲀􏲄􏲁􏱴􏲅􏲆􏳓&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;云南白族扎染暑期社会实践指导&#34;&gt;云南白族扎染暑期社会实践指导&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;Q: 关于实际想探究问题，我们又探究了一下，但感觉都找不到很具体的研究方向，我整理了一下我们大概的想法，麻烦老师你看看有没有哪个想法具有可研究性😶&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;1. 民族旅游发展下的“扎染女”的生存现状与角色变迁&lt;/li&gt;
&lt;li&gt;2.扎染工艺需要做出怎样的改变，才能继续发展?&lt;/li&gt;
&lt;li&gt;3.当地工艺在过去是怎么发展和延续的？&lt;/li&gt;
&lt;li&gt;4.我们可以研究当地人在怎么努力挽救这个工艺么&lt;/li&gt;
&lt;li&gt;5.从扎染看当地经济结构的变迁？&lt;/li&gt;
&lt;li&gt;6.探究云南白族扎染工艺当前生存状况并以白族扎染为例探究对传统手工艺的生产性方式保护.&lt;/li&gt;
&lt;li&gt;7.探究白族扎染的工艺特色，艺术特征，在现代社会的创新运用.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A: 首先概念化，白族扎染是一种国家非物质文化遗产，也是一种传传统手工技艺。这背后对应着什么概念？然后是寻找新的问题。白族扎染作为一种手工技艺，在传承的过程中存在什么新的问题（其它手工艺没有的）？当前产业化的趋势使部分传统扎染技艺走向消亡，原有的民间特色开始退化，污染问题日益突出，市场经营滋生了对经济利益的过度追求，植物染料板兰根供不应求。在此情势下，白族扎染技艺的传承受到困扰。产业化是手工艺凋亡的关键原因，这个解释力很强，白族扎染很难幸免。要确保找到的问题是新的问题，即明确而且有意义。如有可能，要尝试理论化，找找既有的理论如何解释传统工艺的传承。研究问题要将个体（研究者）的困惑与社会结构（问题）联系起来。你们要好好琢磨一下，面对这种手工艺，你们有什么困惑？它又反应了什么social issue？强烈建议你们首先查询相关的论文和图书。&lt;/p&gt;

&lt;h1 id=&#34;华人思想库-省侨办共建智库-华智-项目讨论&#34;&gt;华人思想库-省侨办共建智库（华智）项目讨论&lt;/h1&gt;

&lt;p&gt;《全球治理时代的中国新型智库》&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;陈晓晨 （人大重阳金融研究院专家）&lt;/li&gt;
&lt;li&gt;2017-04-28 9:00 am&lt;/li&gt;
&lt;li&gt;费彝民楼5楼圆桌会议室&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;信得过、用得着、靠得住；建设自己的报送渠道；资金来源稳定，不干涉。&lt;/p&gt;

&lt;h1 id=&#34;计算传播学-用ai穿透你的注意力壁垒&#34;&gt;计算传播学——用AI穿透你的注意力壁垒&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;集智学园简介：&lt;a href=&#34;http://campus.swarma.org/gvid=10140&#34; target=&#34;_blank&#34;&gt;http://campus.swarma.org/gvid=10140&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;时间：2017-04-12 20:00-23:00&lt;/li&gt;
&lt;li&gt;网址：&lt;a href=&#34;http://xue.duobeiyun.com&#34; target=&#34;_blank&#34;&gt;http://xue.duobeiyun.com&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;硕士预答辩&#34;&gt;硕士预答辩&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;费彝民楼503&lt;/li&gt;
&lt;li&gt;2017-03-30 16:00&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;计算传播讲座&#34;&gt;计算传播讲座&lt;/h1&gt;

&lt;p&gt;2017年3月22日 10：00-12：00， 费彝民楼A418， 王成军向2016级博士生同学介绍计算传播学研究进展，包括计算社会科学的发展状况、代表性研究成果，如何评价其理论创新问题，并对社会科学理论进入丛林的问题进行了讨论。&lt;/p&gt;

&lt;h1 id=&#34;博士之家&#34;&gt;博士之家&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;地点：费彝民楼A418&lt;/li&gt;
&lt;li&gt;时间：3月10日 12：00-14：00&lt;/li&gt;
&lt;li&gt;主讲者：&lt;a href=&#34;http://sociology.nju.edu.cn/teacher/social/quanzhi/350.html&#34; target=&#34;_blank&#34;&gt;吴愈晓&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;主持人：王成军&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;创享沙龙-对话音乐与虚拟现实技术&#34;&gt;创享沙龙：对话音乐与虚拟现实技术&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;地点：南京大学（仙林校区） 大学生活动中心南花园206&lt;/li&gt;
&lt;li&gt;时间：2月27日 18：30-19：30&lt;/li&gt;
&lt;li&gt;主讲者：向雪怀、王成军&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;南大新闻 &lt;a href=&#34;http://news.nju.edu.cn/show_article_4_45010&#34; target=&#34;_blank&#34;&gt;音乐人向雪怀与我校师生对话VR技术与校园音乐
&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>手机用户在真实与虚拟空间中的移动</title>
      <link>https://chengjun.github.io/zh/post/cn/2015-12-02-renormalization/</link>
      <pubDate>Wed, 02 Dec 2015 00:00:00 +0000</pubDate>
      
      <guid>https://chengjun.github.io/zh/post/cn/2015-12-02-renormalization/</guid>
      <description>

&lt;p&gt;思考手机用户在真实和虚拟世界的移动本身有什么价值:比较两种类型的网络结构，一个是分形的，一个是小世界的。二者之间不是完全对立的，而是可以平滑过渡的，因而可以计算从小世界到分形的连续变量的取值，即网络增长的过程中，新节点多大程度上按照小世界规则加入网络，多大程度上按照分形加入网络（Song, 2006）。&lt;/p&gt;

&lt;p&gt;自相似或者说分形是本研究切入的重点，分形的特点是在不同尺度上具有相同的特征，这种自相似的特征往往表现为系统特征与观测尺度之间的对应关系。一般对于分形网络而言，重整化之后的盒子数量log(N)与盒子大小log(L)之间存在无标度关系。我们确实也发现人在移动互联网站构成的网络重整化过程中，盒子数量与盒子大小之间具有这种无标度关系，但是人在现实世界的物理移动构成的网络经过重整化，其盒子数量与盒子大小之间则是指数分布。由此可见二者网络结构是如何不同的，前者接近分形，后者接近小世界。&lt;/p&gt;

&lt;p&gt;衡量小世界和分形的重要方法是度相关。分形意味着较强的异配性，而小世界则是同配性。正向的度相关意味着同配性和小世界，负向意味着分形。对于同配性的网络度相关通过重整化，一般会被抹平。有趣的是我们发现真实移动网络会由正变负。即当盒子比较大的时候，局部地区的同配性消失，表现出异配性。&lt;/p&gt;

&lt;p&gt;Song（2006）将网络增长与重整化看成一个逆过程，网络增长的过程可以看成重整化的尺度由大变小的过程。小世界网络增长的过程中，网络的平均直径L与网络中的节点数量具有对数关系，L ~ Log(N)。重整化需要的步数约等于最终网络的平均直径，例如一个直径是14的网络，大约经过14步重整化会由一个完整的网络坍缩为一个节点。在重整化过程中，所选择的盒子的大小l的取值范围约等于最终网络的平均直径range(L)。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www-levich.engr.ccny.cuny.edu/webpage/hmakse/&#34; target=&#34;_blank&#34;&gt;Hernan Makse&lt;/a&gt;是City College of New York的物理学教授，主要从事复杂网络的研究，他和Song Chaoming就分形网络发表了两篇重要论文(Song 2005&amp;amp;2006)。他非常注重代码和数据的分享。我一直关注的是第一作者Song和鼎鼎大名的Havlin，并没有注意到Hernan， 后来搜索实现fractal_model的python代码的时候才找到他，恍然发现他是两篇论文的通讯作者。&lt;/p&gt;

&lt;p&gt;在这个算法里面，主要有四个参数：世代（generation)、新增子代数量(m)、子代间新增链接的数量(x)、断开父代链接的比例(e)。每一代是一个操作过程：以这一代的每一条边为单位，该边上的两个节点a和b各自新增子代m个，其中m对子代间形成（x-1)个链接，若随机概率大于e，那么断开父代节点a和b之间的链接，同时增加一条m对子代间链接。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;generation, m, x, e = 2, 1, 1, 0
G=nx.Graph()
G.add_edge(0,1) #Two seed nodes(generation 0), then add m offsprings to these seed nodes.
node_index = 2
for n in range(1,generation+1):
    print &#39;STEP:&#39;, n, &#39;\n&#39;
    all_links = G.edges()
    while all_links:
            link = all_links.pop() # [(0, 1)] ----&amp;gt; (0, 1)
            print link
            new_nodes_a = range(node_index,node_index + m)
            print link[0], new_nodes_a
            #random.shuffle(new_nodes_a)
            node_index += m
            new_nodes_b = range(node_index,node_index + m)
            #random.shuffle(new_nodes_b)
            print link[1], new_nodes_b
            node_index += m
            G.add_edges_from([(link[0],node) for node in new_nodes_a])
            G.add_edges_from([(link[1],node) for node in new_nodes_b])
            repulsive_links = zip(new_nodes_a,new_nodes_b) # 相斥的的链接
            print repulsive_links
            add_repulsive_links = [repulsive_links.pop() for i in range(x-1)]
            G.add_edges_from(add_repulsive_links) # add edges between offsprings
            print &#39;add repusive edges&#39;, add_repulsive_links
            if random.random() &amp;gt; e: # 当概率大于e的时候，断开hub间的链接
                print &#39;delete&#39;, link
                G.remove_edge(*link) # 减少一对hub的链接
                add_a_repulsive_link = repulsive_links.pop()
                print &#39;add_a_repulsive_link&#39;, add_a_repulsive_link
                G.add_edge(*add_a_repulsive_link) #相应得，增加一对其子代的链接
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;真实世界的移动网络是小世界的。例如Vito Latora 等（2002）分析了波斯顿地铁网络的结构（小世界）。随机网平均直径低，规则网聚类系数高。小世界网络在平均路径长度方面接近随机网，而在聚类系数方面接近规则网。Latora等认为我们对于小世界的两种衡量方式（平均直径L和聚类系数C）有缺陷（ill-defined），因为仅仅强调了链接是否存在，而忽略了链接的&lt;code&gt;权重&lt;/code&gt;，比如链接的实际长度（the physical length of the link）。他们试图提出一种考虑权重的衡量小世界特征的测量:邻接矩阵$$a&lt;em&gt;{ij}$$表示任意两个节点i、j之间是否有链接;$$l&lt;/em&gt;{ij}$$ 表示任意两个节点i、j之间的权重（比如地铁站之间的空间距离;使用邻接矩阵 $$a&lt;em&gt;{ij}$$ 可以得到节点间的最短路径矩阵 $$d&lt;/em&gt;{ij}$$。&lt;/p&gt;

&lt;p&gt;此时，无法算出聚类系数，因为很多地铁站只有两个邻居，算出的平均直径的信息也很少，$$\varepsilon&lt;em&gt;{ij} = \frac{1}{N(N-1)d&lt;/em&gt;{ij}} $$ 表示输运效率，可以在globa和local两个层面计算，分别对应平均路径长度L和聚类系数C。当两个节点无链接时，其$$d&lt;em&gt;{ij}$$无穷大，$$\varepsilon&lt;/em&gt;{ij} = 0$$。避免了计算平均路径长度无穷大的问题。同时可以定义输运成本$$cost = \frac{\sum&lt;em&gt;{i\neq j} a&lt;/em&gt;{ij}l&lt;em&gt;{ij}}{\sum&lt;/em&gt;{i\neq j}l_{ij} }$$。如此计算波斯顿地铁的MBTA全局输运效率为0.63，局部输运效率为0.03，成本为0.002。即网络整体输运效率可以达到理想情况的63%，但是局部输运效率很差，不过整个网络的成本很小。如果加上公交网络MBTA+bus，全局效率上升为 0.72，局部效率大幅度上升为0.46，花费的成本仅仅上升为0.004。&lt;/p&gt;

&lt;h3 id=&#34;参考文献&#34;&gt;参考文献&lt;/h3&gt;

&lt;p&gt;Vito Latora（2002）Is the Boston subway a small-world network? Physica A&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>iching：一个用来算卦的python包</title>
      <link>https://chengjun.github.io/zh/post/cn/2015-07-04-iching-python/</link>
      <pubDate>Sat, 04 Jul 2015 00:00:00 +0000</pubDate>
      
      <guid>https://chengjun.github.io/zh/post/cn/2015-07-04-iching-python/</guid>
      <description>&lt;iframe src=&#34;http://nbviewer.ipython.org/github/chengjun/iching/blob/master/iching_intro.ipynb&#34; scrolling=&#34;no&#34; width=&#34;700&#34; height=&#34;6500&#34;&gt;&lt;/iframe&gt;
</description>
    </item>
    
    <item>
      <title>打包发布python软件包</title>
      <link>https://chengjun.github.io/zh/post/cn/2015-02-22-distribute-python-package/</link>
      <pubDate>Sun, 22 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>https://chengjun.github.io/zh/post/cn/2015-02-22-distribute-python-package/</guid>
      <description>&lt;p&gt;我们经常写一些程序碎片，却很少有动力把它们整合起来。前段时间写了一个爬取并可视化谷歌学术网的python程序。今天想不如把它整合一下，虽然非常简单（只有一个函数）。主要参考python官网的&lt;a href=&#34;https://packaging.python.org/en/latest/distributing.html#uploading-your-project-to-pypi&#34; target=&#34;_blank&#34;&gt;发布指南&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;##注册
于是首先来到pypi网站注册。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://pypi.python.org/pypi?%3Aaction=submit_form&#34; target=&#34;_blank&#34;&gt;https://pypi.python.org/pypi?%3Aaction=submit_form&lt;/a&gt;
记下用户名chengjun和密码W4&lt;/p&gt;

&lt;p&gt;##填写软件包信息
《指南》推荐直接在线填写 &lt;a href=&#34;https://pypi.python.org/pypi?%3Aaction=submit_form&#34; target=&#34;_blank&#34;&gt;https://pypi.python.org/pypi?%3Aaction=submit_form&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;##打包和发布工具
先要安装两个包：twine和wheel。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pip install wheel
pip install twine
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;##整理项目文件夹
找项目实例（&lt;a href=&#34;https://github.com/pypa/sampleproject）下载下来，修改其中的部分内容即可。详见指南，或者自己摸索即可。&#34; target=&#34;_blank&#34;&gt;https://github.com/pypa/sampleproject）下载下来，修改其中的部分内容即可。详见指南，或者自己摸索即可。&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;##打包发布
1.在window环境下，使用cmd，转换工作路径到项目文件夹。
2. 主要参考 &lt;a href=&#34;https://github.com/pypa/twine打包发布：&#34; target=&#34;_blank&#34;&gt;https://github.com/pypa/twine打包发布：&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#Create some distributions in the normal way:
$ python setup.py sdist bdist_wheel

#Upload with twine:
$ twine upload dist/*
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我使用上传的时候出错（typeError），于是直接使用打包好的zip文件（在dist子文件夹当中）手工上传到pypi。注意，每次上传到pypi需要修改一次setup.py中的版本号，并重新打包才可上传。如此而已，比我想象当中要速度快得多、简单的多。&lt;/p&gt;

&lt;p&gt;这里是我刚刚打包发布的一个可视化谷歌学术网络的python软件包：&lt;a href=&#34;https://pypi.python.org/pypi/scholarNetwork/&#34; target=&#34;_blank&#34;&gt;https://pypi.python.org/pypi/scholarNetwork/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>表达、检索、练习——写给Python的初学者</title>
      <link>https://chengjun.github.io/zh/post/cn/2015-02-22-fresh-python/</link>
      <pubDate>Sun, 22 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>https://chengjun.github.io/zh/post/cn/2015-02-22-fresh-python/</guid>
      <description>&lt;p&gt;三年前，我写过一篇小日志，介绍如何从零开始学习R语言。后来我的工作越来越多的使用python，于是摸爬滚打自己探索了挺久的。身边也越来越多的人问起新手如何从零学习python的问题。我想练习、检索、表达这三点依旧是关键，只不过顺序稍有不同。很多人说，学习python，你来推荐一些资料吧。我想了想，资料还不是关键，关键是自身。&lt;/p&gt;

&lt;p&gt;##看书还是上手？
以前我们学语言，比如C语言或者Basic语言，首先讲的都是字符、数字、列表、逻辑符等。几乎所有的编程书都会这么讲，所以很多人会觉得看书没有什么新意。不过我觉得看书还是必须的，重点就是要去掌握这些基本的东西。&lt;/p&gt;

&lt;p&gt;当然了，掌握这些并不能帮助我们完成手上的工作。是的，并不能！总有一些细节的地方你必须去hack现有的代码。于是乎就有了另外一种学习语言的哲学：干中学（learn by doing）。基本的逻辑就是不断摸索，硬着头皮上。这种风格非常强悍，虽然刚开始的时候容易犯非常浅显的毛病，但是却是真正学语言的不二法门。&lt;/p&gt;

&lt;p&gt;##表达
到底看书还是上手呢？我主张先明确自己的问题是什么。做研究的人都知道，我们往往对于自己所面临的问题并不明确。写程序、学语言也是这个样子。首先要明确地表达出来。这是表达的第一重意思。&lt;/p&gt;

&lt;p&gt;##检索
当你问题明确之后，就可以去检索了。去哪里检索？书中、网上，不拘于形式。重要的是解决问题。这个过程中，我们带着问题读书、上网、提问，能够培养我们独立思考的能力。&lt;/p&gt;

&lt;p&gt;互联网的发展，使得很多时候我们并不需要真正去创造什么，只要检索一下，总能找到好的代码，修改以下就能解决自己的问题。我觉得挺好。这符合我们学习语言的初衷。有个说法是十年学会编程，但是使用编程一个月就够了。&lt;/p&gt;

&lt;p&gt;既然可以看书，有什么推荐的吗？我推荐Beginning Python这本书，虽然我是从A Byte of Python开始看的。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A Byte of Python &lt;a href=&#34;http://book.douban.com/subject/5948760/&#34; target=&#34;_blank&#34;&gt;http://book.douban.com/subject/5948760/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Beginning Python （Python 基础教程） &lt;a href=&#34;http://book.douban.com/subject/3205338/&#34; target=&#34;_blank&#34;&gt;http://book.douban.com/subject/3205338/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Hello World！Computer Programming for Kids and Other Beginners 与孩子一起学编程 &lt;a href=&#34;http://book.douban.com/subject/5338024/&#34; target=&#34;_blank&#34;&gt;http://book.douban.com/subject/5338024/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;How to Think Like a Computer Scientist: Learning with Python &lt;a href=&#34;http://book.douban.com/subject/1481058/&#34; target=&#34;_blank&#34;&gt;http://book.douban.com/subject/1481058/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;21 Recipes for Mining Twitter &lt;a href=&#34;http://book.douban.com/subject/5988563/&#34; target=&#34;_blank&#34;&gt;http://book.douban.com/subject/5988563/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Mining the Social Web &lt;a href=&#34;http://book.douban.com/subject/5391582/&#34; target=&#34;_blank&#34;&gt;http://book.douban.com/subject/5391582/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Toby Segaran (2007) Programming Collective Intelligence Building Smart Web 2.0 Applications. O&amp;rsquo;Reilly Media&lt;/li&gt;
&lt;li&gt;Python公开课 中文课程 &lt;a href=&#34;http://www.imooc.com/view/177&#34; target=&#34;_blank&#34;&gt;http://www.imooc.com/view/177&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;##练习
有了问题和思路之后，重要的就是练习了。不要怕麻烦，经常动手写东西。这个是不二法门。&lt;/p&gt;

&lt;p&gt;还有一些琐碎的东西，如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;我觉得学python上手很重要，选一个好的IDE很重要，对于windows用户我推荐winpython，使用集成于其中的spyder编程很方便，不需要指定python的路径，安装第三方包也很方便。&lt;/li&gt;
&lt;li&gt;每天都接触点Python,写写博客。&lt;/li&gt;
&lt;li&gt;既然使用Python了，那么google就是你最好的朋友！用英文检索。&lt;/li&gt;
&lt;li&gt;不要错过Github。上传你的代码。便于保存和分享。python的精神是开放，开源。&lt;/li&gt;
&lt;li&gt;很多时候，最大的问题是你不知道自己面临的问题：我的经验是用英文一句话说出你的问题。然后借助搜索引擎。你一般都能找到答案。Python的email list和stackoverflow中有很多想要的答案。&lt;/li&gt;
&lt;li&gt;实在找不到解决方案，不要过多寄希望于身边的朋友，stackoverflow上有更合适的回答你问题的人！去那里提问。&lt;/li&gt;
&lt;li&gt;熟悉一个package。经常阅读package的文档。&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>再次尝试shinyApp</title>
      <link>https://chengjun.github.io/zh/post/cn/2015-01-11-shinyapp/</link>
      <pubDate>Sun, 11 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>https://chengjun.github.io/zh/post/cn/2015-01-11-shinyapp/</guid>
      <description>&lt;p&gt;昨天收到了shinyapp的一封邮件，想起之前自己做的关于网络扩散的东西，就想把它转化为app的形式。最直接的办法还是看tutorial，比如（&lt;a href=&#34;http://shiny.rstudio.com/tutorial）。&#34; target=&#34;_blank&#34;&gt;http://shiny.rstudio.com/tutorial）。&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;现学现卖，于是我马上做了一个使用igraph绘制BA网络的小应用：&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://chengjun.shinyapps.io/testApp/&#34; target=&#34;_blank&#34;&gt;https://chengjun.shinyapps.io/testApp/&lt;/a&gt;&lt;/p&gt;

&lt;iframe src=&#39;https://chengjun.shinyapps.io/testApp/&#39; scrolling=&#34;no&#34; width=&#34;600&#34; height = &#34;800&#34;&gt;&lt;/iframe&gt;

&lt;p&gt;##通过rstudio学习shinyApp制作
其实R的王牌编辑器Rstudio已经和shiny完美的结合，完全可以通过rstudio学习shinyApp制作。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  system.file(&amp;quot;examples&amp;quot;, package=&amp;quot;shiny&amp;quot;)

  runExample(&amp;quot;01_hello&amp;quot;) # a histogram
  runExample(&amp;quot;02_text&amp;quot;) # tables and data frames
  runExample(&amp;quot;03_reactivity&amp;quot;) # a reactive expression
  runExample(&amp;quot;04_mpg&amp;quot;) # global variables
  runExample(&amp;quot;05_sliders&amp;quot;) # slider bars
  runExample(&amp;quot;06_tabsets&amp;quot;) # tabbed panels
  runExample(&amp;quot;07_widgets&amp;quot;) # help text and submit buttons
  runExample(&amp;quot;08_html&amp;quot;) # shiny app built from HTML
  runExample(&amp;quot;09_upload&amp;quot;) # file upload wizard
  runExample(&amp;quot;10_download&amp;quot;) # file download wizard
  runExample(&amp;quot;11_timer&amp;quot;) # an automated timer
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>NetworkX初步：创建网络、提取属性和绘图</title>
      <link>https://chengjun.github.io/zh/post/cn/2014-08-14-networkx-intro/</link>
      <pubDate>Thu, 14 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>https://chengjun.github.io/zh/post/cn/2014-08-14-networkx-intro/</guid>
      <description>&lt;p&gt;NetworkX是使用python分析网络数据的重要武器。它的使用非常简单。&lt;/p&gt;

&lt;p&gt;首先，创建网络对象：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import matplotlib.pyplot as plt
import networkx as nx

G=nx.DiGraph()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;然后，添加链接：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;G.add_edge(&#39;source&#39;,1,weight=80)
G.add_edge(1,2,weight=50)
G.add_edge(1,3,weight=30)
G.add_edge(3,2,weight=10)
G.add_edge(2,4,weight=20)
G.add_edge(2,5,weight=30)
G.add_edge(4,5,weight=10)
G.add_edge(5,3,weight=5)
G.add_edge(2,&#39;sink&#39;,weight=10)
G.add_edge(4,&#39;sink&#39;,weight=10)
G.add_edge(3,&#39;sink&#39;,weight=25)
G.add_edge(5,&#39;sink&#39;,weight=35)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;可以很容易提取边的权重:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;edges,colors = zip(*nx.get_edge_attributes(G,&#39;weight&#39;).items())
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;计算加权过的出度：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;d = G.out_degree(weight = &#39;weight&#39;) #计算节点的中心度
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;选择一个常用的可视化方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pos=nx.spring_layout(G) #设置网络的布局
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;绘制网络:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;nx.draw(G, pos, node_color = &#39;orange&#39;, with_labels = True,
        nodelist = d.keys(), node_size = [v*5 for v in d.values()], 
        edgelist = edges, edge_color = colors, width = 5, edge_cmap=plt.cm.Blues)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://chengjun.qiniudn.com/demo.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;计算流距离：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&#39;&#39;&#39;
# get flow distance
&#39;&#39;&#39;
def toSink(G, i):
        try:
            di = G[i][&#39;sink&#39;].values()[0]
        except:
            di = 0 
        return di

def flowDistanceDT(G): #input a balanced nx graph
    R = G.reverse()
    mapping = {&#39;source&#39;:&#39;sink&#39;,&#39;sink&#39;:&#39;source&#39;} 
    H = nx.relabel_nodes(R,mapping)
    #---------initialize flow distance dict------
    L = dict((i,1) for i in G.nodes())  #FlowDistance
    #---------prepare weighted out-degree dict------
    D = {i: toSink(G, i) for i in G.nodes()} #Di
    T = G.out_degree(weight=&#39;weight&#39;)        #Ti
    #---------iterate until converge------------
    ls = np.array(L.values())
    delta = len(L)*0.01 + 1
    while delta &amp;gt; len(L)*0.01:
        for i in L:
            l=1
            for m,n in H.edges(i):
                l+=L[n]*H[m][n].values()[0]/float(T[m])
            L[i]=l
        delta = sum(np.abs(np.array(L.values()) - ls))
        ls = np.array(L.values())
    #---------clean the result-------
    del L[&#39;sink&#39;]
    for i in L:
        L[i]-=1
    L[&#39;sink&#39;] = L.pop(&#39;source&#39;)
    T[&#39;sink&#39;] = T.pop(&#39;source&#39;)
    D[&#39;sink&#39;] = D.pop(&#39;source&#39;)
    return L.values(), D.values(), T.values()
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>我们能走多远——读王晓明的《鲁迅传》</title>
      <link>https://chengjun.github.io/zh/post/cn/2009-02-14-read-luxun-in-text/</link>
      <pubDate>Sat, 14 Feb 2009 00:00:00 +0000</pubDate>
      
      <guid>https://chengjun.github.io/zh/post/cn/2009-02-14-read-luxun-in-text/</guid>
      <description>&lt;p&gt;我不再像先前那样&lt;strong&gt;崇拜&lt;/strong&gt;他了，但我自觉在深层的心理和情感距离上，似乎是离他越来越近；我也不再将他视作一个偶像，&lt;em&gt;他分明就在我们中间，和我们一样在深重的危机中苦苦挣扎。&lt;/em&gt; ——王晓明
　　&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://chinadigitaltimes.net/chinese/files/2012/12/%E9%B2%81%E8%BF%85.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;自己在网上碰到这本书，就下了下来，但自己对于鲁迅的记忆却局限于高中教科书《社戏》、《从百草园到三味书屋》里面，恰若我们对于马克思同样很无知一样，我们距离这些曾经吸引民族目光的人物似乎很近，但又的确很远。但人生的某个阶段总会有难解的问题，这个时候就只好去走进这些被我们所远离的灵魂。&lt;/p&gt;

&lt;p&gt;鲁迅本姓周，名樟寿，后改为树人：号豫山，后改为豫才。鲁迅是他的笔名，一八八一年九月二十五日出生在绍兴城内周姓望族，祖父周介孚，出身翰林，让鲁迅在启蒙的时候先读历史，而不是四书五经；父亲周伯宜，对鲁迅非常宽厚，允许他读闲书；母亲鲁瑞更是喜欢他。少年鲁迅便生活在这种颇为繁华而宽厚的环境中读书长大，调皮好斗，有着少年所特有的骄傲。但祖父却因为一次科场行贿案下狱，父亲也吐血并终于去世，亲戚便不再对鲁迅家客气，分房子的时候给鲁迅家最差的房子。这种世态炎凉、由繁华转为凄苦的经历很容易让人想到曹雪芹，另一个在中国的晚期封建社会反儒家的困顿的斗士。后来在广州，青年学生问他为什么憎恶旧社会，他回答：“我小的时候，因为家境好，人们看我像王子一样，但是，一旦我家庭发生变故后，人们就把我看成叫花子都不如了，我感到这不是一个人住的社会，&lt;strong&gt;从那时起，我就恨这个社会&lt;/strong&gt;。”今天再思考这个，我们究竟走出去多远呢？市场经济之后，实用主义潮起，我们仿佛又能看到那些忧伤的娜拉了。&lt;/p&gt;

&lt;p&gt;若以&lt;strong&gt;&lt;em&gt;事件史&lt;/em&gt;&lt;/strong&gt;的眼光看个体的人，就要重视他在十八岁的选择，这一年鲁迅乡试成绩为中上等，却因为一个弟弟的夭亡而放弃复试，家境的中落、人情的冷落也使他不能去经商或者像很多绍兴人一样选择去做师爷、幕僚。*如今日高考无望的情况相识，鲁迅只能选择去读江南水师学堂，差不多是免费。*王晓明写到：“可也惟其如此，学生多不愿以本名注册，而要改换姓名，鲁迅那个‘周树人’的名字，就是这样起的。”但不满意学堂教员的无知只好转到路矿学院，1902年，因为没有钱只好争取公费去那个极度自卑并极度自大的在7年前打败中国的地方留学去了，在日本呆了七年，因为日本人的傲慢、侮辱和留日学生的不成器的丑态而痛苦，终于无法忍受，放弃学医转而从文，要医治这个民族的精神状况。但如王晓明所言，“一九0六年初夏，鲁迅返回东京，这时候他已经二十六岁了。&lt;strong&gt;用去了八年的青春，从中国到日本，又从仙台回东京，四处寻求生路，却总是走不通，兜了一个大圈子，还是回到老地方：没有钱，也没有文凭，两手空空，一无所有。&lt;/strong&gt;”在这一点上，马克思比鲁迅幸运多了，至少他在大学阶段找到了自己受用终生的道路——哲学。&lt;/p&gt;

&lt;p&gt;我们已经知道了鲁迅从少年时代就开始大量阅读各种杂书，他应该感谢自己的这种习惯，王晓明描绘了鲁迅的读书状况：林纾翻译的小说一本不落；梁启超主笔的《时务报》几乎是每期必读；他更用心读理论书，严复翻译的《天演论》和《法意》，他是读了又读，还郑重其事地向别人推荐，还曾给周作人推荐约翰•穆勒的《逻辑体系》。以今日状况来看，有多少人真正读过《进化论》和《论法的精神》呢？更不用说小穆勒的书了。尤其是达尔文的理论使鲁迅走出了历史悲观主义关于“今不胜昔”和“一乱一治”的历史循环主义，对于鲁迅来说这不啻是精神革命，正如马克思读了在大学里成为了一个激进的青年黑格尔主义者一样，鲁迅也成了一个“我以我血荐轩辕”的&lt;strong&gt;社会进化论者&lt;/strong&gt;，从仙台来到东京，他毅然剪掉了辫子，严肃地和同学们一起思考民族的问题。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Tianyanlun.jpg/200px-Tianyanlun.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;话说回来，也许正如张饶庭所说，人本身就是loglinear，是非线性的，总是充满了矛盾。记得我自己更夸张的说过“人即回归”的话，都是一样的，只不过是加入了点宿命的色彩。传统与革命便如此自然地熔铸在鲁迅的遭遇中。热血青年鲁迅在日本思想开始倾向于革命，并且
*真真正正的加入了光复会*，然而当被要求回国像徐锡麟一样刺杀满清高官的时候，鲁迅却问了句：“如果我被抓住，被砍头，剩下我的母亲，谁负责赡养她呢？”领导收回成命，刺杀遂无疾而终。对此王晓明写道：他不能无条件地相信别人。即便一时冲动，时间稍长，他对卑劣人心的体验，对一切冠冕堂皇的东西的习惯性怀疑。正是在1906年，鲁迅被母亲以生病为由骗回家，娶了朱安，后者曾经拒绝按照鲁迅的意见进入新式学堂读书。鲁迅在婚后第四天就回了日本。&lt;/p&gt;

&lt;p&gt;一九○八年夏天，继续在东京读书学德语。从夏天开始，每星期日往章太炎在东京的寓所，听他讲学，历时大约半年。称章太炎为鲁迅的老师，跟这个有关系吧。1909年，与周作人合译的《域外小说集》第一册出版，不久《域外小说集》第二册出版。为了负担家庭经济，离开日本回国，结束了七年的留学生活。 回国后，就任杭州的浙江两级师范学堂的生理和化学教员，兼任日籍教师的翻译。1910年担任绍兴府中学堂的监学，兼教生物课。1911年，武昌起义爆发，绍兴城内一片混乱，遂应府中学堂学生的请求，回校暂管校务。 带领学生演说队上街宣传革命，安定民心。不久，受新任绍兴军政府都督王金发委任，担任山会初级师范学堂监督。但是，革命并没有给民族带来更多的东西，&lt;strong&gt;革命党的官员一样压抑自由和新思想，鲁迅最终选择离开&lt;/strong&gt;。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://encrypted-tbn2.gstatic.com/images?q=tbn:ANd9GcR6vRD0RnK_-qJXAONWIa6HBFZC0a26H5hSTjbUwtZCSbWHx8gW&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;靠着朋友的推荐、应教育总长蔡元培邀请，去南京中华民国临时政府的&lt;strong&gt;教育部&lt;/strong&gt;任职。从此，逃离绍兴。因教育部北迁，单身前往北京，住进宣武门外的绍兴会馆。 任北洋政府教育部佥事，兼第一科科长。这便是鲁迅中年一次重要选择：为官的仕途之路，所幸收入颇高。但鲁迅虽有怀疑的毛病，却无法接受现实的倾轧，他去研究石刻拓本、抄古碑，他每天上午九、十点钟起床，梳洗后直接去部里办公，到黄昏时返回会馆。吃过晚饭，八点钟开始抄碑，看佛经，读墓志，常常要到半夜一两点钟。对此，王晓明说：对自己的个人幸福，他也不能再抱什么希望了。他刻了一方石章，曰“堂”；又给自己选了一个号，叫做“俟堂”。笔划虽不同，意思是一个，就是“待死堂”。这个时候，鲁迅已经不得不去思考死亡、品味孤独、遭遇自我。生活、婚姻、国家的不顺遂和鲁迅的悲观主义都把他引入到了遭遇虚无的境地。从这个角度上讲，鲁迅和陀思妥耶夫斯基和托尔斯泰是有相近之处的。鲁迅的待死堂和陀思妥耶夫斯基的《死屋手记》中的死屋何其相似。&lt;/p&gt;

&lt;p&gt;对于在五四阶段鲁迅带着面具的呐喊，王晓明直言：对启蒙的信心，他其实比其他人小，对中国的前途，也看得比其他人糟。五四的成功使鲁迅获得了极高的文学声望，“他不再是绍兴会馆里那个默默无闻的“待死”者了，他现在成了大学讲台上的名教授，读者钦慕的名作家。”&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://www.cssn.cn/xr/xr_rwgs/xr_lzp/201402/W020140211348575353353.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;1925年，“鲁迅和女师大的学生许广平等人开始来往，通信日渐频繁，好感逐渐加深，他在感情上，也会不自觉地向这批学生倾斜，于是在五月十二日的《京报副刊》上，他公开表态支持学生，随后又联络其他一些教员，联名宣告反对杨荫榆。”这场纠纷使鲁迅失去了他的官位——章士钊撤了她的职。后来，鲁迅起诉了章士钊重新获得了这个职位。对此王晓明说：从少年时代起，他就吃够了贫困的苦头，他很早就懂得了没有钱，什么事都干不成，在那篇《娜拉走后怎样》的演讲中，他那样强调“经济权”，就正是出于自己的痛苦经验。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://encrypted-tbn1.gstatic.com/images?q=tbn:ANd9GcSoJWs2ppOimLhW41hIfvvu2z1FiGmOCZaSEIp501_ezwmqw7vTuA&#34; alt=&#34;章士钊&#34; /&gt;&lt;/p&gt;

&lt;p&gt;人存在的意义是什么？王晓明说：“人的生存意义，就体现在他人对你的需要之中，即使鲁迅对社会的变革完全失去信心，对自己在这变革中的作用也不存指望，他的精神世界大概仍不会垮掉，还有一根坚固的支柱在支撑着他，那就是他对和睦的家庭生活的期待，对自己作为这个家庭的主要维持者的自豪。” 1923年，鲁迅和羽太信子发生严重的冲突 ，随之和周作人兄弟反目，他就迁往西城的砖塔胡同六十一号。鲁迅对自己的母亲并没有太大的好感，家庭的分裂，更让他陷入了痛苦的深渊之中。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;https://encrypted-tbn2.gstatic.com/images?q=tbn:ANd9GcSdv1vo4wbzR4zgme2uLWOcPfv0rHmUhYeDJvrpjTFl2SWIDf0-vQ&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;鲁迅从人道主义转向了个人主义，由启蒙的悲观主义，转向了存在的虚无主义。俄国作家阿尔志跋绥夫在小说《工人绥惠略夫》中，以主人公绥惠略夫表现的一种思想，用鲁迅的话说，就是“要救群众，而反被群众所迫害，终至成了单人，忿激之余，一转而仇视一切，无论对谁都开枪，自己也归于毁灭。” 王晓明写到：从启蒙者的悲观和绝望，从对尼采和绥惠略夫的共鸣和认同，鲁迅一步步走进了虚无感。&lt;/p&gt;

&lt;p&gt;鲁迅在虚无主义的鬼气中滑落的太远，但1925年，他还是找打了一个缺口：对女人的爱情，逃了出来。这一年他和许广平相爱了，许并不漂亮，但对于新思想、自由、革命的坚毅鼓舞了鲁迅。26年，鲁迅到厦门大学任教，后来又去了广州中山大学，此刻许广平已经陪伴在他身边了。因为老对手顾颉刚要到中大，鲁迅和许广平便到了上海。王晓明写道：“从某种意义上讲，鲁迅和许广平相爱而终于同居，在上海建立新的家庭，是他一生中最有光彩的举动。正是在这件事情上，他充分表现了生命意志的执拗的力量，表现了背叛传统礼教的坚决的勇气，表现了一个现代人追求个人自由的个性风采。但是，也恰恰在这件事情上，他内心深处的软弱和自卑，他对传统道德的下意识的认同，他对社会和人性的根深蒂固的不信任，都表现得格外触目。一个人一旦相信爱情，就不再是虚无主义者。”&lt;/p&gt;

&lt;p&gt;鲁迅的人生是在上海画上终点的，在这个时期，鲁迅不断遭遇到华盖运。早期，鲁迅是要医治民族性，对大众，他是轻蔑的，他依靠知识分子的，尤其是学生，但人生的种种遭遇却使他不断看到知识分子的局限性，读了一些马克思主义的东西之后，鲁迅转而提出大众才是推动历史变革者。但是这与他素有的思想是矛盾的。终于，他提出了新知识分子的说法。“一九三二年底，他第二次回北京探望母亲，去北京女子文理学院和北京师范大学演讲，都特别挑起知识阶级会不会灭亡的话题，反复强调说，有一种新的知识者，他们与群众结合，反对个人主义，能够把握住实际人生，因此在将来仍能生存。”&lt;/p&gt;

&lt;p&gt;一九三０年五月，他发起成立了左联的筹备会。他刚刚和共产党人结盟，共产党的一位领导人李立三，就秘密约见他，要他以周树人的名字写一篇骂蒋介石的文章，被鲁迅婉言拒绝。虽然他倾向于共产党，但他同样厌恶成仿吾和周扬那一类共产党人。&lt;/p&gt;

&lt;p&gt;在生命中的最后一年，1936年，鲁迅写了《死》，显示出一种非常特别的态度：既不回避，也不设法改造，就站在那里谈论自己的死。十月十九日，上午五时二十五分逝世。&lt;/p&gt;

&lt;p&gt;这便是鲁迅的一生，一个对我们来说应该熟悉、但其实显然十分陌生的人生，我们在他的身上看到了太多的矛盾，看到了存在主义哲学对人生的思考的深刻的印记。处于一个悲剧似的、似乎看不到希望地转折的时代，鲁迅经历了人生中的种种波折，大到祖国，中到家庭，小到个人，没有一个顺遂人心，这个小个子、坏脾气、多疑的人，在那个时代里耗尽了自己的所有生命力。鲁迅在他的人生的当中不可避免的遭遇虚无，国家风雨飘摇、鸡鸣不已，大众是冷漠的、愚蠢的群体，少年家道中落，慈母是误进的毒药，朱安是无法交流的对象，和弟弟周作人虽志趣相近却终于反目，最后自己所倾向的知识分子也多靠不住。鲁迅的幸运是青少年时的广泛读书、留学日本的经历、加入光复会的经验、许广平的爱情和共产主义的发展。&lt;/p&gt;

&lt;p&gt;鲁迅故去93年了，我们已经从鲁迅的铁屋走出了很远，但我们绝大多数人对鲁迅的认识依然很浅薄。娜拉出走之后怎样？我们挣脱了一个旧的世界，我们又能走多远？鲁迅今日若在，他又能写写什么呢？鲁迅说：“跨过那站着的前人。”我们真的超越了吗？&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Echarts使用简介</title>
      <link>https://chengjun.github.io/zh/post/cn/2015-01-10-myecharts/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chengjun.github.io/zh/post/cn/2015-01-10-myecharts/</guid>
      <description>&lt;p&gt;昨天听谈和讲解了echarts的使用，他的讲解非常直接简单，就是直接修改echarts的实例。之后，我发现林峰将echarts的实例的html代码写得非常复杂，但其实单独调用一个js的时候，却非常简单。具体做法如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;准备工作：建立一个js子文件夹，将esl，echarts，pie，bar，map等各种js放入其中。在js文件夹外新建一个空的html文件。&lt;/li&gt;
&lt;li&gt;首先，调用esl.js，它提供了echarts图片的载体。&lt;/li&gt;
&lt;li&gt;其次，使用require方法调用echarts.js和具体使用的类型图的js（比如map.js）。&lt;/li&gt;
&lt;li&gt;再次，输入需要输入的数据。&lt;/li&gt;

&lt;li&gt;&lt;p&gt;最后，封装。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    &amp;lt;!DCOTYPE html&amp;gt;
    &amp;lt;html&amp;gt;
        &amp;lt;head&amp;gt;
            &amp;lt;meta charset=&amp;quot;utf-8&amp;quot;&amp;gt;
            &amp;lt;title&amp;gt;echarts testing page&amp;lt;/title&amp;gt;
            &amp;lt;script src=&amp;quot;./js/esl.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
            &amp;lt;script src=&amp;quot;./js/echarts.js&amp;quot; type=&amp;quot;text/javascript&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
        &amp;lt;/head&amp;gt;
        &amp;lt;body&amp;gt;
            &amp;lt;div id=&amp;quot;main&amp;quot; style=&amp;quot;height:400px;&amp;quot;&amp;gt;&amp;lt;/div&amp;gt;
            &amp;lt;script type=&amp;quot;text/javascript&amp;quot;&amp;gt;
                require.config({
                    paths:{
                        &amp;quot;echarts&amp;quot;:&amp;quot;js/echarts&amp;quot;,
                        &amp;quot;echarts/chart/map&amp;quot;:&amp;quot;js/map&amp;quot;
                    }
                });

                //using
                require(
                    [
                        &amp;quot;echarts&amp;quot;,
                        &amp;quot;echarts/chart/map&amp;quot;
                    ],
                    function(ec){
                        var myChart=ec.init(document.getElementById(&amp;quot;main&amp;quot;));  
                        &amp;lt;!--Input your code below--&amp;gt;                    

                        &amp;lt;!--Input your code above--&amp;gt;                    
                //loading data
                        myChart.setOption(option);
                    }
                );
            &amp;lt;/script&amp;gt;
        &amp;lt;/body&amp;gt;
    &amp;lt;/html&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;##弦图
&lt;a href=&#34;http://chengjun.github.io/myecharts/chord.html&#34; target=&#34;_blank&#34;&gt;http://chengjun.github.io/myecharts/chord.html&lt;/a&gt;&lt;/p&gt;

&lt;iframe src=&#39;http://chengjun.github.io/myecharts/chord.html&#39; scrolling=&#34;no&#34; width=&#34;600&#34; height = &#34;400&#34;&gt;&lt;/iframe&gt;

&lt;p&gt;##柱状图
&lt;a href=&#34;http://chengjun.github.io/myecharts/bar.html&#34; target=&#34;_blank&#34;&gt;http://chengjun.github.io/myecharts/bar.html&lt;/a&gt;&lt;/p&gt;

&lt;iframe src=&#39;http://chengjun.github.io/myecharts/bar.html&#39; scrolling=&#34;no&#34; width=&#34;600&#34; height = &#34;400&#34;&gt;&lt;/iframe&gt;

&lt;p&gt;##饼图
&lt;a href=&#34;http://chengjun.github.io/myecharts/pie1.html&#34; target=&#34;_blank&#34;&gt;http://chengjun.github.io/myecharts/pie1.html&lt;/a&gt;&lt;/p&gt;

&lt;iframe src=&#39;http://chengjun.github.io/myecharts/pie1.html&#39; scrolling=&#34;no&#34; width=&#34;600&#34; height = &#34;400&#34;&gt;&lt;/iframe&gt;

&lt;p&gt;##线图
&lt;a href=&#34;http://chengjun.github.io/myecharts/line1.html&#34; target=&#34;_blank&#34;&gt;http://chengjun.github.io/myecharts/line1.html&lt;/a&gt;&lt;/p&gt;

&lt;iframe src=&#39;http://chengjun.github.io/myecharts/line1.html&#39; scrolling=&#34;no&#34; width=&#34;600&#34; height = &#34;400&#34;&gt;&lt;/iframe&gt;

&lt;p&gt;##地图
&lt;a href=&#34;http://chengjun.github.io/myecharts/map9.html&#34; target=&#34;_blank&#34;&gt;http://chengjun.github.io/myecharts/map9.html&lt;/a&gt;&lt;/p&gt;

&lt;iframe src=&#39;http://chengjun.github.io/myecharts/map9.html&#39; scrolling=&#34;no&#34; width=&#34;600&#34; height = &#34;400&#34;&gt;&lt;/iframe&gt;

&lt;p&gt;##力图&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://chengjun.github.io/myecharts/force2&#34; target=&#34;_blank&#34;&gt;http://chengjun.github.io/myecharts/force2&lt;/a&gt;&lt;/p&gt;

&lt;iframe src=&#39;http://chengjun.github.io/myecharts/force2.html&#39; scrolling=&#34;no&#34; width=&#34;600&#34; height = &#34;400&#34;&gt;&lt;/iframe&gt;

&lt;p&gt;##后记
发现chrome无法加载，再加入了以下代码后就可以使用了。可惜用了整整一个上午才更正这个问题。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        &amp;lt;script src=&amp;quot;./js/echarts.js&amp;quot; type=&amp;quot;text/javascript&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Parsing Githubarchive Data using Python</title>
      <link>https://chengjun.github.io/zh/post/cn/2016-06-18-parsing-githubarchive-json/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chengjun.github.io/zh/post/cn/2016-06-18-parsing-githubarchive-json/</guid>
      <description>&lt;p&gt;Githubarchive整理了github的历史数据，每一个小时一个数据文件，我下载了2011-2014年四年的数据，结果发现2012年的多数数据存在错误换行的问题，需要清洗。一般而言，正确的情况是一个字典格式的数据占一行，错误的情况就会是所有的字典格式的数据合并为了一行。似乎是缺乏换行符造成的。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://stackoverflow.com/questions/10432432/yajl-parse-error-with-githubarchive-org-json-stream-in-python#&#34; target=&#34;_blank&#34;&gt;http://stackoverflow.com/questions/10432432/yajl-parse-error-with-githubarchive-org-json-stream-in-python#&lt;/a&gt; 这里详细记录了这个问题。&lt;/p&gt;

&lt;p&gt;按照这个帖子，我尝试了很多方法，yajl和ijson，但是都不能很优雅的解决我的问题。不妨采用暴力的方法。&lt;/p&gt;

&lt;p&gt;rirwin利用了{和}出现的偶数关系来分割字符串，但是这种方法容易遗漏数据且速度较慢。&lt;/p&gt;

&lt;p&gt;仔细思考这个问题就是：Parse multiple json objects that are in one line。 搜索之，发现了&lt;a href=&#34;http://stackoverflow.com/questions/36967236/parse-multiple-json-objects-that-are-in-one-line&#34; target=&#34;_blank&#34;&gt;http://stackoverflow.com/questions/36967236/parse-multiple-json-objects-that-are-in-one-line&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;其中，Francesco的解决方法比较高效：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;f = &#39;/Users/chengjun/百度云同步盘/githubarchive/2012-03-10-22.json.gz&#39;
f = gzip.open(f, &#39;rb&#39;)
f = f.readline()
r = re.split(&#39;(\{.*?\})(?= *\{)&#39;, f)
accumulator = &#39;&#39;
res = []
for subs in r:
    accumulator += subs
    try:
        res.append(json.loads(accumulator))
        accumulator = &#39;&#39;
    except:
        pass
len(res)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Out [29]: 1270&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;基于这种方法，可以写一个函数来实现对于数据的正确读取。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#f = &#39;/Users/chengjun/百度云同步盘/test.json&#39;
#f = open(f)

f = &#39;/Users/chengjun/百度云同步盘/githubarchive/2012-06-01-15.json.gz&#39;
f = gzip.open(f, &#39;rb&#39;)
files = f.readlines()
length = len(files)
if  length &amp;gt; 1:
    print &#39;Correct: &#39; + str(length)
else:
    f2 = files[0]
    r = re.split(&#39;(\{.*?\})(?= *\{)&#39;, f2)
    r = [i for i in r if i] # delete the &#39;&#39;
    accumulator = &#39;&#39;
    acts = []
    for subs in r:
        accumulator += subs
        try:
            acts.append(json.loads(accumulator))
            accumulator = &#39;&#39;
        except Exception, e:
            print e
            pass
    print &#39;Wrong: &#39; + str(len(acts))
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Out [30]: Wrong: 4491&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>QAP检验：计算两个网络的关联</title>
      <link>https://chengjun.github.io/zh/post/cn/2013-08-04-qap-test-of-network-analysis/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chengjun.github.io/zh/post/cn/2013-08-04-qap-test-of-network-analysis/</guid>
      <description>

&lt;h3 id=&#34;qap检验-两个网络之间的关联&#34;&gt;QAP检验：两个网络之间的关联&lt;/h3&gt;

&lt;p&gt;通常一组个体具有多种类型的关系，例如友谊关系和经济往来关系。我们通常会对这两种网络关系在多大程度上相互关联感兴趣。当我们知道一组个体之间的两种关系网络，我们就可以计算这个两个关系网络之间的相关程度。在统计学当中，皮尔森相关系数是用来反映两个变量线性相关程度的统计量。与之类似，对于由一组个体所组成的两个网络，也可以计算其相应的相关皮尔逊相关系数。当然，还可以计算其他你感兴趣的统计量，如协相关系数。&lt;/p&gt;

&lt;p&gt;我们使用sna这个R软件包来计算网络相关系数（并调用qaptest命令）。通过安装和使用statnet这个R软件包，就会自动加载sna等子软件包。另外，statnet当中还集成了其他的几个相关的R软件包，包括进行动态网络建模的tergm子软件包。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# R程序11-8：计算网络的皮尔逊相关系数
install.packages(&amp;quot;statnet&amp;quot;)
library(statnet)  
# 首先随机生成3个由10个节点构成的有向网络
g=array(dim=c(3,10,10))
g[1,,] = rgraph(10) 
g[2,,] = rgraph(10,tprob=g[1,,]*0.8) # 设置g1和g2两个网络强相关
g[3,,] = 1; g[3,1,2] = 0 # g3接近于一个派系（clique）
# 绘制这3个网络
par(mfrow=c(1,3))
for(i in 1:3) {
    gplot(g[i,,],usecurv=TRUE, mode = &amp;quot;fruchtermanreingold&amp;quot;, 
          vertex.sides=3:8)}
#计算网络的相关矩阵
gcor(g)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在通常使用皮尔逊相关系数的时候，可以用t统计量对总体相关系数为0的原假设进行检验。但在计算网络的相关系数（graph correlations）时，经典的零假设检验方法往往会带来偏差，因而并不适用。通常使用非参数检验的方法，比如QAP(Quadratic Assignment Procedure)检验。&lt;/p&gt;

&lt;p&gt;矩阵的随机排列（Random matrix permutations）是QAP检验的关键部分，在子软件包sna中主要通过rmperm来进行。通过矩阵的随机排列，可以对网络中的节点编号（而不是链接！！）进行随机置换（relabelling）或重新“洗牌”（reshuffling），并得到&lt;strong&gt;一组&lt;/strong&gt;（比如1000个）重连后的网络。因为只是置换节点，这种操作只是重新标记节点的编号（relabelling）。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# R程序11-9：矩阵的随机置换方法
j = rgraph(5) # 随机生成一个网络
j  #看一下这个网络的矩阵形式
rmperm(j) #随机置换后的网络的矩阵形式
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对这一组重构的网络可以计算其网络级别的参数（如两个网络的相关参数，协相关参数），并因此得到&lt;strong&gt;一个参数分布&lt;/strong&gt;。QAP检验的零假设是实际观测到的网络参数（如）来自于这&lt;strong&gt;一个参数分布&lt;/strong&gt;。也就是说，原假设认为这种观测到的相关关系是由随机因素带来的，因而这种网络相关并不显著。拒绝原假设，就从统计的角度证明了观测到的网络相关系数是显著的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# R程序11-10：QAP检验
q.12 = qaptest(g, gcor, g1 = 1, g2 = 2)
q.13 = qaptest(g, gcor, g1 = 1, g2 = 3)   
# 看一下QAP输出的结果
par(mfrow=c(1,2))
summary(q.12)
plot(q.12)
# 拒绝原假设，图1和图2显著相关
summary(q.13)
plot(q.13)
# 接受原假设，图1和图3不相关
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在检验这个关于两个网络是否存在相关的零假设的时候，我们计算置换后的参数分布中大于这个实际观测到的参数的比例，以及小于这个实际观测到的参数的比例。QAP检验返回实际数据中观测到的参数f(d)、通过置换所得到的参数f(perm)的数学分布、以及单尾的P值。其中单尾的p值包括两种情况：p(f(perm) &amp;gt;= f(d))和p(f(perm) &amp;lt;= f(d))。&lt;/p&gt;

&lt;p&gt;其中P(f(perm) &amp;gt;= f(d))表示随机置换矩阵的相关系数的大于与等于观测值的p值，也就是本研究的检验显著性。一般而言，当p(f(perm) &amp;gt;= f(d))小于p(f(perm) &amp;lt;= f(d))时，拒绝原假设。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>“众说纷纭”抑或“一言九鼎”？——以卡扎菲官邸攻陷事件在新浪微博上的信息扩散为例</title>
      <link>https://chengjun.github.io/zh/post/cn/2011-08-22-gaddafi-diffusion/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chengjun.github.io/zh/post/cn/2011-08-22-gaddafi-diffusion/</guid>
      <description>&lt;p&gt;本文载于《传媒透视》&lt;/p&gt;

&lt;p&gt;作者：王成军、张昕之 （作者皆为 香港城市大学 媒体与传播系 博士候选人）&lt;/p&gt;

&lt;p&gt;2011年8月22日，利比亚反对派军队攻占卡扎菲官邸。初传卡扎菲被击毙，后又有消息称，卡扎菲实已经密道逃亡，截止本文完成之时（8月27日），仍无其最新下落。&lt;/p&gt;

&lt;p&gt;如今微博已成为普通受众获知新闻、分享内容、沟通友人的重要渠道。利比亚内战一事在微博上迅速成为热门话题。然而微博用户众多，每个人的职业、地域、社会经济地位、社会网络、认知水平乃至动机，均有参差。尽管每个人用户都看似卷入了对重大事件的讨论，但是其发言的“质量”（如逻辑、可信度等）值得商榷。同时，微博有“V认证”以标识媒体工作者、社会名流、公共知识分子等人。这些人或因粉丝众多、或因观点犀利、或因独家爆料，其发言常被众多用户转发或评论，比起普通用户的“众说纷纭”，可谓“一言九鼎”。那么，事件发生后，在一个特定的微博用户群中，究竟是每个人平等参与了新闻的讨论和扩散抑或仅有几个活跃用户推进参与讨论的过程？新闻的信源与信息发布者之间、活跃用户与其粉丝之间、粉丝与粉丝之间——的互动关系在某个特定时间段表现为何种形式？&lt;/p&gt;

&lt;p&gt;为回答以上问题，作为一个探索研究，本文采用新浪微博API，抓取短短4分钟内（2011/8/22 15:06—15:10）所有新浪微博上对于卡扎菲官邸攻陷事件的信息发布、转发、评论，共计738条微博。四分钟的信息流，处于信息扩散阶段的一个短暂的瞬间，因而无法窥见整体层面的信息扩散规律。但因所采集的数据为随着时间有规律增长的宏大信息流的片段，同样可能起到窥一斑而见全豹的目的，其独特意义在于：海量数据流给信息处理和网络抽样带来巨大挑战，面对庞大的关系数据，传统统计方法不再适用；了解瞬间的数据流的特点是理解海量信息流的基础。本文从“瞬间数据流”的整体（而非样本）出发，探究其内在特点，可为网络舆论研究提供新视角。抛开技术的细枝末节，我们有四个重要发现:&lt;/p&gt;

&lt;p&gt;##发现一：自发性信息推荐群体的出现：从“自说自话”到“信息推荐”
738条微博信息中，转发的内容达262条，占三成，剩余约三分之二为原创新内容（476条）。主动的受众积极地转载来自其他渠道的信息。例如，其中有152条微博带有网络链接，30条提及电视台，26条提及路透社，23条提及齐鲁晚报。这表明有一个活跃的、自发性的“信息推荐群体”存在着，他们看似“自言自语”的原创微博发挥着信息推荐的作用。&lt;/p&gt;

&lt;p&gt;##发现二： “一言九鼎”的“舆论领袖”
在短短4分钟内，就有44条帖子被转发262次，平均每条帖子被转发6次。最多的一条帖子——华尔街日报关于卡扎菲倒、油价可能上涨的消息,在四分钟内被转发了43次；该消息发布于本研究抓取数据一个小时前的14:18， 一个小时中累计被转发290次，被评论105次。四分钟内转发数量超过9条的微博客用户，主要为媒体、媒体评论人，几乎全是加V的新浪认证用户（只有 “愤青3D”例外），并具有超过百万的粉丝数量。转发情况如图1。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm4.staticflickr.com/3679/12744680075_2d9d4fbbcc_o.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;图 1 微博用户及其微博被转发次数（Top 10）&lt;/p&gt;

&lt;p&gt;##发现三：活跃的信息转发者：多次转发同一条信息的个体
本文分析了信息转发的网络关系，该有向网络由99个节点构成，拥有262条边，然而出乎意料的是这仅仅构成了一个稀疏的信息扩散网络：该网络的网络密度仅为0.027。网络密度衡量的是网络中真实存在的边与可能存在的边的数量的比值，本转发网络密度(0.027)低于期望值（0.054），说明有些节点之间存在多条链接，即某一个特定人物发布的某一条重要的信息，被“同一个转发者”多次转发。比如新闻评论人李承鹏所发的一条微博在4分钟内被转发15次。这15次并非被15个人“一人转发一次”，而是其中被一个ID为“古火拉兹-五毛”的人转发高达9次之上，剩下的几次由另两个ID（“慧-丽” 和“跪下去求婚站起来演说”）所转发。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm6.staticflickr.com/5515/12745135864_4561dbd471_o.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;图 2  微博转发网络（箭头代表信息流动方向）&lt;/p&gt;

&lt;p&gt;##发现四: 原创型微博和被转发微博的语义网络：经济的还是政治的？
上文中我们提到了两类微博：一是“推荐者”们原创的微博，二是意见领袖被转发的微博。为粗略地比较两类微博内容的区别，本文对这四分钟内抽取的微博内容，通过提取高频词，构建语义网络进行分析。发现：两类微博除了都关注新闻事件本身的基本经过、“5W”之外，“原创”微博关注政治及社会自由方面（图3）， 而社会名人被转发的微博主要针对新闻事件对经济的影响（图4）。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm4.staticflickr.com/3830/12744833063_a14400925f_o.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;图 3原创型微博的语义网络&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm8.staticflickr.com/7423/12744697025_08edbc3340_o.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;图 4 被转发微博的语义网络&lt;/p&gt;

&lt;p&gt;##总结
本例中，微博资讯的传播和扩散主要始于少数的意见领袖，跨国大型新闻机构亦是主要消息来源。微博时代，传统媒体如果善用网络平台仍可成为主要的信息来源。作为信息聚合作用的草根媒体开始出现，并扮演对传统媒体之信息进行过滤的把关人角色。此外，一些知名的草根ID通过自身的独特定位和宣传包装技巧，同样开始广泛的发挥舆论领袖的作用。值得注意的是，公共讨论和自言自语相互结合的原创类型微博占到了信息流的主体。“自媒体”从“自广播”开始起步，普通的微博使用者开始运用微博转载新闻、表明态度，使得微博成为孕育公共讨论的新机制。这种底层的广泛的信息转载虽然在四分钟内未获得转发，但在其各自子群中，都扮演着信息推荐者的角色。&lt;/p&gt;

&lt;p&gt;最后，作为一个探索性的研究，后续研究仍需要大量的文献梳理和技术支持。未来可以考量更为漫长的时间段内信息的扩散过程。另外，对制度性的不同话语主体之间（如政府、媒介、国际机构等）的互动，不同类型事件、乃至国内（敏感新闻、突发新闻更甚）新闻的扩散机制——凡此种种，均可成为后续研究之鉴。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>东风夜放花千树：对宋词进行主题分析初探</title>
      <link>https://chengjun.github.io/zh/post/cn/2013-09-27-topic-modeling-of-song-peom/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chengjun.github.io/zh/post/cn/2013-09-27-topic-modeling-of-song-peom/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://forum.eedu.org.cn/upload/2008/02/27/90815055.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;邱怡轩在统计之都中展示了对宋词进行的分析（参见&lt;a href=&#34;http://cos.name/tag/%E5%AE%8B%E8%AF%8D/），因为当时缺乏中文分词的工具，他独辟蹊径，假设宋词中任意两个相邻的汉字构成一个词语，进而找到了宋词当中的高频词。本文则尝试使用他所提供的宋词语料（http://cos.name/wp-content/uploads/2011/03/SongPoem.tar.gz），分析一下使用R进行中文分词、构建词云、高频词语聚类以及主题模型分析。&#34; target=&#34;_blank&#34;&gt;http://cos.name/tag/%E5%AE%8B%E8%AF%8D/），因为当时缺乏中文分词的工具，他独辟蹊径，假设宋词中任意两个相邻的汉字构成一个词语，进而找到了宋词当中的高频词。本文则尝试使用他所提供的宋词语料（http://cos.name/wp-content/uploads/2011/03/SongPoem.tar.gz），分析一下使用R进行中文分词、构建词云、高频词语聚类以及主题模型分析。&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;首先要载入使用的R包并读入数据。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;library(Rwordseg)
require(rJava)
library(tm)
library(slam)
library(topicmodels)
library(wordcloud)
library(igraph)
setwd(&amp;quot;D:/github/text mining/song&amp;quot;) # 更改为你的工作路径，并存放数据在此。
txt=read.csv(&amp;quot;SongPoem.csv&amp;quot;,colClasses=&amp;quot;character&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{:lang=&amp;ldquo;ruby&amp;rdquo;}&lt;/p&gt;

&lt;p&gt;然后进行对数据的操作。当然，第一步是进行中文分词，主要使用Rwordseg这个R包，其分词效果不错。分词的过程可以自动去掉标点符号。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;poem_words &amp;lt;- lapply(1:length(txt$Sentence), function(i) segmentCN(txt$Sentence[i], nature = TRUE))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{:lang=&amp;ldquo;ruby&amp;rdquo;}&lt;/p&gt;

&lt;p&gt;然后，我们将数据通过tm这个R包转化为文本-词矩阵（DocumentTermMatrix）。
    wordcorpus &amp;lt;- Corpus(VectorSource(poem_words), encoding = &amp;ldquo;UTF-8&amp;rdquo;) # 组成语料库格式&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Sys.setlocale(locale=&amp;quot;Chinese&amp;quot;)
dtm1 &amp;lt;- DocumentTermMatrix(wordcorpus,
                          control = list(
                            wordLengths=c(1, Inf), # to allow long words
                            bounds = list(global = c(5,Inf)), # each term appears in at least 5 docs
                            removeNumbers = TRUE, 
                            # removePunctuation  = list(preserve_intra_word_dashes = FALSE),
                            weighting = weightTf, 
                            encoding = &amp;quot;UTF-8&amp;quot;)
)

colnames(dtm1)
findFreqTerms(dtm1, 1000) # 看一下高频词
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{:lang=&amp;ldquo;ruby&amp;rdquo;}&lt;/p&gt;

&lt;p&gt;这里需要注意的是，这里我们默认词语的长度为1到无穷大，稍后，我们可以对其长度进行修改。例如，本文中，作者对改为长度为2以上以及长度为3以上，分别得到另外两个文本-词矩阵：dtm2和dtm3。随后，我们可以在文本-词矩阵进行一系列的分析。这里，先做一个简单的词云分析。为更好展示效果，最多只列出100个词。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;m &amp;lt;- as.matrix(dtm1)
v &amp;lt;- sort(colSums(m), decreasing=TRUE)
myNames &amp;lt;- names(v)
d &amp;lt;- data.frame(word=myNames, freq=v)
par(mar = rep(2, 4))

png(paste(getwd(), &amp;quot;/wordcloud50_&amp;quot;,  &amp;quot;.png&amp;quot;, sep = &#39;&#39;), 
    width=10, height=10, 
    units=&amp;quot;in&amp;quot;, res=700)

pal2 &amp;lt;- brewer.pal(8,&amp;quot;Dark2&amp;quot;)
wordcloud(d$word,d$freq, scale=c(5,.2), min.freq=mean(d$freq),
          max.words=100, random.order=FALSE, rot.per=.15, colors=pal2)
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{:lang=&amp;ldquo;ruby&amp;rdquo;}&lt;/p&gt;

&lt;p&gt;我们可以看一下效果，如下图所示，主要是一个字长度的词。最多的是“人”和“不”， 然后是“春”和“花”。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm6.staticflickr.com/5480/9953518693_004590d6e3_z.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;但我们也许对长度大于2的词以及长度大于3的词更感兴趣（同时这样也可以和邱怡轩做的结果做一下比较）。使用前面步骤中生成的dtm2重复构建词云的R程序，我们可以得到以下两个词云：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm3.staticflickr.com/2821/9953413634_9a0d9020f7_z.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;同样，对dtm3构建词云，效果如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm8.staticflickr.com/7442/9953417766_8e6f160461_z.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;以上结果和邱怡轩得到的结果类似，可见对高频词的处理方面，他的方法的确有创见。对词云分析之后，我们还可以尝试根据词与词之间共同出现的概率对词进行聚类。这里我们展示长度大于2的词的聚类结果。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dtm01 &amp;lt;- weightTfIdf(dtm2)
N = 0.9
dtm02 &amp;lt;- removeSparseTerms(dtm01, N);dtm02
# 注意，为展示方便，这里我调节N的大小，使得dtm02中的词语数量在50左右。    
tdm = as.TermDocumentMatrix(dtm02)
tdm &amp;lt;- weightTfIdf(tdm)
# convert the sparse term-document matrix to a standard data frame
mydata.df &amp;lt;- as.data.frame(inspect(tdm))
mydata.df.scale &amp;lt;- scale(mydata.df)
d &amp;lt;- dist(mydata.df.scale, method = &amp;quot;euclidean&amp;quot;) # distance matrix
fit &amp;lt;- hclust(d, method=&amp;quot;ward&amp;quot;)

png(paste(&amp;quot;d:/chengjun/honglou/honglou_termcluster_50_&amp;quot;, &amp;quot;.png&amp;quot;, sep = &#39;&#39;), 
    width=10, height=10, 
    units=&amp;quot;in&amp;quot;, res=700)
plot(fit) # display dendogram?
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{:lang=&amp;ldquo;ruby&amp;rdquo;}&lt;/p&gt;

&lt;p&gt;对长度大于2的词的聚类结果如下图所示，可见宋词的确注重“风流倜傥”，连分类都和风向有关系。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm3.staticflickr.com/2875/9953397625_abdc6e9874_z.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;当然读者还可以尝试对长度大于1词和长度大于3的词聚类，对长度大于1词聚类的效果图如下所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm6.staticflickr.com/5548/9953397345_05a8944455_z.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;长度大于3的词聚类结果如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm8.staticflickr.com/7375/9953419256_f80a00e9e8_z.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;完成这一步之后，作者还尝试对宋词进行简单的主题模型分析，首先还是从长度大于2的词开始吧。第一步是确定主题的数量。先对文本-词矩阵进行简单处理，以消除高频词被高估和低频词被低估的问题。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dtm = dtm2
term_tfidf &amp;lt;-tapply(dtm$v/row_sums(dtm)[dtm$i], dtm$j, mean) * log2(nDocs(dtm)/col_sums(dtm &amp;gt; 0))
l1=term_tfidf &amp;gt;= quantile(term_tfidf, 0.5)       # second quantile, ie. median
dtm &amp;lt;- dtm[,l1]
dtm = dtm[row_sums(dtm)&amp;gt;0, ]; dim(dtm) # 2246 6210
summary(col_sums(dtm))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{:lang=&amp;ldquo;ruby&amp;rdquo;}&lt;/p&gt;

&lt;p&gt;之后就可以正式地开始确定主题数量的R程序了。这里，笔者主要参考了朱雪宁《微博名人那些事儿》一文中的R程序（&lt;a href=&#34;http://cos.name/2013/08/something_about_weibo/）。&#34; target=&#34;_blank&#34;&gt;http://cos.name/2013/08/something_about_weibo/）。&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fold_num = 10
kv_num =  c(5, 10*c(1:5, 10))
seed_num = 2003
try_num = 1

smp&amp;lt;-function(cross=fold_num,n,seed)
{
  set.seed(seed)
  dd=list()
  aa0=sample(rep(1:cross,ceiling(n/cross))[1:n],n)
  for (i in 1:cross) dd[[i]]=(1:n)[aa0==i]
  return(dd)
}

selectK&amp;lt;-function(dtm,kv=kv_num,SEED=seed_num,cross=fold_num,sp) # change 60 to 15
{
  per_ctm=NULL
  log_ctm=NULL
  for (k in kv)
  {
    per=NULL
    loglik=NULL
    for (i in 1:try_num)  #only run for 3 replications# 
    {
      cat(&amp;quot;R is running for&amp;quot;, &amp;quot;topic&amp;quot;, k, &amp;quot;fold&amp;quot;, i,
          as.character(as.POSIXlt(Sys.time(), &amp;quot;Asia/Shanghai&amp;quot;)),&amp;quot;\n&amp;quot;)
      te=sp[[i]]
      tr=setdiff(1:dtm$nrow, te) # setdiff(nrow(dtm),te)  ## fix here when restart r session

      # VEM = LDA(dtm[tr, ], k = k, control = list(seed = SEED)),
      # VEM_fixed = LDA(dtm[tr,], k = k, control = list(estimate.alpha = FALSE, seed = SEED)),

      #       CTM = CTM(dtm[tr,], k = k, 
      #                 control = list(seed = SEED, var = list(tol = 10^-4), em = list(tol = 10^-3)))  
      #       
      Gibbs = LDA(dtm[tr,], k = k, method = &amp;quot;Gibbs&amp;quot;,
                  control = list(seed = SEED, burnin = 1000,thin = 100, iter = 1000))

      per=c(per,perplexity(Gibbs,newdata=dtm[te,]))
      loglik=c(loglik,logLik(Gibbs,newdata=dtm[te,]))
    }
    per_ctm=rbind(per_ctm,per)
    log_ctm=rbind(log_ctm,loglik)
  }
  return(list(perplex=per_ctm,loglik=log_ctm))
}

sp=smp(n=dtm$nrow, seed=seed_num) # n = nrow(dtm)

system.time((ctmK=selectK(dtm=dtm,kv=kv_num,SEED=seed_num,cross=fold_num,sp=sp)))

## plot the perplexity

m_per=apply(ctmK[[1]],1,mean)
m_log=apply(ctmK[[2]],1,mean)

k=c(kv_num)
df = ctmK[[1]]  # perplexity matrix
logLik = ctmK[[2]]  # perplexity matrix


write.csv(data.frame(k, df, logLik), paste(getwd(), &amp;quot;/Perplexity2_&amp;quot;,&amp;quot;gibbs5_100&amp;quot;, &amp;quot;.csv&amp;quot;, sep = &amp;quot;&amp;quot;))

# save the figure
png(paste(getwd(), &amp;quot;/Perplexity2_&amp;quot;,try_num, &amp;quot;_gibbs5_100&amp;quot;,&amp;quot;.png&amp;quot;, sep = &#39;&#39;), 
    width=5, height=5, 
    units=&amp;quot;in&amp;quot;, res=700)


matplot(k, df, type = c(&amp;quot;b&amp;quot;), xlab = &amp;quot;Number of topics&amp;quot;, 
        ylab = &amp;quot;Perplexity&amp;quot;, pch=1:try_num,col = 1, main = &#39;&#39;)       
legend(&amp;quot;topright&amp;quot;, legend = paste(&amp;quot;fold&amp;quot;, 1:try_num), col=1, pch=1:try_num) 

dev.off()

png(paste(getwd(), &amp;quot;/LogLikelihood2_&amp;quot;, &amp;quot;gibbs5_100&amp;quot;,&amp;quot;.png&amp;quot;, sep = &#39;&#39;), 
    width=5, height=5, 
    units=&amp;quot;in&amp;quot;, res=700)
matplot(k, logLik, type = c(&amp;quot;b&amp;quot;), xlab = &amp;quot;Number of topics&amp;quot;, 
        ylab = &amp;quot;Log-Likelihood&amp;quot;, pch=1:try_num,col = 1, main = &#39;&#39;)       
legend(&amp;quot;topright&amp;quot;, legend = paste(&amp;quot;fold&amp;quot;, 1:try_num), col=1, pch=1:try_num) 
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{:lang=&amp;ldquo;ruby&amp;rdquo;}&lt;/p&gt;

&lt;p&gt;于是可以得到对数似然率和主题数量的关系图，如下所示。可见选择10作为主题数量是比较合适的。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm4.staticflickr.com/3727/9953452316_6518296960.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;以下，我们将主要对主题数量为10的主题模型进行估计。topicmodels这个R包是由Bettina Grun和
Johannes Kepler两个人贡献的，目前支持VEM, VEM (fixed alpha)，Gibbs和CTM四种主题模型，关于其详细介绍，可以阅读他们的论文，关于主题模型的更多背景知识可以阅读Blei的相关文章。闲话少叙，看一下R代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# &#39;Refer to http://cos.name/2013/08/something_about_weibo/&#39;
k = 10
SEED &amp;lt;- 2003
jss_TM2 &amp;lt;- list(
  VEM = LDA(dtm, k = k, control = list(seed = SEED)),
  VEM_fixed = LDA(dtm, k = k, control = list(estimate.alpha = FALSE, seed = SEED)),
  Gibbs = LDA(dtm, k = k, method = &amp;quot;Gibbs&amp;quot;, 
              control = list(seed = SEED, burnin = 1000, thin = 100, iter = 1000)),
  CTM = CTM(dtm, k = k, 
            control = list(seed = SEED, var = list(tol = 10^-4), em = list(tol = 10^-3))) )   
save(jss_TM2, file = paste(getwd(), &amp;quot;/jss_TM2.Rdata&amp;quot;, sep = &amp;quot;&amp;quot;))
save(jss_TM, file = paste(getwd(), &amp;quot;/jss_TM1.Rdata&amp;quot;, sep = &amp;quot;&amp;quot;))

termsForSave1&amp;lt;- terms(jss_TM2[[&amp;quot;VEM&amp;quot;]], 10)
termsForSave2&amp;lt;- terms(jss_TM2[[&amp;quot;VEM_fixed&amp;quot;]], 10)
termsForSave3&amp;lt;- terms(jss_TM2[[&amp;quot;Gibbs&amp;quot;]], 10)
termsForSave4&amp;lt;- terms(jss_TM2[[&amp;quot;CTM&amp;quot;]], 10)

write.csv(as.data.frame(t(termsForSave1)), 
          paste(getwd(), &amp;quot;/topic-document_&amp;quot;, &amp;quot;_VEM_&amp;quot;, k, &amp;quot;_2.csv&amp;quot;, sep=&amp;quot;&amp;quot;),
          fileEncoding = &amp;quot;UTF-8&amp;quot;)

write.csv(as.data.frame(t(termsForSave2)), 
          paste(getwd(), &amp;quot;/topic-document_&amp;quot;, &amp;quot;_VEM_fixed_&amp;quot;, k, &amp;quot;_2.csv&amp;quot;, sep=&amp;quot;&amp;quot;),
          fileEncoding = &amp;quot;UTF-8&amp;quot;)

write.csv(as.data.frame(t(termsForSave3)), 
          paste(getwd(), &amp;quot;/topic-document_&amp;quot;, &amp;quot;_Gibbs_&amp;quot;, k, &amp;quot;_2.csv&amp;quot;, sep=&amp;quot;&amp;quot;),
          fileEncoding = &amp;quot;UTF-8&amp;quot;)
write.csv(as.data.frame(t(termsForSave4)), 
          paste(getwd(), &amp;quot;/topic-document_&amp;quot;, &amp;quot;_CTM_&amp;quot;, k, &amp;quot;_2.csv&amp;quot;, sep=&amp;quot;&amp;quot;),
          fileEncoding = &amp;quot;UTF-8&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{:lang=&amp;ldquo;ruby&amp;rdquo;}&lt;/p&gt;

&lt;p&gt;对主题模型进行估计之后，一般选择展示每个主题的前10个词语。因为主题之间可以共享相同词语，所以构成网络关系，因此这里我选择用网络的方法展示其结果。首先看一下吉布斯抽样算法得到的主题网络图，其R程序如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#&#39;topic graphs&#39;


tfs = as.data.frame(termsForSave3, stringsAsFactors = F); tfs[,1]
adjacent_list = lapply(1:10, function(i) embed(tfs[,i], 2)[, 2:1]) 
edgelist = as.data.frame(do.call(rbind, adjacent_list), stringsAsFactors =F)
# topic = unlist(lapply(1:10, function(i) rep(i, 9)))
edgelist$topic = topic
g &amp;lt;-graph.data.frame(edgelist,directed=T )
l&amp;lt;-layout.fruchterman.reingold(g)
# edge.color=&amp;quot;black&amp;quot;
nodesize = centralization.degree(g)$res 
V(g)$size = log( centralization.degree(g)$res )

nodeLabel = V(g)$name
E(g)$color =  unlist(lapply(sample(colors()[26:137], 10), function(i) rep(i, 9))); unique(E(g)$color)

# 保存图片格式
png(  paste(getwd(), &amp;quot;/topic_graph_gibbs.png&amp;quot;, sep=&amp;quot;&amp;quot;）,
    width=5, height=5, 
    units=&amp;quot;in&amp;quot;, res=700)

plot(g, vertex.label= nodeLabel,  edge.curved=TRUE, 
     vertex.label.cex =0.5,  edge.arrow.size=0.2, layout=l )

# 结束保存图片
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{:lang=&amp;ldquo;ruby&amp;rdquo;}&lt;/p&gt;

&lt;p&gt;得到的图形如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm8.staticflickr.com/7330/9953452256_fb1baaa16d_c.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;当然，我们还可以看一下其他算法得到的网络图，这里我们看一下根据VEM和CTM两个主题模型得到的网络图。&lt;/p&gt;

&lt;p&gt;CTM模型的主题网络图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm8.staticflickr.com/7425/9953448564_d24b8e348a_c.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;VEM模型的主题网络图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm4.staticflickr.com/3727/9953448704_ec8291a0e3_c.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>中国高校间的友谊网络：大学排名、地理位置和演化规律</title>
      <link>https://chengjun.github.io/zh/post/cn/2014-07-21-chinese-university-friendship-network/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chengjun.github.io/zh/post/cn/2014-07-21-chinese-university-friendship-network/</guid>
      <description>&lt;p&gt;许小可老师和我决定使用可视化的方法分析一下中国高校之间的友谊关系网络。我们是想通过这个图从一个侧面说明各个大学在社交网络上的影响力和关系：所以节点大小表示每个大学在该网络中的友谊关系数量，连边宽度表示节点之间的连接关系，节点颜色的不同可以表示节点的影响力大小（介度中心性指标）。&lt;/p&gt;

&lt;p&gt;###数据清洗
现有数据有学校信息。我可以通过构建字典的方式来将user_id和学校信息（以最新的学校信息为主）剥离出来。使用这个字典可以计算学校人数的分布并挑选前一百的学校。然后根据user_id来match社交网络数据和学校数据，构建：university1&amp;mdash;university2&amp;mdash;date的数据形式。如果该行数据中的学校都在top100的名单中，则保留，否则不保留，这样可以构建所需要的学校和学校的随时间变化的网络，并采用考虑连边权重的贪婪算法来划分网络社团。&lt;/p&gt;

&lt;p&gt;###数据分析&lt;/p&gt;

&lt;p&gt;数据清洗之后，使用R软件进行数据分析，使用igraph包进行数据的可视化。代码如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;library(igraph)
setwd(&amp;quot;F:/xiaonei/&amp;quot;)

################
# all data
################
data = read.table(&amp;quot;./friends_university_top100_by_all.txt&amp;quot;, header = FALSE, 
                 sep = &#39;\t&#39;, stringsAsFactors = FALSE)

data = data[which(data[,3] &amp;gt;= mean(data[,3])*1.2), ]
data = data[which(data[,1] != data[,2]),]
g =graph.data.frame(data[,1:2],directed=FALSE )
E(g)$weight = data[,3]
E(g)$color = &amp;quot;lightgrey&amp;quot;
# layout
set.seed(34)   ## to make this reproducable
l=layout.fruchterman.reingold(g)
# size
nodeSize = graph.strength(g)
V(g)$size = (nodeSize - min(nodeSize))/(max(nodeSize) - min(nodeSize))*20
centrality = betweenness(g)
# colors
colors = heat.colors(37)
position = rank(-centrality, ties.method = &amp;quot;first&amp;quot;)
V(g)$color = colors[position] 
# width
E(g)$width = (log(E(g)$weight)- 8)*1.5
# label
nodeLabel = V(g)$name
V(g)$label.cex = log(centrality+1)/20 + 0.5
V(g)$label.color = &amp;quot;black&amp;quot;
# community detection
fc = fastgreedy.community(g); sizes(fc)
mfc = membership(fc)
# plot
drawFigure = function(g){
  plot(g, vertex.label= nodeLabel,  
       edge.curved = FALSE, vertex.frame.color=&amp;quot;#FFFFFF&amp;quot;,
       layout=l,mark.groups = by(seq_along(mfc), mfc, invisible) )
}

drawFigure(g) 

# save png
png(&amp;quot;./all_color.png&amp;quot;,
    width=10, height=10, 
    units=&amp;quot;in&amp;quot;, res=700)
drawFigure(g) 
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://chengjun.qiniudn.com/all_color.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;数据的可视化表明：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;学校间的友谊关系的建立取决于学校的排名 （排名越靠前的学校的网络中心性较高）&lt;/li&gt;
&lt;li&gt;同一个省的学校之间存在更多的友谊关系 （存在地理上的proximity）&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;还可以根据月份来可视化，看一下2006年一月的情况吧：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://chengjun.qiniudn.com/month_color_%201.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;使用R生成24个月的图片，使用&lt;a href=&#34;http://makeagif.com/&#34; target=&#34;_blank&#34;&gt;makeagif&lt;/a&gt;生成GIF动态图片：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://cdn.makeagif.com/media/8-07-2014/iuOvDr.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;显然：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;最早加入这个社交网络的是北京的几所著名高校；&lt;/li&gt;
&lt;li&gt;2006年4月才走出北京；&lt;/li&gt;
&lt;li&gt;随后友谊关系开始在全国扩张；&lt;/li&gt;
&lt;li&gt;扩张的过程围绕着那些著名高校为中心进行。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;本文提供&lt;a href=&#34;http://chengjun.qiniudn.com/friends_university_top100_by_all.txt&#34; target=&#34;_blank&#34;&gt;汇总的数据&lt;/a&gt;下载，供感兴趣的同学玩。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>互联网的法则：网络生态学的起点</title>
      <link>https://chengjun.github.io/zh/post/cn/2012-12-06-intenet-ecology/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chengjun.github.io/zh/post/cn/2012-12-06-intenet-ecology/</guid>
      <description>

&lt;p&gt;作者： 黠之大者（网名），王成军，香港城市大学博士（在读）&lt;/p&gt;

&lt;p&gt;本文载于《数字媒体阅读报告》&lt;/p&gt;

&lt;h2 id=&#34;the-laws-of-the-web&#34;&gt;The Laws of the Web&lt;/h2&gt;

&lt;h3 id=&#34;patterns-in-the-ecology-of-information&#34;&gt;Patterns in the Ecology of Information&lt;/h3&gt;

&lt;p&gt;&lt;img src=&#34;http://d202m5krfqbpi5.cloudfront.net/books/1347698642l/706406.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Bernardo A. Huberman (Author)&lt;/p&gt;

&lt;p&gt;Paperback: 115 pages&lt;/p&gt;

&lt;p&gt;Publisher: The MIT Press (April 1, 2001)&lt;/p&gt;

&lt;p&gt;Language: English&lt;/p&gt;

&lt;p&gt;ISBN-10: 0262582252&lt;/p&gt;

&lt;p&gt;伯纳德-胡伯曼是惠普实验室的科学家，同时是斯坦福大学应用物理系的顾问教授（Consulting Professor）。他培养了很多学生，其中比较有名的当属Lada Adamic。他们两个在1999年以来的关于互联网的研究使之成为网络科学研究的重要成员，也奠定了本书基本的架构。他的第一篇论文发表于1970年，可以说是一个常青树。直到现在，依然保持着较高的学术产量。&lt;/p&gt;

&lt;p&gt;浏览其发表的论文，会很明显得发现他从物理学转型到互联网的结构和动态的研究，而现在则主要关注注意力经济的研究。从Strong Regularities in World Wide Web Surfing （1998， 《科学》），Evolutionary Dynamics of the World Wide Web  （1999， 《自然》）等文章开始，胡伯曼的研究成为与其它主流研究者（如巴拉巴西）对话的重要人物，尤其是关于互联网的直径问题和互联网的增长机制问题 （读者可参见更好地一篇书评 ）。在其1998年的文章中，胡伯曼就提出了互联网的访问量满足幂律分布，而1999年的文章则表明互联网的结构同样满足幂律分布。这种普世的现象，胡伯曼将之称之为法则（law）。&lt;/p&gt;

&lt;p&gt;在讨论网络的演化与结构的过程中，胡伯曼强调关于市场中的个体的计划和策略的相吸信息不足以帮助我们理解一个市场的行为。主要是因为集体行为是互动的结果，单纯的个体信息只抓住了节点，而忽略的动态的互动。因此，追踪一些单个的个体的网页浏览行为也不能预测整体上的网络浏览规律。胡伯曼说，“我们必须放弃这些个体信息，而代之以更为整体的、系统水平的行为特征 ”（p. 23）。他将这种思路称之为整体的思路（the aggregate way），并认为这是一个强大的方法论，可以用来解决的大的分布系统，如股票市场、计算机网络、社会组织。不得不说，这是一种过于简略的思路。胡伯曼在这个问题上似乎对于微观的机制并没有太多兴趣，他执意要绕开从微观行为到宏观结果的涌现，而在系统层面讨论系统问题。恰因如此，他能够顺畅地讨论网络增长和网路规模这种系统水平的问题，而避开令人畏惧的个体行为。&lt;/p&gt;

&lt;p&gt;在等待牛顿的道路上，这对于普通研究者而言，不失为一种明智之举。却也显露出胡伯曼的局限。不过，这也是不绝对的，科学的道路，在发现法则之后，必然要走向背后的机制（mechanism）和普世的原理（principle）。物理学方法重视动力学方程的建立，他们以另外一种方式——数学和理论的途径面对微观的问题，但十分清楚这个从微观到宏观的过程不是简单的个体信息能顺利解决的。网络的另一特特征是小世界特性，胡伯曼指出目前的研究，尤其是巴拉巴西等人提出的优先链接机制不能较好的揭示网络的聚类特征，因此网络科学仍在寻找一个能够综合小世界特征和无标度特征的网络生成机制。其它几章讲解互联网阻塞、信息下载和互联网市场的问题，也多真知灼见。整体来看，这是一本非常简明的小书（正文只有95页）。作为较早的对网络科学研究的一个总结，值得读一下。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
