<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R on Academic</title>
    <link>https://chengjunwang.com/zh/tags/r/</link>
    <description>Recent content in R on Academic</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <copyright>&amp;copy; 2016 Cheng-Jun Wang</copyright>
    <lastBuildDate>Wed, 27 Aug 2014 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://chengjunwang.com/zh/tags/r/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>使用d3network做网络可视化</title>
      <link>https://chengjunwang.com/zh/post/cn/2014-08-27-d3network/</link>
      <pubDate>Wed, 27 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>https://chengjunwang.com/zh/post/cn/2014-08-27-d3network/</guid>
      <description>在之前的一个博客中，我介绍了使用R进行社区划分并可视化的方法。这里使用相同的数据，介绍如何使用d3network实现网络可视化的方法。
首先，安装d3network
devtools::install_github(&amp;quot;d3Network&amp;quot;, &amp;quot;christophergandrud&amp;quot;) require(d3Network)  之后可以使用简单的可视化方法：
d3SimpleNetwork(data[,1:2], file = &amp;quot;chinese_university100.html&amp;quot;, width = 1024, height = 763, fontsize = 12)  我想要展现社区划分的结果：
#链接数据 links = data names(links) = c(&amp;quot;source&amp;quot;, &amp;quot;target&amp;quot;, &amp;quot;value&amp;quot;) #节点列表 fc = fastgreedy.community(g); sizes(fc) mfc = membership(fc) nodes = data.frame(name = names(mfc), group = mfc) #对应链接数据和节点数据 ids = 0:(nrow(nodes)-1) # notice: start with zero! links[,1] = ids[match(links[,1], nodes$name )] links[,2] = ids[match(links[,2], nodes$name )] links = links[with(links, order(source)), ] # sort by source #处理边的权重大小 links$value = log(links$value)  之后就可以使实现可视化结果啦：</description>
    </item>
    
    <item>
      <title>空间分析初步：空间点类型分析</title>
      <link>https://chengjunwang.com/zh/post/cn/2014-03-12-first-step-spatial-analysis-with-r/</link>
      <pubDate>Wed, 12 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>https://chengjunwang.com/zh/post/cn/2014-03-12-first-step-spatial-analysis-with-r/</guid>
      <description>引言 空间分析（spatial analysis）对于扩散研究非常重要，它揭示了传播在空间维度上的分布。令人略感惊奇的是空间分析的研究者越来越多地使用R软件。其中一个原因是R包罗万象，而空间分析仍在发展且神情未定。在这个时候难以判定哪种方法最优。此时，策略当然是博观约取。R因其囊括众多统计方法而成为连接不同分析套路的首选；另外在R当中使用者可以继续开发新的数据分析包。可谓一举两得。　数据读入 我使用的是2013年米兰城12月份推特用户的地理信息数据。该数据来自Big Data Challenge of Telecommunication。使用Python写很简单的script从其服务器api接口读取数据:
# Download milano tweets data using python # chengjun wang @ cmc # 2014 Mar 11 import urllib2 import json f = open(&#39;D:/chengjun/Milan/Social pulse/Milano_sample.csv&#39;, &#39;w&#39;) for offset in range(0,269290/100 +1): print &amp;quot;working on offset: &amp;quot;, offset req_url = &#39;https://api.dandelion.eu/datagem/social-pulse-milano/data/v1/?$limit=100&amp;amp;$offset=&#39;+str(offset)+&#39;&amp;amp;$app_id=d...a&amp;amp;$app_key=2e...7c&#39; jstr = urllib2.urlopen(req_url).read() # json string &amp;quot;&amp;quot;&amp;quot; these are flickr-specific &amp;quot;&amp;quot;&amp;quot; jinfo = json.loads( jstr ) for i in range(0, len(jinfo[&#39;items&#39;])): lan = jinfo[&#39;items&#39;][i][&#39;language&#39;] time = jinfo[&#39;items&#39;][i][&#39;created&#39;] geo = jinfo[&#39;items&#39;][i][&#39;geometry&#39;][&#39;coordinates&#39;] timestamp = jinfo[&#39;items&#39;][i][&#39;timestamp&#39;] municipality_name = jinfo[&#39;items&#39;][i][&#39;municipality&#39;][&#39;name&#39;] municipality_id = jinfo[&#39;items&#39;][i][&#39;municipality&#39;][&#39;acheneID&#39;] entities = jinfo[&#39;items&#39;][i][&#39;entities&#39;] user = jinfo[&#39;items&#39;][i][&#39;user&#39;] print &amp;gt;&amp;gt;f, &amp;quot;%s;%s;%s;%s;%s;%s;&#39;%s&#39;;%s&amp;quot; % (lan, time, geo, timestamp, municipality_name, municipality_id, entities, user) f.</description>
    </item>
    
    <item>
      <title>使用R模拟网络扩散</title>
      <link>https://chengjunwang.com/zh/post/cn/2014-02-28-simulate-network-diffusion-with-r/</link>
      <pubDate>Fri, 28 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>https://chengjunwang.com/zh/post/cn/2014-02-28-simulate-network-diffusion-with-r/</guid>
      <description>与普通的扩散研究不同，网络扩散开始考虑网络结构对于扩散过程的影响。
这里介绍一个使用R模拟网络扩散的例子。基本的算法非常简单：
 生成一个网络:g(V, E)。 随机选择一个或几个节点作为种子（seeds）。 每个感染者以概率p（可视作该节点的传染能力,通常表示为$$\beta$$）影响与其相连的节点。  其实这是一个最简单的SI模型在网络中的实现。S表示可感染（susceptible）, I表示被感染（infected）。SI模型描述了个体的状态从S到I之间的转变。因为形式简单，SI模型是可以求出其解析解的。考虑一个封闭的群体，没有出生、死亡和迁移。并假设个体是均匀混合的（homogeneous mixing),也就是要求个体的地理分布均匀，且被感染的概率也相同(T. G. Lewis, 2011)。那么β表示传染率（transmission rate)。SI模型可以表达为：
$$\frac{dS}{dt}=-\beta SI$$
$$\frac{dI}{dt}=\beta SI$$
且满足 I + S = 1，那么以上方程$$\frac{dI}{dt}=\beta SI$$可以表达为：
$$\frac{dI}{dt}=\beta I(1-I)$$
解这个微分方程，我们可以得到累计增长曲线的表达式。有趣的是，这是一个logistic增长，具有明显的S型曲线（S-shaped curve）特征。该模型在初期跨越临界点之后增长较快，后期则变得缓慢。 因而可以用来描述和拟合创新扩散过程（diffusion of innovations）。
当然，对疾病传播而言，SI模型是非常初级的（naive），主要因为受感染的个体以一定的概率恢复健康，或者继续进入可以被感染状态(S，据此扩展为SIS模型)或者转为免疫状态（R,据此扩展为SIR模型）。 免疫表示为R，用$$\gamma$$代表免疫概率（removal or recovery rate)。对于信息扩散而言，这种考虑暂时是不需要的。
第一步，生成网络。
require(igraph) # generate a social graph size = 50 # 规则网 g = graph.tree(size, children = 2); plot(g) g = graph.star(size); plot(g) g = graph.full(size); plot(g) g = graph.ring(size); plot(g) g = connect.</description>
    </item>
    
    <item>
      <title>东风夜放花千树：对宋词进行主题分析初探</title>
      <link>https://chengjunwang.com/zh/post/cn/2013-09-27-topic-modeling-of-song-peom/</link>
      <pubDate>Fri, 27 Sep 2013 00:00:00 +0000</pubDate>
      
      <guid>https://chengjunwang.com/zh/post/cn/2013-09-27-topic-modeling-of-song-peom/</guid>
      <description>邱怡轩在统计之都中展示了对宋词进行的分析（参见http://cos.name/tag/%E5%AE%8B%E8%AF%8D/），因为当时缺乏中文分词的工具，他独辟蹊径，假设宋词中任意两个相邻的汉字构成一个词语，进而找到了宋词当中的高频词。本文则尝试使用他所提供的宋词语料（http://cos.name/wp-content/uploads/2011/03/SongPoem.tar.gz），分析一下使用R进行中文分词、构建词云、高频词语聚类以及主题模型分析。
首先要载入使用的R包并读入数据。
library(Rwordseg) require(rJava) library(tm) library(slam) library(topicmodels) library(wordcloud) library(igraph) setwd(&amp;quot;D:/github/text mining/song&amp;quot;) # 更改为你的工作路径，并存放数据在此。 txt=read.csv(&amp;quot;SongPoem.csv&amp;quot;,colClasses=&amp;quot;character&amp;quot;)  {:lang=&amp;ldquo;ruby&amp;rdquo;}
然后进行对数据的操作。当然，第一步是进行中文分词，主要使用Rwordseg这个R包，其分词效果不错。分词的过程可以自动去掉标点符号。
poem_words &amp;lt;- lapply(1:length(txt$Sentence), function(i) segmentCN(txt$Sentence[i], nature = TRUE))  {:lang=&amp;ldquo;ruby&amp;rdquo;}
然后，我们将数据通过tm这个R包转化为文本-词矩阵（DocumentTermMatrix）。 wordcorpus &amp;lt;- Corpus(VectorSource(poem_words), encoding = &amp;ldquo;UTF-8&amp;rdquo;) # 组成语料库格式
Sys.setlocale(locale=&amp;quot;Chinese&amp;quot;) dtm1 &amp;lt;- DocumentTermMatrix(wordcorpus, control = list( wordLengths=c(1, Inf), # to allow long words bounds = list(global = c(5,Inf)), # each term appears in at least 5 docs removeNumbers = TRUE, # removePunctuation = list(preserve_intra_word_dashes = FALSE), weighting = weightTf, encoding = &amp;quot;UTF-8&amp;quot;) ) colnames(dtm1) findFreqTerms(dtm1, 1000) # 看一下高频词  {:lang=&amp;ldquo;ruby&amp;rdquo;}</description>
    </item>
    
    <item>
      <title>使用聚类分析为主题模型划分主题类型</title>
      <link>https://chengjunwang.com/zh/post/cn/2013-09-08-using-cluster-analysis-to-classify-topics-generated-by-topic-modeling/</link>
      <pubDate>Sun, 08 Sep 2013 00:00:00 +0000</pubDate>
      
      <guid>https://chengjunwang.com/zh/post/cn/2013-09-08-using-cluster-analysis-to-classify-topics-generated-by-topic-modeling/</guid>
      <description>使用主题模型（topic models）可以较为高效地划分文本的主题，但一个不得不面对的问题是有时候主题的划分过细，使得解读和归类成为困难。其实，聚类分析作为一个“古老”的分析方法可以较为简洁的解决这个问题。
举一个小例子，我们主要使用tm这个R包来完成文本挖掘的前期任务。在得到DocumentTermMatrix之后，可以通过计算cosine 相似度的方法来计算文本之间的不一致性（dissimilarity）。
# Using cluster analysis to classify topics generated by topic modeling # 2013 Sep 08 # Cheng-Jun Wang library(tm) library(topicmodels) require(proxy) data(acq) data(crude) m &amp;lt;- c(acq, crude) dtm &amp;lt;- DocumentTermMatrix(m) dtm &amp;lt;- removeSparseTerms(dtm, 0.8) inspect(dtm[1:5, 1:5]) # cluster analysis of documents based on DocumentTermMatrix dist_dtm &amp;lt;- dissimilarity(mtd, method = &#39;cosine&#39;) hc &amp;lt;- hclust(dist_dtm, method = &#39;ave&#39;) plot(hc, xlab = &#39;&#39;)  {:lang=&amp;ldquo;ruby&amp;rdquo;}
我在做RA的时候，面临的一个问题就是在做主体模型的时候出现的：模型拟合得到的主题数量太多。我们用下面这个例子进行简单的介绍。
# topic modeling topic_num = 50 for (k in c(topic_num)) { # k &amp;lt;- 10 SEED &amp;lt;- 2010 jss_TM &amp;lt;- list( VEM = LDA(dtm, k = k, control = list(seed = SEED)), VEM_fixed = LDA(dtm, k = k, control = list(estimate.</description>
    </item>
    
    <item>
      <title>使用R做主题模型：词语筛选和主题数量确定</title>
      <link>https://chengjunwang.com/zh/post/cn/2013-08-31-topic-modeling-with-r/</link>
      <pubDate>Sat, 31 Aug 2013 00:00:00 +0000</pubDate>
      
      <guid>https://chengjunwang.com/zh/post/cn/2013-08-31-topic-modeling-with-r/</guid>
      <description>1. 筛选单词 在数据清理（pre-processing）之后，需要对数据进行适当筛选。对数据的筛选包括至少两个步骤：
第一步，在DocumentTermMatrix中设定
使用R的topicmodels发现设定在DocumentTermMatrix里的约束条件失效，解决方法在此，其实在topicmodels的包里也粗略提及，只是用习惯了tm包的人觉得二者是无缝对接的。其实还很多差异，比如在tm里相似功能称之为TermDocumentMatrix
dtm &amp;lt;- DocumentTermMatrix(corpus, control = list(stemming = TRUE, stopwords = TRUE, wordLengths=c(4, 15), bounds = list(global = c(5,Inf)), # each term appears in at least 5 docs removeNumbers = TRUE, removePunctuation = list(preserve_intra_word_dashes = FALSE) #,encoding = &amp;quot;UTF-8&amp;quot; ) ) colnames(dtm) ## inspect all the words for errors dim(dtm)  {:lang=&amp;ldquo;ruby&amp;rdquo;}
第二步，通过tf-idf和col_sums选择高频词
这背后的逻辑在于主题模型是要对文本进行分类，频次较少的词的贡献并不大。但会显著的占用计算资源。
term_tfidf &amp;lt;-tapply(dtm$v/row_sums(dtm)[dtm$i], dtm$j, mean) * log2(nDocs(dtm)/col_sums(dtm &amp;gt; 0)) l1=term_tfidf &amp;gt;= quantile(term_tfidf, 0.</description>
    </item>
    
    <item>
      <title>使用R做主题模型：举一个处理Encoding问题的例子</title>
      <link>https://chengjunwang.com/zh/post/cn/2013-08-29-encoding-in-r-for-text-mining/</link>
      <pubDate>Thu, 29 Aug 2013 00:00:00 +0000</pubDate>
      
      <guid>https://chengjunwang.com/zh/post/cn/2013-08-29-encoding-in-r-for-text-mining/</guid>
      <description>引言 遭遇“邪恶的拉丁引号” 我遇到的问题比较复杂，因为原文里混合了latin1和UTF-8两种encoding的字形，最初我统一再读入text数据的时候采用encoding =&amp;ldquo;UTF-8&amp;rdquo;的方法，结果发现了很多奇诡的单引号和双引号错误。在生成的DocumentTermMatrix里出现了很多以引号开始或结束的terms，例如：“grandfather， “deputy with the constitution” 。用Encoding命令看一下它的原形是：
&amp;gt; Encoding(&amp;quot;“&amp;quot;) [1] &amp;quot;latin1&amp;quot;  只所以说是原形，是因为它们可以变形！&amp;rdquo;â€œ&amp;rdquo;， &amp;ldquo;â€™&amp;rdquo;， &amp;ldquo;â€\u009d&amp;rdquo;， &amp;ldquo;â€&amp;rdquo;都是它在不设定Encoding的环境下的形状。但我觉得不足以刻画我对它的厌恶，特别附图一张：
直到最后，我也没彻底搞定这些邪恶的拉丁引号，但我使用了一些tricks解决的我的问题。
1. 读入数据不设定encoding！ 因为邪恶的拉丁引号在UTF-8格式下根本就无法对付，在不设encoding方法的时候，它们现身为â€“, â€™, â€œ等形式，还可以对付。
dat1 = read.csv(&amp;quot;D:/chengjun/Crystal/Schwab_data_cleaningSep.csv&amp;quot;, header = F, sep = &amp;quot;|&amp;quot;, quote = &amp;quot;&amp;quot;, stringsAsFactors=F, fileEncoding = &amp;quot;&amp;quot;) # , encoding =&amp;quot;UTF-8&amp;quot;); dim(dat1) names(dat1) = c(&#39;name&#39;, &#39;organization&#39;, &#39;year&#39;, &#39;country&#39;, &#39;website&#39;, &#39;shortIntro&#39;, &#39;focus&#39;, &#39;geo&#39;, &#39;model&#39;, &#39;benefit&#39;, &#39;budget&#39;, &#39;revenue&#39;, &#39;recognization&#39;, &#39;background&#39;, &#39;innovation&#39;, &#39;entrepreneur&#39;)  2. 文本数据清理第一步：载入R包，选取变量 library(tm) library(topicmodels) text = dat1$entrepreneur  3.</description>
    </item>
    
    <item>
      <title>QAP检验：计算两个网络的关联</title>
      <link>https://chengjunwang.com/zh/post/cn/2013-08-04-qap-test-of-network-analysis/</link>
      <pubDate>Sun, 04 Aug 2013 00:00:00 +0000</pubDate>
      
      <guid>https://chengjunwang.com/zh/post/cn/2013-08-04-qap-test-of-network-analysis/</guid>
      <description>QAP检验：两个网络之间的关联 通常一组个体具有多种类型的关系，例如友谊关系和经济往来关系。我们通常会对这两种网络关系在多大程度上相互关联感兴趣。当我们知道一组个体之间的两种关系网络，我们就可以计算这个两个关系网络之间的相关程度。在统计学当中，皮尔森相关系数是用来反映两个变量线性相关程度的统计量。与之类似，对于由一组个体所组成的两个网络，也可以计算其相应的相关皮尔逊相关系数。当然，还可以计算其他你感兴趣的统计量，如协相关系数。
我们使用sna这个R软件包来计算网络相关系数（并调用qaptest命令）。通过安装和使用statnet这个R软件包，就会自动加载sna等子软件包。另外，statnet当中还集成了其他的几个相关的R软件包，包括进行动态网络建模的tergm子软件包。
# R程序11-8：计算网络的皮尔逊相关系数 install.packages(&amp;quot;statnet&amp;quot;) library(statnet) # 首先随机生成3个由10个节点构成的有向网络 g=array(dim=c(3,10,10)) g[1,,] = rgraph(10) g[2,,] = rgraph(10,tprob=g[1,,]*0.8) # 设置g1和g2两个网络强相关 g[3,,] = 1; g[3,1,2] = 0 # g3接近于一个派系（clique） # 绘制这3个网络 par(mfrow=c(1,3)) for(i in 1:3) { gplot(g[i,,],usecurv=TRUE, mode = &amp;quot;fruchtermanreingold&amp;quot;, vertex.sides=3:8)} #计算网络的相关矩阵 gcor(g)  在通常使用皮尔逊相关系数的时候，可以用t统计量对总体相关系数为0的原假设进行检验。但在计算网络的相关系数（graph correlations）时，经典的零假设检验方法往往会带来偏差，因而并不适用。通常使用非参数检验的方法，比如QAP(Quadratic Assignment Procedure)检验。
矩阵的随机排列（Random matrix permutations）是QAP检验的关键部分，在子软件包sna中主要通过rmperm来进行。通过矩阵的随机排列，可以对网络中的节点编号（而不是链接！！）进行随机置换（relabelling）或重新“洗牌”（reshuffling），并得到一组（比如1000个）重连后的网络。因为只是置换节点，这种操作只是重新标记节点的编号（relabelling）。
# R程序11-9：矩阵的随机置换方法 j = rgraph(5) # 随机生成一个网络 j #看一下这个网络的矩阵形式 rmperm(j) #随机置换后的网络的矩阵形式  对这一组重构的网络可以计算其网络级别的参数（如两个网络的相关参数，协相关参数），并因此得到一个参数分布。QAP检验的零假设是实际观测到的网络参数（如）来自于这一个参数分布。也就是说，原假设认为这种观测到的相关关系是由随机因素带来的，因而这种网络相关并不显著。拒绝原假设，就从统计的角度证明了观测到的网络相关系数是显著的。
# R程序11-10：QAP检验 q.12 = qaptest(g, gcor, g1 = 1, g2 = 2) q.</description>
    </item>
    
  </channel>
</rss>