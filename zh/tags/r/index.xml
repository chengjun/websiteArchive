<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R on </title>
    <link>https://chengjun.github.io/zh/tags/r/index.xml</link>
    <description>Recent content in R on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-cn</language>
    <copyright>&amp;copy; 2016 Cheng-Jun Wang</copyright>
    <atom:link href="/zh/tags/r/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>使用d3network做网络可视化</title>
      <link>https://chengjun.github.io/zh/post/cn/2014-08-27-d3network/</link>
      <pubDate>Wed, 27 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>https://chengjun.github.io/zh/post/cn/2014-08-27-d3network/</guid>
      <description>&lt;p&gt;在之前的&lt;a href=&#34;http://chengjun.github.io/cn/2014/07/chinese-university-friendship-network/&#34; target=&#34;_blank&#34;&gt;一个博客&lt;/a&gt;中，我介绍了使用R进行社区划分并可视化的方法。这里使用相同的数据，介绍如何使用d3network实现网络可视化的方法。&lt;/p&gt;

&lt;p&gt;首先，安装d3network&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;devtools::install_github(&amp;quot;d3Network&amp;quot;, &amp;quot;christophergandrud&amp;quot;)
require(d3Network)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;之后可以使用简单的可视化方法：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;d3SimpleNetwork(data[,1:2],
                file = &amp;quot;chinese_university100.html&amp;quot;,
                width = 1024,
                height = 763,
                fontsize = 12)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我想要展现社区划分的结果：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#链接数据
links = data
names(links) = c(&amp;quot;source&amp;quot;, &amp;quot;target&amp;quot;, &amp;quot;value&amp;quot;)

#节点列表
fc = fastgreedy.community(g); sizes(fc)
mfc = membership(fc)
nodes = data.frame(name = names(mfc), group = mfc)

#对应链接数据和节点数据
ids = 0:(nrow(nodes)-1) # notice: start with zero!
links[,1] = ids[match(links[,1], nodes$name )]
links[,2] = ids[match(links[,2], nodes$name )]
links = links[with(links, order(source)), ] # sort by source

#处理边的权重大小
links$value = log(links$value)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;之后就可以使实现可视化结果啦：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;d3ForceNetwork(Links = links, Nodes = nodes,
               Source = &amp;quot;source&amp;quot;, Target = &amp;quot;target&amp;quot;,
               Value = &amp;quot;value&amp;quot;,
               NodeID = &amp;quot;name&amp;quot;,
               Group = &amp;quot;group&amp;quot;,
               file = &amp;quot;chinese_university_groups100.html&amp;quot;,
               width = 1550, height = 800,iframe = TRUE,
               opacity = 0.9, zoom = TRUE)
&lt;/code&gt;&lt;/pre&gt;

&lt;iframe src=&#39;http://chengjun.github.io/vis/chinese_university_groups100.html&#39; scrolling=&#34;no&#34; width=&#34;800&#34; height = &#34;800&#34;&gt;&lt;/iframe&gt;

&lt;p&gt;但是对于中文要麻烦一些，需要手工修改html里的encoding设置，包括meta部分和script部分两个地方：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;meta charset=&amp;quot;gbk&amp;quot;&amp;gt;
&amp;lt;script charset=&amp;quot;gbk&amp;quot; src=http://d3js.org/d3.v3.min.js&amp;gt;&amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;除此之外，我还尝试了下&lt;a href=&#34;http://chengjun.github.io/vis/chinese_university_groups200more.html&#34; target=&#34;_blank&#34;&gt;两百所学校的情况：点这里。&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>空间分析初步：空间点类型分析</title>
      <link>https://chengjun.github.io/zh/post/cn/2014-03-12-first-step-spatial-analysis-with-r/</link>
      <pubDate>Wed, 12 Mar 2014 00:00:00 +0000</pubDate>
      
      <guid>https://chengjun.github.io/zh/post/cn/2014-03-12-first-step-spatial-analysis-with-r/</guid>
      <description>

&lt;h3 id=&#34;引言&#34;&gt;引言&lt;/h3&gt;

&lt;p&gt;空间分析（spatial analysis）对于扩散研究非常重要，它揭示了传播在空间维度上的分布。令人略感惊奇的是空间分析的研究者越来越多地使用R软件。其中一个原因是R包罗万象，而空间分析仍在发展且神情未定。在这个时候难以判定哪种方法最优。此时，策略当然是博观约取。R因其囊括众多统计方法而成为连接不同分析套路的首选；另外在R当中使用者可以继续开发新的数据分析包。可谓一举两得。　&lt;/p&gt;

&lt;h3 id=&#34;数据读入&#34;&gt;数据读入&lt;/h3&gt;

&lt;p&gt;我使用的是2013年米兰城12月份推特用户的地理信息数据。该数据来自&lt;a href=&#34;http://www.telecomitalia.com/tit/en/bigdatachallenge/contest/dataset.html&#34; target=&#34;_blank&#34;&gt;Big Data Challenge&lt;/a&gt; of Telecommunication。使用Python写很简单的script从其服务器api接口读取数据:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Download milano tweets data using python
# chengjun wang @ cmc
# 2014 Mar 11

import urllib2
import json

f = open(&#39;D:/chengjun/Milan/Social pulse/Milano_sample.csv&#39;, &#39;w&#39;)
for offset in range(0,269290/100 +1):
    print &amp;quot;working on offset: &amp;quot;, offset
    req_url = &#39;https://api.dandelion.eu/datagem/social-pulse-milano/data/v1/?$limit=100&amp;amp;$offset=&#39;+str(offset)+&#39;&amp;amp;$app_id=d...a&amp;amp;$app_key=2e...7c&#39;
    jstr = urllib2.urlopen(req_url).read() # json string
    &amp;quot;&amp;quot;&amp;quot; these are flickr-specific &amp;quot;&amp;quot;&amp;quot;
    jinfo = json.loads( jstr )
    for i in range(0, len(jinfo[&#39;items&#39;])):
        lan = jinfo[&#39;items&#39;][i][&#39;language&#39;]
        time = jinfo[&#39;items&#39;][i][&#39;created&#39;]
        geo = jinfo[&#39;items&#39;][i][&#39;geometry&#39;][&#39;coordinates&#39;]
        timestamp = jinfo[&#39;items&#39;][i][&#39;timestamp&#39;]
        municipality_name = jinfo[&#39;items&#39;][i][&#39;municipality&#39;][&#39;name&#39;]
        municipality_id = jinfo[&#39;items&#39;][i][&#39;municipality&#39;][&#39;acheneID&#39;]
        entities = jinfo[&#39;items&#39;][i][&#39;entities&#39;]
        user = jinfo[&#39;items&#39;][i][&#39;user&#39;]
        print &amp;gt;&amp;gt;f, &amp;quot;%s;%s;%s;%s;%s;%s;&#39;%s&#39;;%s&amp;quot; % (lan, time, geo, timestamp, municipality_name, municipality_id, entities, user)
f.close()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;首先，读入点的时空分布数据。　&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# read data
library(maptools)
library(sp)
library(rgdal)

setwd(&amp;quot;D:/chengjun/Milan&amp;quot;)
dat = read.csv(&amp;quot;./Social pulse/Milano_sample.csv&amp;quot;, header = FALSE, stringsAsFactors = FALSE, sep = &amp;quot;;&amp;quot;, quote = &amp;quot;&amp;quot;)
names(dat) = c(&amp;quot;lan&amp;quot;, &amp;quot;time&amp;quot;, &amp;quot;geo&amp;quot;, &amp;quot;timestamp&amp;quot;, &amp;quot;mname&amp;quot;, &amp;quot;mid&amp;quot;, &amp;quot;entities&amp;quot;, &amp;quot;user&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;进行简单的清洗：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# clean data
dat = subset(dat, dat$time &amp;gt;= as.POSIXlt(&amp;quot;2013-12-01 00:00:00&amp;quot;)); dim(dat)
dat$time = do.call(rbind, strsplit(dat$time, split = &amp;quot;\\.&amp;quot;))[,1]
dat$time = gsub(&amp;quot;T&amp;quot;, &amp;quot; &amp;quot;, dat$time)
dat$time = as.POSIXlt(dat$time)
dat$geo = gsub(&amp;quot;[&amp;quot;, &amp;quot;&amp;quot;, dat$geo, fixed = T)
dat$geo = gsub(&amp;quot;]&amp;quot;, &amp;quot;&amp;quot;, dat$geo, fixed = T)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;从openstreetmap下载米兰城的交通地理信息。使用rgdal这个R包读入数据：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# download shp data from
# http://metro.teczno.com/#milan
ost=readOGR(&amp;quot;./Milano Grid/milano-grid/milan.imposm-shapefiles/milan.osm-mainroads.shp&amp;quot;, layer = &amp;quot;milan.osm-mainroads&amp;quot;) #will load the shapefile
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;要把地理信息转为经纬度的数据表示形式：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;spl = spTransform(ost, CRS(&amp;quot;+proj=longlat&amp;quot;)) # convert to longitude and latitude
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;如果要画出spl的话，速度有点慢, 因为绘制的点比较多。&lt;/p&gt;

&lt;p&gt;用了其中一个小数据(涵盖一天中的几个小时)，为了展现了每个小时的&lt;a href=&#34;http://chengjun.github.io/big_data_challenge/visualization.html&#34; target=&#34;_blank&#34;&gt;动态变化&lt;/a&gt;，使用CartoDB网站来制作了一个简单的可视化。顺便找了一遍各种javascript的库和其它包（googleVis, Echarts等），发现都不实用，所以还是用R吧。&lt;/p&gt;

&lt;p&gt;设置绘图的函数，来看一下数据的形式：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# plot function
make_plot = function(){
  tz = as.POSIXlt(&amp;quot;2013-12-01 00:00:00&amp;quot;)
  end_time = as.POSIXlt(&amp;quot;2013-12-02 00:00:00&amp;quot;)
  while(tz &amp;lt;= end_time){
    print(tz)
    datd = subset(dat, dat$time &amp;gt; tz&amp;amp; dat$time &amp;lt;= tz + 3600)
    plot(spl, col = &amp;quot;pink&amp;quot;)
    title(tz)
    p = do.call(rbind, strsplit(datd$geo, split=&#39;,&#39;))
    p1 = as.numeric(p[,1])
    p2 = as.numeric(p[,2])
    points(p2~p1, pch = 1, col = &amp;quot;red&amp;quot;, cex = 0.1)
    tz = tz + 3600
  }
}

# save figures
png(file = &amp;quot;./linear%2d.png&amp;quot;,
    width=5, height=5,
    units=&amp;quot;in&amp;quot;, res=700)
make_plot()
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;读者也可以直接使用animation这个R包来绘图。我在实验室的机器上安装ImageMagick有点问题，干脆存为图片了，再转为gif了。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm3.staticflickr.com/2610/13103072964_078e5abcc6_o.gif&#34; alt=&#34;米兰城12月1日每个小时发推特的时空分布&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;图1&lt;/strong&gt; 米兰城一天当中的发推特的时空分布&lt;/p&gt;

&lt;p&gt;把12月31天的数据累积起来，我们可以得到米兰城2013年12月当中的发推特的空间分布。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# the overall geographical distribution
png(&amp;quot;./milano_social_pulse_December.png&amp;quot;,
    width=10, height=10,
    units=&amp;quot;in&amp;quot;, res=700)
plot(spl, col = &amp;quot;purple&amp;quot;)
p = do.call(rbind, strsplit(dat$geo, split=&#39;,&#39;))
p1 = as.numeric(p[,1])
p2 = as.numeric(p[,2])
points(p2~p1, pch = 1, col = &amp;quot;red&amp;quot;, cex = 0.01)
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://farm8.staticflickr.com/7349/13103304784_11e17eae38.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;图2&lt;/strong&gt; 米兰城2013年12月当中的发推特的空间分布&lt;/p&gt;

&lt;p&gt;这个图还是有点意思的：街道是城市人流的管道（Tube,伦敦好像把地铁直接称为tube）,人的移动等行为（包括社会媒体使用行为）则是穿行其间的流。推特聚集的地方与街道的轮廓高度契合。城市的中心推特的密度大（因为人流的密度大？）。所以可以检验下点的分布是否是随机的。&lt;/p&gt;

&lt;h3 id=&#34;空间点类型分析&#34;&gt;空间点类型分析&lt;/h3&gt;

&lt;p&gt;这里涉及到空间点类型分析（spatial point pattern analysis）。检验下点的分布是否是随机的最简单的方法是进行完全空间随机（complete spatial randomness， CSR）分析。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;G方程方法&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;这里说的最近邻居，英文当中却成为nearest event。把一个点的存在称之为事件也挺好玩。我们知道两个节点$$E_i$$和$$E_j$$的距离为:&lt;/p&gt;

&lt;p&gt;$$d(E_i, E_j) = \sqrt{(x_i - x_j)^2 + (y_i-y_j)^2}$$&lt;/p&gt;

&lt;p&gt;平均最近邻居距离可以表示为:&lt;/p&gt;

&lt;p&gt;$$\overline{d}&lt;em&gt;{min} = \frac{\sum&lt;/em&gt;{i}^{n}d_{min}(X_i)}{n}$$&lt;/p&gt;

&lt;p&gt;于是可以定义事件-事件最近邻居距离，即任意一个事件到它的最近事件之间的距离。任意一个事件$$E_i$$的事件-事件最近邻居距离：&lt;/p&gt;

&lt;p&gt;$$d_i = {min}&lt;em&gt;j {d&lt;/em&gt;{ij}, \forall j\neq i }$$&lt;/p&gt;

&lt;p&gt;对于一个距离d, 可定义G(d)为最近邻居距离的累计频数分布：&lt;/p&gt;

&lt;p&gt;$$ G(d) = \frac{# d_{min}E_i&amp;lt;d}{n} $$&lt;/p&gt;

&lt;p&gt;说以G方程方法测量的最近邻居距离小于d的事件的比率。当事件分布存在聚集的情况的时候，G在距离较小的时候就增长特别快；当事件分布均匀时，距离较小的时候G增长缓慢，当距离达到使得多数事件分隔的大小后，G开始快速增长。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;F方程方法&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;另外一个有用的测度是F方程。F方程测量了从空间中任意一个&lt;strong&gt;点&lt;/strong&gt;到与它最近的&lt;strong&gt;事件&lt;/strong&gt;之间的距离，因而它测量的是&lt;strong&gt;点-事件最近邻居距离&lt;/strong&gt;。据此，F方程也被称之为空虚空间距离。&lt;/p&gt;

&lt;p&gt;计算F方程或点-事件距离的时候要先随机的抽取一些空间中的点, 计算它的最短距离，然后统计其中满足最短距离小于d的比率。&lt;/p&gt;

&lt;p&gt;$$F(d) = \frac{#d_{min}(x_i&amp;lt;d)}{m}$$&lt;/p&gt;

&lt;p&gt;因为F方程是随机抽取空间中的点来统计，因而可以应对较大的数据规模。一个小的事件的聚集会导致G方程快速增长，但其实其它多数空间都是空的，所以F方程增长会较慢。当然了，对于规则分布的点，这种对比的结果则可能相反。&lt;/p&gt;

&lt;p&gt;使用G和F方程可以测量事件分布的实际情况，我们的零假设是事件分布是随机的，符合泊松分布:&lt;/p&gt;

&lt;p&gt;$$f(k, \lambda) = \frac{\lambda^k e^{-\lambda}}{k!}$$&lt;/p&gt;

&lt;p&gt;用$$\lambda$$表示事件的空间分布密度，在一个半径为d的平面$$\pi d^2$$里面理论上存在的事件数量是$$\lambda \pi d^2$$。&lt;/p&gt;

&lt;p&gt;此处推导略去（汗，我不会啊。），那么G和F的理论值按照泊松分布应该是：&lt;/p&gt;

&lt;p&gt;$$G = F = 1-e^{-\lambda \pi d^2}$$&lt;/p&gt;

&lt;p&gt;有了理论值，有了实际值，我们就可以对比二者之间的差距。进而推论空间事件的点分布是否是随机的了。&lt;/p&gt;

&lt;p&gt;这个推断的过程使用spatstat这个R包进行：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;require(spatstat)
#the analysis of point patterns
geo = data.frame(dat$p1, dat$p2)
# Convert data to ppp format
geo_ppp = ppp(geo[,1], geo[,2],
               c(min(geo[,1]), max(geo[,1])),
               c(min(geo[,2]), max(geo[,2]))
               ) # slow
# G function method
g = Gest(geo_ppp)
plot(g)

# F function method
f = Fest(geo_ppp)
plot(f)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;这里可以使用envelope的方法，使用蒙特卡洛的方法，根据一些算法来随机生成n个（比如100个）数据，以保证分析的准确性。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;g = envelope(geo_ppp, Gest, nsim = 100)
plot(g)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://farm4.staticflickr.com/3824/13124103465_64f43f59bc.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;f = envelope(geo_ppp, Fest, nsim = 100)
plot(f)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://farm8.staticflickr.com/7362/13124385414_f32593880e.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;显然发推特的空间位置的分布并非随机的，具有较明显的聚集现象，所以G方程一开始就增长很快，而虚空空间函数F方程则增长缓慢。&lt;/p&gt;

&lt;h3 id=&#34;空间点过程分析&#34;&gt;空间点过程分析&lt;/h3&gt;

&lt;p&gt;这毕竟还是有点不够形象，有没有高大上的形象的方法？试试&lt;a href=&#34;http://cran.at.r-project.org/web/packages/spatstat/vignettes/getstart.pdf&#34; target=&#34;_blank&#34;&gt;kernel smoother of point density&lt;/a&gt;:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;plot(density.ppp(geo_ppp), main = &amp;quot;&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://farm3.staticflickr.com/2636/13124643824_4ee4b447d5_c.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;注意density.ppp返回的不是一个概率密度。它是对点密度的估计。密度是每个单位空间里随机点的期望。密度通常与空间位置有关。使用空间面积对密度函数积分，得到的是落入该区域的点的数量。&lt;/p&gt;

&lt;p&gt;于是乎，规律就更明显了：不仅仅是简单的点聚集，而且是箭靶形式的聚集，像北京环城路一样。越是中心，点就越密集。&lt;/p&gt;

&lt;p&gt;不过不要高兴太早，因为这个结果还是太粗糙。我们明明看到点的聚集情况并非如此完美的圆环。因为使用kernel平滑方法估计点的密度这种方法对于频宽（bandwidth）的大小特别敏感。有必要加以控制。另外，这里涉及到两种kernel的方法：四次多项式平滑和高斯平滑。这里要使用splancs这个R包。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;##############
&amp;quot;quartic and Gaussian kernels&amp;quot;
##############

library(splancs)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;抱怨一下，因为以下用到的bw.diggle这个用来为kernel密度来选择经过交叉检验的频宽（Cross Validated Bandwidth Selection for Kernel Density）的命令，我不得不使用部分数据，因为它实在太消耗内存了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# subset a week-long small data
dat1 = subset(dat, dat$day &amp;gt;=as.Date(&amp;quot;2013-12-01&amp;quot;)&amp;amp;dat$day &amp;lt;=as.Date(&amp;quot;2013-12-07&amp;quot;))

geo = data.frame(dat1$p1, dat1$p2)
geo_ppp = ppp(geo[,1], geo[,2],
  c(min(geo[,1]), max(geo[,1])),
  c(min(geo[,2]), max(geo[,2]))
) # slow

## Quartic kernel
mserwq&amp;lt;-mse2d(as.points(coordinates(geo)),
  as.points(list(x=c(0,1,1,0), y=c(0,0,1,1))), 100, range = .001) # flexible range
bwq&amp;lt;-mserwq$h[which.min(mserwq$mse)]
bwq

## Gaussian kernel
mserw&amp;lt;-bw.diggle(as(geo_ppp, &amp;quot;ppp&amp;quot;)) #   Reached total allocation of 32765Mb: see help(memory.size)
bw&amp;lt;-as.numeric(mserw)
bw

&amp;quot;plot the Mean Square Error-Bandwidth&amp;quot;
par(mfrow=c(1, 2))
plot(mserwq$h, mserwq$mse, xlab=&amp;quot;Bandwidth&amp;quot;, ylab=&amp;quot;MSE&amp;quot;, type=&amp;quot;l&amp;quot;, main=&amp;quot;Quartic kernel&amp;quot;)
i&amp;lt;-which.min(mserwq$mse)
points(mserwq$h[i], mserwq$mse[i], col = &amp;quot;red&amp;quot;)
plot(mserw, main=&amp;quot;Gaussian kernel&amp;quot;, xlab=&amp;quot;Bandwidth&amp;quot;, ylab=&amp;quot;MSE&amp;quot;)
points(attr(mserw, &amp;quot;h&amp;quot;)[attr(mserw, &amp;quot;iopt&amp;quot;)], bw, col = &amp;quot;red&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://farm8.staticflickr.com/7310/13145953075_2b2ae6a1e1.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;看，最优化的频宽选择并不太有用，不过频宽真得很小。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;geos = SpatialPointsDataFrame(geo, geo)
poly = as.points(list(x = c(0, 0, 1, 1), y = c(0, 1, 1, 0)))

sG &amp;lt;- Sobj_SpatialGrid(geos, maxDim=100)$SG
grd &amp;lt;- slot(sG, &amp;quot;grid&amp;quot;)
summary(grd)

# k0 &amp;lt;- spkernel2d(geos, poly, h0=bw, grd)
# k1 &amp;lt;- spkernel2d(geos, poly, h0=.05, grd)
# k2 &amp;lt;- spkernel2d(geos, poly, h0=.1, grd)
# k3 &amp;lt;- spkernel2d(geos, poly, h0=.15, grd)
# df &amp;lt;- data.frame(k0=k0, k1=k1, k2=k2, k3=k3)
# kernels &amp;lt;- SpatialGridDataFrame(grd, data=df)
# summary(kernels)
# 这里都是NA,四次多项式的结果并不好

##################################
cc &amp;lt;- coordinates(sG); head(cc)
xy&amp;lt;-list(x=cc[,1], y=cc[,2])
k0&amp;lt;-density(as(geos, &amp;quot;ppp&amp;quot;), .5*bw, dimyx=c(100, 100), xy=xy)
k1&amp;lt;-density(as(geos, &amp;quot;ppp&amp;quot;), .5*bw*200, dimyx=c(100, 100), xy=xy)
k2&amp;lt;-density(as(geos, &amp;quot;ppp&amp;quot;), .5*bw*500, dimyx=c(100, 100), xy=xy)
k3&amp;lt;-density(as(geos, &amp;quot;ppp&amp;quot;), .5*bw*600, dimyx=c(100, 100), xy=xy)
k4&amp;lt;-density(as(geos, &amp;quot;ppp&amp;quot;), .5*bw*800, dimyx=c(100, 100), xy=xy)
k5&amp;lt;-density(as(geos, &amp;quot;ppp&amp;quot;), .5*bw*1000, dimyx=c(100, 100), xy=xy)
k6&amp;lt;-density(as(geos, &amp;quot;ppp&amp;quot;), .5*bw*1500, dimyx=c(100, 100), xy=xy)
k7&amp;lt;-density(as(geos, &amp;quot;ppp&amp;quot;), .5*bw*2000, dimyx=c(100, 100), xy=xy)
&amp;quot;plot the MSE-Bandwidth&amp;quot;
png(file = &amp;quot;./gaussian_kernel_density_first_week_in_december2.png&amp;quot;,
width=8, height=16,
units=&amp;quot;in&amp;quot;, res=700)

par(mfrow=c(4, 2), mar=rep(1, 4))
plot(k0)
plot(k1)
plot(k2)
plot(k3)
plot(k4)
plot(k5)
plot(k6)
plot(k7)

dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://farm8.staticflickr.com/7408/13145698615_5559e6ba2c_o.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这里列出几个比较小的频宽的核密度图：这与我们的观察比较一致。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# kernels$k7&amp;lt;-as(k7, &amp;quot;SpatialGridDataFrame&amp;quot;)$v
df &amp;lt;- data.frame(k0=k0, k1=k1, k2=k2, k3=k3, k4=k4, k5=k5， k6 = k6, k7 = k7)
kernels &amp;lt;- SpatialGridDataFrame(grd, data=df)
summary(kernels)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;参考文献&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;Lloyd，D.C.(2007) Local Models for Spatial Analysis. CRC press&lt;/p&gt;

&lt;p&gt;Baddeley, A. (2010) Analysing spatial point patterns in R. Workshop notes. CSIRO online technical publication. URL: www.csiro.au/resources/pf16h.html&lt;/p&gt;

&lt;p&gt;Diggle, P.J. (1985) A kernel method for smoothing point process data. Applied Statistics (Journal of the Royal Statistical Society, Series C) 34 (1985) 138–147.&lt;/p&gt;

&lt;p&gt;Diggle, P.J. (2003) Statistical analysis of spatial point patterns, Second edition. Arnold.&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>使用R模拟网络扩散</title>
      <link>https://chengjun.github.io/zh/post/cn/2014-02-28-simulate-network-diffusion-with-r/</link>
      <pubDate>Fri, 28 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>https://chengjun.github.io/zh/post/cn/2014-02-28-simulate-network-diffusion-with-r/</guid>
      <description>&lt;p&gt;与普通的扩散研究不同，网络扩散开始考虑网络结构对于扩散过程的影响。&lt;/p&gt;

&lt;p&gt;这里介绍一个使用R模拟网络扩散的例子。基本的算法非常简单：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;生成一个网络:g(V, E)。&lt;/li&gt;
&lt;li&gt;随机选择一个或几个节点作为种子（seeds）。&lt;/li&gt;
&lt;li&gt;每个感染者以概率p（可视作该节点的传染能力,通常表示为$$\beta$$）影响与其相连的节点。&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;其实这是一个最简单的SI模型在网络中的实现。S表示可感染（susceptible）, I表示被感染（infected）。SI模型描述了个体的状态从S到I之间的转变。因为形式简单，SI模型是可以求出其解析解的。考虑一个封闭的群体，没有出生、死亡和迁移。并假设个体是均匀混合的（homogeneous mixing),也就是要求个体的地理分布均匀，且被感染的概率也相同(T. G. Lewis, 2011)。那么β表示传染率（transmission rate)。SI模型可以表达为：&lt;/p&gt;

&lt;p&gt;$$\frac{dS}{dt}=-\beta SI$$&lt;/p&gt;

&lt;p&gt;$$\frac{dI}{dt}=\beta SI$$&lt;/p&gt;

&lt;p&gt;且满足 I + S = 1，那么以上方程$$\frac{dI}{dt}=\beta SI$$可以表达为：&lt;/p&gt;

&lt;p&gt;$$\frac{dI}{dt}=\beta I(1-I)$$&lt;/p&gt;

&lt;p&gt;解这个微分方程，我们可以得到累计增长曲线的表达式。有趣的是，这是一个logistic增长，具有明显的S型曲线（S-shaped curve）特征。该模型在初期跨越临界点之后增长较快，后期则变得缓慢。 因而可以用来描述和拟合创新扩散过程（diffusion of innovations）。&lt;/p&gt;

&lt;p&gt;当然，对疾病传播而言，SI模型是非常初级的（naive），主要因为受感染的个体以一定的概率恢复健康，或者继续进入可以被感染状态(S，据此扩展为SIS模型)或者转为免疫状态（R,据此扩展为SIR模型）。 免疫表示为R，用$$\gamma$$代表免疫概率（removal or recovery rate)。对于信息扩散而言，这种考虑暂时是不需要的。&lt;/p&gt;

&lt;p&gt;第一步，生成网络。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;require(igraph)
# generate a social graph
size = 50

# 规则网
g = graph.tree(size, children = 2); plot(g)
g = graph.star(size); plot(g)
g = graph.full(size); plot(g)
g = graph.ring(size); plot(g)
g = connect.neighborhood(graph.ring(size), 2); plot(g) # 最近邻耦合网络

# 随机网络
g = erdos.renyi.game(size, 0.1)

# 小世界网络
g = rewire.edges(erdos.renyi.game(size, 0.1), prob = 0.8 )
# 无标度网络
g = barabasi.game(size) ; plot(g)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;第二步，随机选取一个或n个种子。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#  initiate the diffusers
seeds_num = 1
set.seed(2014); diffusers = sample(V(g),seeds_num) ; diffusers
infected =list()
infected[[1]]= diffusers
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;第三步，在这个简单的例子中，每个节点的传染能力是0.5，即与其相连的节点以0.5的概率被其感染。在R中的实现是通过抛硬币的方式来实现的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# for example, set percolation probability = 0.5
coins = c(0,1)
n = length(coins)
sample(coins, 1, replace=TRUE, prob=rep(1/n, n))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;显然，这很容易扩展到更一般的情况，比如节点的平均感染能力是0.128，那么可以这么写：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;p = 0.128
coins = c(rep(1, p*1000), rep(0,(1-p)*1000))
n = length(coins)
sample(coins, 1, replace=TRUE, prob=rep(1/n, n))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;当然最重要的一步是要能按照“时间”更新网络节点被感染的信息。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# function for updating the diffusers
update_diffusers = function(diffusers){
  nearest_neighbors = neighborhood(g, 1, diffusers)
  nearest_neighbors = data.frame(table(unlist(nearest_neighbors)))
  nearest_neighbors = subset(nearest_neighbors, !(nearest_neighbors[,1]%in%diffusers))
  # toss the coins
  toss = function(freq) {
    tossing = NULL
    for (i in 1:freq ) tossing[i] = sample(coins, 1, replace=TRUE, prob=rep(1/n, times=n))
    tossing = sum(tossing)
    return (tossing)
  }
  keep = unlist(lapply(nearest_neighbors[,2], toss))
  new_infected = as.numeric(as.character(nearest_neighbors[,1][keep &amp;gt;= 1]))
  diffusers = unique(c(diffusers, new_infected))
  return(diffusers)
  }
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;完成了以上三步。准备好了吗，现在开始开启扩散过程！&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;## Start the contagion!
i = 1
while(length(infected[[i]]) &amp;lt; size){
  infected[[i+1]] = sort(update_diffusers(infected[[i]]))
  cat(length(infected[[i+1]]), &amp;quot;\n&amp;quot;)
  i = i + 1
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;先看看S曲线吧：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# &amp;quot;growth_curve&amp;quot;
num_cum = unlist(lapply(1:i, function(x) length(infected［x］) ))
p_cum = num_cum/max(num_cum)
time = 1:i

png(file = &amp;quot;./temporal_growth_curve.png&amp;quot;,
    width=5, height=5,
    units=&amp;quot;in&amp;quot;, res=300)
plot(p_cum~time, type = &amp;quot;b&amp;quot;)
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://farm8.staticflickr.com/7299/12845959103_e19cd9cd99_n.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;为了可视化这个扩散的过程，我们用红色来标记被感染者。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# generate a palette
E(g)$color = &amp;quot;blueviolet&amp;quot;
V(g)$color = &amp;quot;white&amp;quot;
set.seed(2014); layout.old = layout.fruchterman.reingold(g)
V(g)$color[V(g)%in%diffusers] = &amp;quot;red&amp;quot;
plot(g, layout =layout.old)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;使用谢益辉开发的animation的R包可视化。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;library(animation)

saveGIF({
  ani.options(interval = 0.5, convert = shQuote(&amp;quot;C:/Program Files/ImageMagick-6.8.8-Q16/convert.exe&amp;quot;))
  # start the plot
  m = 1
  while(m &amp;lt;= length(infected)){
    V(g)$color = &amp;quot;white&amp;quot;
    V(g)$color[V(g)%in%infected[[m]]] = &amp;quot;red&amp;quot;
    plot(g, layout =layout.old)
    m = m + 1}
})
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://farm4.staticflickr.com/3806/12826172695_368a6f50a2_o.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm3.staticflickr.com/2848/12826237753_d8c97b1019_o.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm4.staticflickr.com/3729/12826584654_c84452f397_o.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm3.staticflickr.com/2851/12826173505_34649f488d_o.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm8.staticflickr.com/7391/12826173255_574e471023_o.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm4.staticflickr.com/3675/12826584484_7c6f35380c_o.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm8.staticflickr.com/7432/12826173045_ef3548ec04_o.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;如同在Netlogo里一样，我们可以把网络扩散与增长曲线同时展示出来：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;saveGIF({
  ani.options(interval = 0.5, convert = shQuote(&amp;quot;C:/Program Files/ImageMagick-6.8.8-Q16/convert.exe&amp;quot;))
  # start the plot
  m = 1
  while(m &amp;lt;= length(infected)){
    # start the plot
    layout(matrix(c(1, 2, 1, 3), 2,2, byrow = TRUE), widths=c(3,1), heights=c(1, 1))
    V(g)$color = &amp;quot;white&amp;quot;
    V(g)$color[V(g)%in%infected[[m]]] = &amp;quot;red&amp;quot;
    num_cum = unlist(lapply(1:m, function(x) length(infected[[x]]) ))
    p_cum = num_cum/size
    p = diff(c(0, p_cum))
    time = 1:m
    plot(g, layout =layout.old, edge.arrow.size=0.2)
    title(paste(&amp;quot;Scale-free Network \n Day&amp;quot;, m))
    plot(p_cum~time, type = &amp;quot;b&amp;quot;, ylab = &amp;quot;CDF&amp;quot;, xlab = &amp;quot;Time&amp;quot;,
         xlim = c(0,i), ylim =c(0,1))
    plot(p~time, type = &amp;quot;h&amp;quot;, ylab = &amp;quot;PDF&amp;quot;, xlab = &amp;quot;Time&amp;quot;,
         xlim = c(0,i), ylim =c(0,1), frame.plot = FALSE)
    m = m + 1}
}, ani.width = 800, ani.height = 500)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://farm4.staticflickr.com/3672/12848749413_7f9da8b8c7_o.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>东风夜放花千树：对宋词进行主题分析初探</title>
      <link>https://chengjun.github.io/zh/post/cn/2013-09-27-topic-modeling-of-song-peom/</link>
      <pubDate>Fri, 27 Sep 2013 00:00:00 +0000</pubDate>
      
      <guid>https://chengjun.github.io/zh/post/cn/2013-09-27-topic-modeling-of-song-peom/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://forum.eedu.org.cn/upload/2008/02/27/90815055.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;邱怡轩在统计之都中展示了对宋词进行的分析（参见&lt;a href=&#34;http://cos.name/tag/%E5%AE%8B%E8%AF%8D/），因为当时缺乏中文分词的工具，他独辟蹊径，假设宋词中任意两个相邻的汉字构成一个词语，进而找到了宋词当中的高频词。本文则尝试使用他所提供的宋词语料（http://cos.name/wp-content/uploads/2011/03/SongPoem.tar.gz），分析一下使用R进行中文分词、构建词云、高频词语聚类以及主题模型分析。&#34; target=&#34;_blank&#34;&gt;http://cos.name/tag/%E5%AE%8B%E8%AF%8D/），因为当时缺乏中文分词的工具，他独辟蹊径，假设宋词中任意两个相邻的汉字构成一个词语，进而找到了宋词当中的高频词。本文则尝试使用他所提供的宋词语料（http://cos.name/wp-content/uploads/2011/03/SongPoem.tar.gz），分析一下使用R进行中文分词、构建词云、高频词语聚类以及主题模型分析。&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;首先要载入使用的R包并读入数据。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;library(Rwordseg)
require(rJava)
library(tm)
library(slam)
library(topicmodels)
library(wordcloud)
library(igraph)
setwd(&amp;quot;D:/github/text mining/song&amp;quot;) # 更改为你的工作路径，并存放数据在此。
txt=read.csv(&amp;quot;SongPoem.csv&amp;quot;,colClasses=&amp;quot;character&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{:lang=&amp;ldquo;ruby&amp;rdquo;}&lt;/p&gt;

&lt;p&gt;然后进行对数据的操作。当然，第一步是进行中文分词，主要使用Rwordseg这个R包，其分词效果不错。分词的过程可以自动去掉标点符号。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;poem_words &amp;lt;- lapply(1:length(txt$Sentence), function(i) segmentCN(txt$Sentence[i], nature = TRUE))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{:lang=&amp;ldquo;ruby&amp;rdquo;}&lt;/p&gt;

&lt;p&gt;然后，我们将数据通过tm这个R包转化为文本-词矩阵（DocumentTermMatrix）。
    wordcorpus &amp;lt;- Corpus(VectorSource(poem_words), encoding = &amp;ldquo;UTF-8&amp;rdquo;) # 组成语料库格式&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Sys.setlocale(locale=&amp;quot;Chinese&amp;quot;)
dtm1 &amp;lt;- DocumentTermMatrix(wordcorpus,
                          control = list(
                            wordLengths=c(1, Inf), # to allow long words
                            bounds = list(global = c(5,Inf)), # each term appears in at least 5 docs
                            removeNumbers = TRUE,
                            # removePunctuation  = list(preserve_intra_word_dashes = FALSE),
                            weighting = weightTf,
                            encoding = &amp;quot;UTF-8&amp;quot;)
)

colnames(dtm1)
findFreqTerms(dtm1, 1000) # 看一下高频词
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{:lang=&amp;ldquo;ruby&amp;rdquo;}&lt;/p&gt;

&lt;p&gt;这里需要注意的是，这里我们默认词语的长度为1到无穷大，稍后，我们可以对其长度进行修改。例如，本文中，作者对改为长度为2以上以及长度为3以上，分别得到另外两个文本-词矩阵：dtm2和dtm3。随后，我们可以在文本-词矩阵进行一系列的分析。这里，先做一个简单的词云分析。为更好展示效果，最多只列出100个词。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;m &amp;lt;- as.matrix(dtm1)
v &amp;lt;- sort(colSums(m), decreasing=TRUE)
myNames &amp;lt;- names(v)
d &amp;lt;- data.frame(word=myNames, freq=v)
par(mar = rep(2, 4))

png(paste(getwd(), &amp;quot;/wordcloud50_&amp;quot;,  &amp;quot;.png&amp;quot;, sep = &#39;&#39;),
    width=10, height=10,
    units=&amp;quot;in&amp;quot;, res=700)

pal2 &amp;lt;- brewer.pal(8,&amp;quot;Dark2&amp;quot;)
wordcloud(d$word,d$freq, scale=c(5,.2), min.freq=mean(d$freq),
          max.words=100, random.order=FALSE, rot.per=.15, colors=pal2)
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{:lang=&amp;ldquo;ruby&amp;rdquo;}&lt;/p&gt;

&lt;p&gt;我们可以看一下效果，如下图所示，主要是一个字长度的词。最多的是“人”和“不”， 然后是“春”和“花”。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm6.staticflickr.com/5480/9953518693_004590d6e3_z.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;但我们也许对长度大于2的词以及长度大于3的词更感兴趣（同时这样也可以和邱怡轩做的结果做一下比较）。使用前面步骤中生成的dtm2重复构建词云的R程序，我们可以得到以下两个词云：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm3.staticflickr.com/2821/9953413634_9a0d9020f7_z.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;同样，对dtm3构建词云，效果如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm8.staticflickr.com/7442/9953417766_8e6f160461_z.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;以上结果和邱怡轩得到的结果类似，可见对高频词的处理方面，他的方法的确有创见。对词云分析之后，我们还可以尝试根据词与词之间共同出现的概率对词进行聚类。这里我们展示长度大于2的词的聚类结果。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dtm01 &amp;lt;- weightTfIdf(dtm2)
N = 0.9
dtm02 &amp;lt;- removeSparseTerms(dtm01, N);dtm02
# 注意，为展示方便，这里我调节N的大小，使得dtm02中的词语数量在50左右。
tdm = as.TermDocumentMatrix(dtm02)
tdm &amp;lt;- weightTfIdf(tdm)
# convert the sparse term-document matrix to a standard data frame
mydata.df &amp;lt;- as.data.frame(inspect(tdm))
mydata.df.scale &amp;lt;- scale(mydata.df)
d &amp;lt;- dist(mydata.df.scale, method = &amp;quot;euclidean&amp;quot;) # distance matrix
fit &amp;lt;- hclust(d, method=&amp;quot;ward&amp;quot;)

png(paste(&amp;quot;d:/chengjun/honglou/honglou_termcluster_50_&amp;quot;, &amp;quot;.png&amp;quot;, sep = &#39;&#39;),
    width=10, height=10,
    units=&amp;quot;in&amp;quot;, res=700)
plot(fit) # display dendogram?
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{:lang=&amp;ldquo;ruby&amp;rdquo;}&lt;/p&gt;

&lt;p&gt;对长度大于2的词的聚类结果如下图所示，可见宋词的确注重“风流倜傥”，连分类都和风向有关系。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm3.staticflickr.com/2875/9953397625_abdc6e9874_z.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;当然读者还可以尝试对长度大于1词和长度大于3的词聚类，对长度大于1词聚类的效果图如下所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm6.staticflickr.com/5548/9953397345_05a8944455_z.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;长度大于3的词聚类结果如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm8.staticflickr.com/7375/9953419256_f80a00e9e8_z.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;完成这一步之后，作者还尝试对宋词进行简单的主题模型分析，首先还是从长度大于2的词开始吧。第一步是确定主题的数量。先对文本-词矩阵进行简单处理，以消除高频词被高估和低频词被低估的问题。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dtm = dtm2
term_tfidf &amp;lt;-tapply(dtm$v/row_sums(dtm)[dtm$i], dtm$j, mean) * log2(nDocs(dtm)/col_sums(dtm &amp;gt; 0))
l1=term_tfidf &amp;gt;= quantile(term_tfidf, 0.5)       # second quantile, ie. median
dtm &amp;lt;- dtm[,l1]
dtm = dtm[row_sums(dtm)&amp;gt;0, ]; dim(dtm) # 2246 6210
summary(col_sums(dtm))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{:lang=&amp;ldquo;ruby&amp;rdquo;}&lt;/p&gt;

&lt;p&gt;之后就可以正式地开始确定主题数量的R程序了。这里，笔者主要参考了朱雪宁《微博名人那些事儿》一文中的R程序（&lt;a href=&#34;http://cos.name/2013/08/something_about_weibo/）。&#34; target=&#34;_blank&#34;&gt;http://cos.name/2013/08/something_about_weibo/）。&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fold_num = 10
kv_num =  c(5, 10*c(1:5, 10))
seed_num = 2003
try_num = 1

smp&amp;lt;-function(cross=fold_num,n,seed)
{
  set.seed(seed)
  dd=list()
  aa0=sample(rep(1:cross,ceiling(n/cross))[1:n],n)
  for (i in 1:cross) dd[[i]]=(1:n)[aa0==i]
  return(dd)
}

selectK&amp;lt;-function(dtm,kv=kv_num,SEED=seed_num,cross=fold_num,sp) # change 60 to 15
{
  per_ctm=NULL
  log_ctm=NULL
  for (k in kv)
  {
    per=NULL
    loglik=NULL
    for (i in 1:try_num)  #only run for 3 replications#
    {
      cat(&amp;quot;R is running for&amp;quot;, &amp;quot;topic&amp;quot;, k, &amp;quot;fold&amp;quot;, i,
          as.character(as.POSIXlt(Sys.time(), &amp;quot;Asia/Shanghai&amp;quot;)),&amp;quot;\n&amp;quot;)
      te=sp[[i]]
      tr=setdiff(1:dtm$nrow, te) # setdiff(nrow(dtm),te)  ## fix here when restart r session

      # VEM = LDA(dtm[tr, ], k = k, control = list(seed = SEED)),
      # VEM_fixed = LDA(dtm[tr,], k = k, control = list(estimate.alpha = FALSE, seed = SEED)),

      #       CTM = CTM(dtm[tr,], k = k,
      #                 control = list(seed = SEED, var = list(tol = 10^-4), em = list(tol = 10^-3)))  
      #       
      Gibbs = LDA(dtm[tr,], k = k, method = &amp;quot;Gibbs&amp;quot;,
                  control = list(seed = SEED, burnin = 1000,thin = 100, iter = 1000))

      per=c(per,perplexity(Gibbs,newdata=dtm[te,]))
      loglik=c(loglik,logLik(Gibbs,newdata=dtm[te,]))
    }
    per_ctm=rbind(per_ctm,per)
    log_ctm=rbind(log_ctm,loglik)
  }
  return(list(perplex=per_ctm,loglik=log_ctm))
}

sp=smp(n=dtm$nrow, seed=seed_num) # n = nrow(dtm)

system.time((ctmK=selectK(dtm=dtm,kv=kv_num,SEED=seed_num,cross=fold_num,sp=sp)))

## plot the perplexity

m_per=apply(ctmK[[1]],1,mean)
m_log=apply(ctmK[[2]],1,mean)

k=c(kv_num)
df = ctmK[[1]]  # perplexity matrix
logLik = ctmK[[2]]  # perplexity matrix


write.csv(data.frame(k, df, logLik), paste(getwd(), &amp;quot;/Perplexity2_&amp;quot;,&amp;quot;gibbs5_100&amp;quot;, &amp;quot;.csv&amp;quot;, sep = &amp;quot;&amp;quot;))

# save the figure
png(paste(getwd(), &amp;quot;/Perplexity2_&amp;quot;,try_num, &amp;quot;_gibbs5_100&amp;quot;,&amp;quot;.png&amp;quot;, sep = &#39;&#39;),
    width=5, height=5,
    units=&amp;quot;in&amp;quot;, res=700)


matplot(k, df, type = c(&amp;quot;b&amp;quot;), xlab = &amp;quot;Number of topics&amp;quot;,
        ylab = &amp;quot;Perplexity&amp;quot;, pch=1:try_num,col = 1, main = &#39;&#39;)       
legend(&amp;quot;topright&amp;quot;, legend = paste(&amp;quot;fold&amp;quot;, 1:try_num), col=1, pch=1:try_num)

dev.off()

png(paste(getwd(), &amp;quot;/LogLikelihood2_&amp;quot;, &amp;quot;gibbs5_100&amp;quot;,&amp;quot;.png&amp;quot;, sep = &#39;&#39;),
    width=5, height=5,
    units=&amp;quot;in&amp;quot;, res=700)
matplot(k, logLik, type = c(&amp;quot;b&amp;quot;), xlab = &amp;quot;Number of topics&amp;quot;,
        ylab = &amp;quot;Log-Likelihood&amp;quot;, pch=1:try_num,col = 1, main = &#39;&#39;)       
legend(&amp;quot;topright&amp;quot;, legend = paste(&amp;quot;fold&amp;quot;, 1:try_num), col=1, pch=1:try_num)
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{:lang=&amp;ldquo;ruby&amp;rdquo;}&lt;/p&gt;

&lt;p&gt;于是可以得到对数似然率和主题数量的关系图，如下所示。可见选择10作为主题数量是比较合适的。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm4.staticflickr.com/3727/9953452316_6518296960.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;以下，我们将主要对主题数量为10的主题模型进行估计。topicmodels这个R包是由Bettina Grun和
Johannes Kepler两个人贡献的，目前支持VEM, VEM (fixed alpha)，Gibbs和CTM四种主题模型，关于其详细介绍，可以阅读他们的论文，关于主题模型的更多背景知识可以阅读Blei的相关文章。闲话少叙，看一下R代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# &#39;Refer to http://cos.name/2013/08/something_about_weibo/&#39;
k = 10
SEED &amp;lt;- 2003
jss_TM2 &amp;lt;- list(
  VEM = LDA(dtm, k = k, control = list(seed = SEED)),
  VEM_fixed = LDA(dtm, k = k, control = list(estimate.alpha = FALSE, seed = SEED)),
  Gibbs = LDA(dtm, k = k, method = &amp;quot;Gibbs&amp;quot;,
              control = list(seed = SEED, burnin = 1000, thin = 100, iter = 1000)),
  CTM = CTM(dtm, k = k,
            control = list(seed = SEED, var = list(tol = 10^-4), em = list(tol = 10^-3))) )   
save(jss_TM2, file = paste(getwd(), &amp;quot;/jss_TM2.Rdata&amp;quot;, sep = &amp;quot;&amp;quot;))
save(jss_TM, file = paste(getwd(), &amp;quot;/jss_TM1.Rdata&amp;quot;, sep = &amp;quot;&amp;quot;))

termsForSave1&amp;lt;- terms(jss_TM2[[&amp;quot;VEM&amp;quot;]], 10)
termsForSave2&amp;lt;- terms(jss_TM2[[&amp;quot;VEM_fixed&amp;quot;]], 10)
termsForSave3&amp;lt;- terms(jss_TM2[[&amp;quot;Gibbs&amp;quot;]], 10)
termsForSave4&amp;lt;- terms(jss_TM2[[&amp;quot;CTM&amp;quot;]], 10)

write.csv(as.data.frame(t(termsForSave1)),
          paste(getwd(), &amp;quot;/topic-document_&amp;quot;, &amp;quot;_VEM_&amp;quot;, k, &amp;quot;_2.csv&amp;quot;, sep=&amp;quot;&amp;quot;),
          fileEncoding = &amp;quot;UTF-8&amp;quot;)

write.csv(as.data.frame(t(termsForSave2)),
          paste(getwd(), &amp;quot;/topic-document_&amp;quot;, &amp;quot;_VEM_fixed_&amp;quot;, k, &amp;quot;_2.csv&amp;quot;, sep=&amp;quot;&amp;quot;),
          fileEncoding = &amp;quot;UTF-8&amp;quot;)

write.csv(as.data.frame(t(termsForSave3)),
          paste(getwd(), &amp;quot;/topic-document_&amp;quot;, &amp;quot;_Gibbs_&amp;quot;, k, &amp;quot;_2.csv&amp;quot;, sep=&amp;quot;&amp;quot;),
          fileEncoding = &amp;quot;UTF-8&amp;quot;)
write.csv(as.data.frame(t(termsForSave4)),
          paste(getwd(), &amp;quot;/topic-document_&amp;quot;, &amp;quot;_CTM_&amp;quot;, k, &amp;quot;_2.csv&amp;quot;, sep=&amp;quot;&amp;quot;),
          fileEncoding = &amp;quot;UTF-8&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{:lang=&amp;ldquo;ruby&amp;rdquo;}&lt;/p&gt;

&lt;p&gt;对主题模型进行估计之后，一般选择展示每个主题的前10个词语。因为主题之间可以共享相同词语，所以构成网络关系，因此这里我选择用网络的方法展示其结果。首先看一下吉布斯抽样算法得到的主题网络图，其R程序如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#&#39;topic graphs&#39;


tfs = as.data.frame(termsForSave3, stringsAsFactors = F); tfs[,1]
adjacent_list = lapply(1:10, function(i) embed(tfs[,i], 2)[, 2:1])
edgelist = as.data.frame(do.call(rbind, adjacent_list), stringsAsFactors =F)
# topic = unlist(lapply(1:10, function(i) rep(i, 9)))
edgelist$topic = topic
g &amp;lt;-graph.data.frame(edgelist,directed=T )
l&amp;lt;-layout.fruchterman.reingold(g)
# edge.color=&amp;quot;black&amp;quot;
nodesize = centralization.degree(g)$res
V(g)$size = log( centralization.degree(g)$res )

nodeLabel = V(g)$name
E(g)$color =  unlist(lapply(sample(colors()[26:137], 10), function(i) rep(i, 9))); unique(E(g)$color)

# 保存图片格式
png(  paste(getwd(), &amp;quot;/topic_graph_gibbs.png&amp;quot;, sep=&amp;quot;&amp;quot;）,
    width=5, height=5,
    units=&amp;quot;in&amp;quot;, res=700)

plot(g, vertex.label= nodeLabel,  edge.curved=TRUE,
     vertex.label.cex =0.5,  edge.arrow.size=0.2, layout=l )

# 结束保存图片
dev.off()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{:lang=&amp;ldquo;ruby&amp;rdquo;}&lt;/p&gt;

&lt;p&gt;得到的图形如下：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm8.staticflickr.com/7330/9953452256_fb1baaa16d_c.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;当然，我们还可以看一下其他算法得到的网络图，这里我们看一下根据VEM和CTM两个主题模型得到的网络图。&lt;/p&gt;

&lt;p&gt;CTM模型的主题网络图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm8.staticflickr.com/7425/9953448564_d24b8e348a_c.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;VEM模型的主题网络图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm4.staticflickr.com/3727/9953448704_ec8291a0e3_c.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>使用聚类分析为主题模型划分主题类型</title>
      <link>https://chengjun.github.io/zh/post/cn/2013-09-08-using-cluster-analysis-to-classify-topics-generated-by-topic-modeling/</link>
      <pubDate>Sun, 08 Sep 2013 00:00:00 +0000</pubDate>
      
      <guid>https://chengjun.github.io/zh/post/cn/2013-09-08-using-cluster-analysis-to-classify-topics-generated-by-topic-modeling/</guid>
      <description>&lt;p&gt;使用主题模型（topic models）可以较为高效地划分文本的主题，但一个不得不面对的问题是有时候主题的划分过细，使得解读和归类成为困难。其实，聚类分析作为一个“古老”的分析方法可以较为简洁的解决这个问题。&lt;/p&gt;

&lt;p&gt;举一个小例子，我们主要使用tm这个R包来完成文本挖掘的前期任务。在得到DocumentTermMatrix之后，可以通过计算cosine 相似度的方法来计算文本之间的不一致性（dissimilarity）。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Using cluster analysis to classify topics generated by topic modeling
# 2013 Sep 08
# Cheng-Jun Wang

library(tm)
library(topicmodels)
require(proxy)


data(acq)
data(crude)

m &amp;lt;- c(acq, crude)
dtm &amp;lt;- DocumentTermMatrix(m)
dtm &amp;lt;- removeSparseTerms(dtm, 0.8)
inspect(dtm[1:5, 1:5])

# cluster analysis of documents based on DocumentTermMatrix
dist_dtm &amp;lt;- dissimilarity(mtd, method = &#39;cosine&#39;)
hc &amp;lt;- hclust(dist_dtm, method = &#39;ave&#39;)
plot(hc, xlab = &#39;&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{:lang=&amp;ldquo;ruby&amp;rdquo;}&lt;/p&gt;

&lt;p&gt;我在做RA的时候，面临的一个问题就是在做主体模型的时候出现的：模型拟合得到的主题数量太多。我们用下面这个例子进行简单的介绍。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# topic modeling
topic_num = 50
for (k in c(topic_num))  {
  # k &amp;lt;- 10
  SEED &amp;lt;- 2010
  jss_TM &amp;lt;- list(
    VEM = LDA(dtm, k = k, control = list(seed = SEED)),
    VEM_fixed = LDA(dtm, k = k, control = list(estimate.alpha = FALSE, seed = SEED)),
    Gibbs = LDA(dtm, k = k, method = &amp;quot;Gibbs&amp;quot;,
                control = list(seed = SEED, burnin = 1000, thin = 100, iter = 1000))    )
}

rs = posterior(jss_TM$Gibbs, dtm)

mtd = t(rs$topics) # topics and documents
mtt = rs$terms  # topic and terms
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{:lang=&amp;ldquo;ruby&amp;rdquo;}&lt;/p&gt;

&lt;p&gt;使用k mean聚类方法的好处是可以较为方便地知道聚类的数量&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#########################################
#
# K means analysis for rownames of a matrix
#
#########################################


# Determine number of clusters
mydata = mtt
wss &amp;lt;- (nrow(mydata)-1)*sum(apply(mydata,2,var))
max_group = nrow(mydata)-1
for (i in 2:max_group) wss[i] &amp;lt;- sum(kmeans(mydata,
                                            centers=i)$withinss)
plot(1:max_group, wss, type=&amp;quot;b&amp;quot;, xlab=&amp;quot;Number of Clusters&amp;quot;,
     ylab=&amp;quot;Within groups sum of squares&amp;quot;)
##
# K-Means Cluster Analysis
fit &amp;lt;- kmeans(mydata, 5) # 5 cluster solution
# get cluster means
cluster_means = aggregate(mydata,by=list(fit$cluster),FUN=mean)
# append cluster assignment
mydata &amp;lt;- data.frame(rownames(mydata), fit$cluster)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{:lang=&amp;ldquo;ruby&amp;rdquo;}&lt;/p&gt;

&lt;p&gt;以下，我们对矩阵计算cosine similarity并使用阶层聚类方法得到结果。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;############################################
#
#  Hierarchical Clustering
#
############################################

cos.sim &amp;lt;- function(ix)
{
  A = X[ix[1],]
  B = X[ix[2],]
  return( sum(A*B)/sqrt(sum(A^2)*sum(B^2)) )
}   

mdt = as.matrix(dtm)
X = mtt  # whether to scale it


n &amp;lt;- nrow(X)
cmb &amp;lt;- expand.grid(i=1:n, j=1:n)
simdt &amp;lt;- matrix(apply(cmb,1,cos.sim),n,n)
rownames(simdt) = rownames(mtt)
hc &amp;lt;- hclust(dist(simdt, method = &amp;quot;euclidean&amp;quot;), method = &#39;ave&#39;)
# hc &amp;lt;- hclust(dist(simdt)^2, method = &#39;cen&#39;)

plot(hc, xlab = &#39;&#39;)
k = 10
groups &amp;lt;- cutree(hc, k=k)  # cut tree into 5 clusters
rect.hclust(hc, k=k, border=&amp;quot;red&amp;quot;) # draw dendogram with red borders
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{:lang=&amp;ldquo;ruby&amp;rdquo;}&lt;/p&gt;

&lt;p&gt;看一下效果吧：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm8.staticflickr.com/7302/9699627576_6744f02576_c.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;当然了，如果你觉得这个方法过于粗暴，还可以尝试构建主题网络并进行社区划分的方法。不再赘述。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>使用R做主题模型：词语筛选和主题数量确定</title>
      <link>https://chengjun.github.io/zh/post/cn/2013-08-31-topic-modeling-with-r/</link>
      <pubDate>Sat, 31 Aug 2013 00:00:00 +0000</pubDate>
      
      <guid>https://chengjun.github.io/zh/post/cn/2013-08-31-topic-modeling-with-r/</guid>
      <description>

&lt;h2 id=&#34;1-筛选单词&#34;&gt;1. 筛选单词&lt;/h2&gt;

&lt;p&gt;在数据清理（pre-processing）之后，需要对数据进行适当筛选。对数据的筛选包括至少两个步骤：&lt;/p&gt;

&lt;p&gt;第一步，在DocumentTermMatrix中设定&lt;/p&gt;

&lt;p&gt;使用R的topicmodels发现设定在DocumentTermMatrix里的约束条件失效，解决方法&lt;a href=&#34;http://stackoverflow.com/questions/13366897/r-documenttermmatrix-control-list-not-working&#34; target=&#34;_blank&#34;&gt;在此&lt;/a&gt;，其实在topicmodels的包里也粗略提及，只是用习惯了tm包的人觉得二者是无缝对接的。其实还很多差异，比如在tm里相似功能称之为TermDocumentMatrix&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dtm &amp;lt;- DocumentTermMatrix(corpus, control = list(stemming = TRUE,  
                                                 stopwords = TRUE,
                                                 wordLengths=c(4, 15),
                                                 bounds = list(global = c(5,Inf)), # each term appears in at least 5 docs
                                                 removeNumbers = TRUE,
                                                 removePunctuation  = list(preserve_intra_word_dashes = FALSE)
                                                 #,encoding = &amp;quot;UTF-8&amp;quot;
                                                 )
                         )


colnames(dtm)   ##  inspect all the words for errors
dim(dtm)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{:lang=&amp;ldquo;ruby&amp;rdquo;}&lt;/p&gt;

&lt;p&gt;第二步，通过tf-idf和col_sums选择高频词&lt;/p&gt;

&lt;p&gt;这背后的逻辑在于主题模型是要对文本进行分类，频次较少的词的贡献并不大。但会显著的占用计算资源。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;term_tfidf &amp;lt;-tapply(dtm$v/row_sums(dtm)[dtm$i], dtm$j, mean) * log2(nDocs(dtm)/col_sums(dtm &amp;gt; 0))

l1=term_tfidf &amp;gt;= quantile(term_tfidf, 0.1)       # fix this!
dtm &amp;lt;- dtm[,l1]
l2=col_sums(dtm) &amp;gt;= quantile(col_sums(dtm), 0.1)  # fix this!
dtm &amp;lt;- dtm[,l2]

dtm = dtm[row_sums(dtm)&amp;gt;0, ]; dim(dtm) # 2246 6210
range(col_sums(dtm))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{:lang=&amp;ldquo;ruby&amp;rdquo;}&lt;/p&gt;

&lt;h2 id=&#34;2-确定主题数量&#34;&gt;2. 确定主题数量&lt;/h2&gt;

&lt;p&gt;在对单个词进行筛选之后，就可以正式进行主题模型的设定了。这是一步非常耗时间的工作，但第一步当然是理解主题模型的基本思路。&lt;/p&gt;

&lt;p&gt;主题模型是在概率潜在语义分析（probabilistic latent semanticanalysis，PLSI）的基础上发展起来的。从对矩阵进行因子分解的角度而言，可以看做是对离散数据进行主成分分析。其具体内容可以参见Blei发表在Communication of ACM上的&lt;a href=&#34;http://cacm.acm.org/magazines/2012/4/147361-probabilistic-topic-models/fulltext&#34; target=&#34;_blank&#34;&gt;文献回顾文章&lt;/a&gt;（Blei(2012) Probalistic topic models. Communication of ACM, 55, 77-84）。&lt;/p&gt;

&lt;p&gt;简而言之，我们看到是单词在文本中的分布。1. 我们认为在单词和文本之间存在潜在的主题，并且每个主题$$β&lt;em&gt;{1:K}$$可以表达为一些词语在这个文本里的分布；2. 一篇文章可能对应多个主题，假设我们已经知道了存在哪些主题，那么一个主题在一个文本中的比例$$θ&lt;/em&gt;{d:k}$$也应该知道；3. 我们将主题和词语对应起来，建立一个映射$$z&lt;em&gt;{d:n}$$，这样我们就知道把文章d中的第n个词赋给哪个主题；4.但实际上前三步都是不能直接观察到的，之间看到的就是词语在文本中的分布$$w&lt;/em&gt;{d:n}$$ ，例如文本d当中第n个词语是什么，即词在文本中的分布。&lt;/p&gt;

&lt;p&gt;前三个隐变量和第四个先变量之间的联合分布（joint distribution）就是主题生成的过程，这个过程还可以使用概率图模型的方法进行建模。如下图所示：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm4.staticflickr.com/3756/9634040055_1bdc1c8013.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;其主要逻辑是机器学习的思路：给定了可以观察到的词语在文本中的分布$$w_{d:n}$$，主题结构可以表达为一个条件分布：$$p(\beta _{1:K},\theta &lt;em&gt;{1:D},z&lt;/em&gt;{1:D} \mid w_{1:D})$$。这是后验分布的分析思路。因为可能的主题结构太多，这个后验分布无法计算出来，而主题模型的主要目的也只是逼近这个后验分布。&lt;/p&gt;

&lt;p&gt;通常逼近这个后验分布的方法可以分为两类：1. 变异算法（variational algorithms）,这是一种决定论式的方法。变异式算法假设一些参数分布，并根据这些理想中的分布与后验的数据相比较，并从中找到最接近的。由此，将一个估计问题转化为最优化问题。最主要的算法是变异式的期望最大化算法(variational expectation-maximization，VEM)。这个方法是最主要使用的方法。在R软件的tomicmodels包中被重点使用。 2. 基于抽样的算法。抽样的算法，如吉布斯抽样（gibbs sampling）主要是构造一个马尔科夫链，从后验的实证的分布中抽取一些样本，以之估计后验分布。吉布斯抽样的方法在R软件的lda包中广泛使用。&lt;/p&gt;

&lt;p&gt;常用的主题模型是LDA, 可以使用VEM和gibbs两种方法估计。之后的模型的发展，主要是要放松严格的模型假设，其中之一是允许主题之间存在相关。由此Blei等人提出了相关的主题模型（correalted topic model，CTM），可以使用VEM方法估计。在本文当中，我们采用CTM为例。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;fold_num = 10
kv_num = c(5, 10*c(1:5, 10))
seed_num = 2003


smp&amp;lt;-function(cross=fold_num,n,seed)
{
  set.seed(seed)
  dd=list()
  aa0=sample(rep(1:cross,ceiling(n/cross))[1:n],n)
  for (i in 1:cross) dd[[i]]=(1:n)[aa0==i]
  return(dd)
}

selectK&amp;lt;-function(dtm,kv=kv_num,SEED=seed_num,cross=fold_num,sp) # change 60 to 15
{
  per_ctm=NULL
  log_ctm=NULL
  for (k in kv)
  {
    per=NULL
    loglik=NULL
    for (i in 1:3)  #only run for 3 replications#
    {
      cat(&amp;quot;R is running for&amp;quot;, &amp;quot;topic&amp;quot;, k, &amp;quot;fold&amp;quot;, i,
          as.character(as.POSIXlt(Sys.time(), &amp;quot;Asia/Shanghai&amp;quot;)),&amp;quot;\n&amp;quot;)
      te=sp[[i]]
      tr=setdiff(1:nrow(dtm),te)

      # VEM = LDA(dtm[tr, ], k = k, control = list(seed = SEED)),
      # VEM_fixed = LDA(dtm[tr,], k = k, control = list(estimate.alpha = FALSE, seed = SEED)),

      CTM = CTM(dtm[tr,], k = k,
                control = list(seed = SEED, var = list(tol = 10^-4), em = list(tol = 10^-3)))  

      # Gibbs = LDA(dtm[tr,], k = k, method = &amp;quot;Gibbs&amp;quot;,
      # control = list(seed = SEED, burnin = 1000,thin = 100, iter = 1000))

      per=c(per,perplexity(CTM,newdata=dtm[te,]))
      loglik=c(loglik,logLik(CTM,newdata=dtm[te,]))
    }
    per_ctm=rbind(per_ctm,per)
    log_ctm=rbind(log_ctm,loglik)
  }
  return(list(perplex=per_ctm,loglik=log_ctm))
}

sp=smp(n=nrow(dtm),seed=seed_num)

system.time((ctmK=selectK(dtm=dtm,kv=kv_num,SEED=seed_num,cross=fold_num,sp=sp)))

## plot the perplexity

m_per=apply(ctmK[[1]],1,mean)
m_log=apply(ctmK[[2]],1,mean)

k=c(kv_num)
df = ctmK[[1]]  # perplexity matrix
matplot(k, df, type = c(&amp;quot;b&amp;quot;), xlab = &amp;quot;Number of topics&amp;quot;,
        ylab = &amp;quot;Perplexity&amp;quot;, pch=1:5,col = 1, main = &#39;&#39;)       
legend(&amp;quot;bottomright&amp;quot;, legend = paste(&amp;quot;fold&amp;quot;, 1:5), col=1, pch=1:5)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{:lang=&amp;ldquo;ruby&amp;rdquo;}&lt;/p&gt;

&lt;p&gt;有趣的是计算时间：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; system.time((ctmK=selectK(dtm=dtm,kv=kv_num,SEED=seed_num,cross=fold_num,sp=sp)))
R is running for topic 5 fold 1 2013-08-31 18:26:32
R is running for topic 5 fold 2 2013-08-31 18:26:39
R is running for topic 5 fold 3 2013-08-31 18:26:45
R is running for topic 10 fold 1 2013-08-31 18:26:50
R is running for topic 10 fold 2 2013-08-31 18:27:14
R is running for topic 10 fold 3 2013-08-31 18:27:36
R is running for topic 20 fold 1 2013-08-31 18:27:57
R is running for topic 20 fold 2 2013-08-31 18:29:42
R is running for topic 20 fold 3 2013-08-31 18:32:00
R is running for topic 30 fold 1 2013-08-31 18:33:42
R is running for topic 30 fold 2 2013-08-31 18:37:39
R is running for topic 30 fold 3 2013-08-31 18:45:46
R is running for topic 40 fold 1 2013-08-31 18:52:52
R is running for topic 40 fold 2 2013-08-31 18:57:26
R is running for topic 40 fold 3 2013-08-31 19:00:31
R is running for topic 50 fold 1 2013-08-31 19:03:47
R is running for topic 50 fold 2 2013-08-31 19:04:02
R is running for topic 50 fold 3 2013-08-31 19:04:52
R is running for topic 100 fold 1 2013-08-31 19:05:42
R is running for topic 100 fold 2 2013-08-31 19:06:05
R is running for topic 100 fold 3 2013-08-31 19:06:28
   user  system elapsed
2417.801.13 2419.28
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;{:lang=&amp;ldquo;ruby&amp;rdquo;}&lt;/p&gt;

&lt;p&gt;看一下最终绘制的perplexity的图，如下可见，在本例当中，当主题数量为30的时候，perplexity最小，模型的最大似然率最高，由此确定主题数量为30。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm8.staticflickr.com/7290/9633619385_13a1472d1b.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>使用R做主题模型：举一个处理Encoding问题的例子</title>
      <link>https://chengjun.github.io/zh/post/cn/2013-08-29-encoding-in-r-for-text-mining/</link>
      <pubDate>Thu, 29 Aug 2013 00:00:00 +0000</pubDate>
      
      <guid>https://chengjun.github.io/zh/post/cn/2013-08-29-encoding-in-r-for-text-mining/</guid>
      <description>

&lt;h2 id=&#34;引言&#34;&gt;引言&lt;/h2&gt;

&lt;h3 id=&#34;遭遇-邪恶的拉丁引号&#34;&gt;遭遇“邪恶的拉丁引号”&lt;/h3&gt;

&lt;p&gt;我遇到的问题比较复杂，因为原文里混合了latin1和UTF-8两种encoding的字形，最初我统一再读入text数据的时候采用encoding =&amp;ldquo;UTF-8&amp;rdquo;的方法，结果发现了很多奇诡的单引号和双引号错误。在生成的DocumentTermMatrix里出现了很多以引号开始或结束的terms，例如：“grandfather， “deputy with the constitution” 。用Encoding命令看一下它的原形是：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;gt; Encoding(&amp;quot;“&amp;quot;)
[1] &amp;quot;latin1&amp;quot;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;只所以说是原形，是因为它们可以变形！&amp;rdquo;â€œ&amp;rdquo;， &amp;ldquo;â€™&amp;rdquo;， &amp;ldquo;â€\u009d&amp;rdquo;， &amp;ldquo;â€&amp;rdquo;都是它在不设定Encoding的环境下的形状。但我觉得不足以刻画我对它的厌恶，特别附图一张：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm6.staticflickr.com/5521/9621347348_c9b66db982_o.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;直到最后，我也没彻底搞定这些邪恶的拉丁引号，但我使用了一些tricks解决的我的问题。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://farm4.staticflickr.com/3703/9618226049_b87d57c266.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;1-读入数据不设定encoding&#34;&gt;1. 读入数据不设定encoding！&lt;/h2&gt;

&lt;p&gt;因为邪恶的拉丁引号在UTF-8格式下根本就无法对付，在不设encoding方法的时候，它们现身为â€“, â€™, â€œ等形式，还可以对付。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;dat1 = read.csv(&amp;quot;D:/chengjun/Crystal/Schwab_data_cleaningSep.csv&amp;quot;,
               header = F, sep = &amp;quot;|&amp;quot;, quote = &amp;quot;&amp;quot;, stringsAsFactors=F,
           fileEncoding = &amp;quot;&amp;quot;) # , encoding =&amp;quot;UTF-8&amp;quot;); dim(dat1)

names(dat1) = c(&#39;name&#39;, &#39;organization&#39;, &#39;year&#39;, &#39;country&#39;, &#39;website&#39;,
               &#39;shortIntro&#39;, &#39;focus&#39;, &#39;geo&#39;, &#39;model&#39;, &#39;benefit&#39;, &#39;budget&#39;,
             &#39;revenue&#39;, &#39;recognization&#39;, &#39;background&#39;,
             &#39;innovation&#39;, &#39;entrepreneur&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;2-文本数据清理第一步-载入r包-选取变量&#34;&gt;2. 文本数据清理第一步：载入R包，选取变量&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;library(tm)
library(topicmodels)

text = dat1$entrepreneur
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;3-终于可以删除部分邪恶的拉丁引号&#34;&gt;3. 终于可以删除部分邪恶的拉丁引号&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;text = gsub(&amp;quot;.&amp;quot;, &amp;quot; &amp;quot;, text, fixed = TRUE )
text = gsub(&amp;quot;!&amp;quot;, &amp;quot; &amp;quot;, text, fixed = TRUE )
text = gsub(&amp;quot;?&amp;quot;, &amp;quot; &amp;quot;, text, fixed = TRUE )
text = gsub(&amp;quot;â€œ&amp;quot;, &amp;quot; &amp;quot;, text, fixed = TRUE )
text = gsub(&amp;quot;â€™&amp;quot;, &amp;quot; &amp;quot;, text, fixed = TRUE )
text = gsub(&amp;quot;â€\u009d&amp;quot;, &amp;quot; &amp;quot;, text, fixed = TRUE )
text = gsub(&amp;quot;â€&amp;quot;, &amp;quot; &amp;quot;, text,  fixed = TRUE )
text = gsub(&amp;quot;&amp;lt;/b&amp;gt;&amp;quot;, &amp;quot; &amp;quot;, text,  fixed = TRUE )
text = gsub(&amp;quot;&amp;lt;b&amp;gt;&amp;quot;, &amp;quot; &amp;quot;, text,  fixed = TRUE )
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;注意：1. 这些不规则的latin表达在r的script里再次打开会改变。所以，每次使用都要来这个网页里粘贴复制，也算是一种state-of-art，markdown都比R script保存的持久啊。2.使用gsub的代价是corpus被再度转化为character。所以这段代码如放在下面使用还要用Corpus命令再度转回来。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;corpus &amp;lt;- Corpus(   VectorSource( text )  )  # corpus[[1]] ## inspect the first corpus
# make each letter lowercase
corpus &amp;lt;- tm_map(corpus, tolower)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;4-我这里根据研究需要-要剔除人名-地名-组织名&#34;&gt;4. 我这里根据研究需要，要剔除人名、地名、组织名。&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;# remove generic and custom stopwords
human_find_special_cases = c(&amp;quot;Fundação Pró-Cerrado&amp;quot;, &amp;quot;Fundação PróCerrado&amp;quot;, &amp;quot;FundaÃ§Ã£o PrÃ³-Cerrado&amp;quot;)
my_stopwords &amp;lt;- c(dat1$country,
              dat1$organization,
            dat1$name,
            human_find_special_cases)

my_stopwords &amp;lt;- Corpus(   VectorSource(my_stopwords)  )
my_stopwords &amp;lt;- tm_map(my_stopwords, tolower) # to lowercase my_stopwords here

# Finally we can delete the country/org/person names
corpus &amp;lt;- tm_map(corpus, removeWords, my_stopwords); corpus[[1]]

corpus &amp;lt;- tm_map(corpus, removePunctuation)
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;5-将语料转化为documenttermmatrix&#34;&gt;5. 将语料转化为DocumentTermMatrix&lt;/h2&gt;

&lt;pre&gt;&lt;code&gt;# install.packages(&amp;quot;SnowballC&amp;quot;)
Sys.setlocale(&amp;quot;LC_COLLATE&amp;quot;, &amp;quot;C&amp;quot;) # set this for reproducible results

corpus &amp;lt;- Corpus(   VectorSource( corpus )  )  # corpus[[1]] ## inspect the first corpus
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;另外，到这里还是会有孤立的邪恶的拉丁单引号存在，但已经和其它term分开了，在以下DocumentTermMatrix的通过设置minWordLength = 3可以将其完全清理。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# corpus = tm_map(corpus, function(x) iconv(enc2utf8(x), sub = &amp;quot;byte&amp;quot;)) # very important to convert encoding
JSS_dtm &amp;lt;- DocumentTermMatrix(corpus, control = list(stemming = TRUE,  stopwords = TRUE,
                                     minWordLength = 3, removeNumbers = TRUE,
                                     removePunctuation = TRUE
                                     # weighting =
                                             #  function(x)
                                             #  weightTfIdf(x, normalize =FALSE),
                                     ,encoding = &amp;quot;UTF-8&amp;quot;
                               )  )

findFreqTerms(JSS_dtm, lowfreq=0) # 一定要看一下还有没有错误。inspect all the words for errors
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;6-你需要的一些背景知识&#34;&gt;6. 你需要的一些背景知识&lt;/h2&gt;

&lt;h3 id=&#34;6-1-如何识别和转化encoding的类型&#34;&gt;6.1 如何识别和转化encoding的类型?&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;iconvlist() # 看一下玲琅满目的encoding方法
iconv(x, from, to, sub=NA) # Convert Character Vector between Encodings

## convert from Latin-2 to UTF-8: two of the glibc iconv variants.
iconv(x, &amp;quot;ISO_8859-2&amp;quot;, &amp;quot;UTF-8&amp;quot;)
iconv(x, &amp;quot;LATIN2&amp;quot;, &amp;quot;UTF-8&amp;quot;)

## Both x below are in latin1 and will only display correctly in a
## latin1 locale.
(x &amp;lt;- &amp;quot;fa\xE7ile&amp;quot;)
charToRaw(xx &amp;lt;- iconv(x, &amp;quot;latin1&amp;quot;, &amp;quot;UTF-8&amp;quot;))
## in a UTF-8 locale, print(xx)

iconv(x, &amp;quot;latin1&amp;quot;, &amp;quot;ASCII&amp;quot;)          #   NA
iconv(x, &amp;quot;latin1&amp;quot;, &amp;quot;ASCII&amp;quot;, &amp;quot;?&amp;quot;)     # &amp;quot;fa?ile&amp;quot;
iconv(x, &amp;quot;latin1&amp;quot;, &amp;quot;ASCII&amp;quot;, &amp;quot;&amp;quot;)      # &amp;quot;faile&amp;quot;
iconv(x, &amp;quot;latin1&amp;quot;, &amp;quot;ASCII&amp;quot;, &amp;quot;byte&amp;quot;)  # &amp;quot;fa&amp;lt;e7&amp;gt;ile&amp;quot;

# Extracts from R help files
(x &amp;lt;- c(&amp;quot;Ekstr\xf8m&amp;quot;, &amp;quot;J\xf6reskog&amp;quot;, &amp;quot;bi\xdfchen Z\xfcrcher&amp;quot;))
iconv(x, &amp;quot;latin1&amp;quot;, &amp;quot;ASCII//TRANSLIT&amp;quot;)
iconv(x, &amp;quot;latin1&amp;quot;, &amp;quot;ASCII&amp;quot;, sub=&amp;quot;byte&amp;quot;)
## End(Not run)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;6-2-如何看documenttermmatrix的内容&#34;&gt;6.2 如何看DocumentTermMatrix的内容？&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;inspect(JSS_dtm)
colnames(JSS_dtm) # we can find that: [3206]   &amp;quot;works\u009d&amp;quot;    [3207]      &amp;quot;work\u009d&amp;quot;
inspect(JSS_dtm[,3206])
inspect(JSS_dtm[,&amp;quot;works\u009d&amp;quot;])
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;6-3-关于一些需要跳出的符号&#34;&gt;6.3 关于一些需要跳出的符号&lt;/h3&gt;

&lt;pre&gt;&lt;code&gt;unlist(strsplit(&amp;quot;a.b.c&amp;quot;, &amp;quot;.&amp;quot;))
## [1] &amp;quot;&amp;quot; &amp;quot;&amp;quot; &amp;quot;&amp;quot; &amp;quot;&amp;quot; &amp;quot;&amp;quot;
## Note that &#39;split&#39; is a regexp!
## If you really want to split on &#39;.&#39;, use
unlist(strsplit(&amp;quot;a.b.c&amp;quot;, &amp;quot;[.]&amp;quot;))
## [1] &amp;quot;a&amp;quot; &amp;quot;b&amp;quot; &amp;quot;c&amp;quot;
## or
unlist(strsplit(&amp;quot;a.b.c&amp;quot;, &amp;quot;.&amp;quot;, fixed = TRUE))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;同理，gsub查找替换掉句点comma的时候也有类似的问题。我在使用python处理相邻多个段落的时候，直接使用了list的append的方法，导致两个自然段之间没有空格。这可要害苦我了。很多可以作为段落结尾的标点都要转化为空格。幸好老外写文章没有那么多问号和叹号结尾。&lt;/p&gt;

&lt;p&gt;fixed = TRUE意味着要use exact matching.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;text1 = gsub(&amp;quot;[.]&amp;quot;, &amp;quot; &amp;quot;, text）
text1 = gsub(&amp;quot;[.]&amp;quot;, &amp;quot; &amp;quot;, text, fixed = TRUE )
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>QAP检验：计算两个网络的关联</title>
      <link>https://chengjun.github.io/zh/post/cn/2013-08-04-qap-test-of-network-analysis/</link>
      <pubDate>Sun, 04 Aug 2013 00:00:00 +0000</pubDate>
      
      <guid>https://chengjun.github.io/zh/post/cn/2013-08-04-qap-test-of-network-analysis/</guid>
      <description>

&lt;h3 id=&#34;qap检验-两个网络之间的关联&#34;&gt;QAP检验：两个网络之间的关联&lt;/h3&gt;

&lt;p&gt;通常一组个体具有多种类型的关系，例如友谊关系和经济往来关系。我们通常会对这两种网络关系在多大程度上相互关联感兴趣。当我们知道一组个体之间的两种关系网络，我们就可以计算这个两个关系网络之间的相关程度。在统计学当中，皮尔森相关系数是用来反映两个变量线性相关程度的统计量。与之类似，对于由一组个体所组成的两个网络，也可以计算其相应的相关皮尔逊相关系数。当然，还可以计算其他你感兴趣的统计量，如协相关系数。&lt;/p&gt;

&lt;p&gt;我们使用sna这个R软件包来计算网络相关系数（并调用qaptest命令）。通过安装和使用statnet这个R软件包，就会自动加载sna等子软件包。另外，statnet当中还集成了其他的几个相关的R软件包，包括进行动态网络建模的tergm子软件包。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# R程序11-8：计算网络的皮尔逊相关系数
install.packages(&amp;quot;statnet&amp;quot;)
library(statnet)  
# 首先随机生成3个由10个节点构成的有向网络
g=array(dim=c(3,10,10))
g[1,,] = rgraph(10)
g[2,,] = rgraph(10,tprob=g[1,,]*0.8) # 设置g1和g2两个网络强相关
g[3,,] = 1; g[3,1,2] = 0 # g3接近于一个派系（clique）
# 绘制这3个网络
par(mfrow=c(1,3))
for(i in 1:3) {
    gplot(g[i,,],usecurv=TRUE, mode = &amp;quot;fruchtermanreingold&amp;quot;,
          vertex.sides=3:8)}
#计算网络的相关矩阵
gcor(g)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在通常使用皮尔逊相关系数的时候，可以用t统计量对总体相关系数为0的原假设进行检验。但在计算网络的相关系数（graph correlations）时，经典的零假设检验方法往往会带来偏差，因而并不适用。通常使用非参数检验的方法，比如QAP(Quadratic Assignment Procedure)检验。&lt;/p&gt;

&lt;p&gt;矩阵的随机排列（Random matrix permutations）是QAP检验的关键部分，在子软件包sna中主要通过rmperm来进行。通过矩阵的随机排列，可以对网络中的节点编号（而不是链接！！）进行随机置换（relabelling）或重新“洗牌”（reshuffling），并得到&lt;strong&gt;一组&lt;/strong&gt;（比如1000个）重连后的网络。因为只是置换节点，这种操作只是重新标记节点的编号（relabelling）。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# R程序11-9：矩阵的随机置换方法
j = rgraph(5) # 随机生成一个网络
j  #看一下这个网络的矩阵形式
rmperm(j) #随机置换后的网络的矩阵形式
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;对这一组重构的网络可以计算其网络级别的参数（如两个网络的相关参数，协相关参数），并因此得到&lt;strong&gt;一个参数分布&lt;/strong&gt;。QAP检验的零假设是实际观测到的网络参数（如）来自于这&lt;strong&gt;一个参数分布&lt;/strong&gt;。也就是说，原假设认为这种观测到的相关关系是由随机因素带来的，因而这种网络相关并不显著。拒绝原假设，就从统计的角度证明了观测到的网络相关系数是显著的。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# R程序11-10：QAP检验
q.12 = qaptest(g, gcor, g1 = 1, g2 = 2)
q.13 = qaptest(g, gcor, g1 = 1, g2 = 3)   
# 看一下QAP输出的结果
par(mfrow=c(1,2))
summary(q.12)
plot(q.12)
# 拒绝原假设，图1和图2显著相关
summary(q.13)
plot(q.13)
# 接受原假设，图1和图3不相关
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在检验这个关于两个网络是否存在相关的零假设的时候，我们计算置换后的参数分布中大于这个实际观测到的参数的比例，以及小于这个实际观测到的参数的比例。QAP检验返回实际数据中观测到的参数f(d)、通过置换所得到的参数f(perm)的数学分布、以及单尾的P值。其中单尾的p值包括两种情况：p(f(perm) &amp;gt;= f(d))和p(f(perm) &amp;lt;= f(d))。&lt;/p&gt;

&lt;p&gt;其中P(f(perm) &amp;gt;= f(d))表示随机置换矩阵的相关系数的大于与等于观测值的p值，也就是本研究的检验显著性。一般而言，当p(f(perm) &amp;gt;= f(d))小于p(f(perm) &amp;lt;= f(d))时，拒绝原假设。&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
