<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Nltk on Academic</title>
    <link>https://chengjunwang.com/en/tags/nltk/index.xml</link>
    <description>Recent content in Nltk on Academic</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2016 Cheng-Jun Wang</copyright>
    <atom:link href="/en/tags/nltk/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Sentiment analysis with Python</title>
      <link>https://chengjunwang.com/en/post/en/2012-03-19-sentiment-analysi-with-python/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chengjunwang.com/en/post/en/2012-03-19-sentiment-analysi-with-python/</guid>
      <description>&lt;p&gt;Learning To Do Sentiment Analysis Using Python &amp;amp; NLTK&lt;/p&gt;

&lt;p&gt;This is my first try to learn sentiment analysis using python. You can find the original post by Laurent Luce &lt;a href=&#34;http://www.laurentluce.com/posts/twitter-sentiment-analysis-using-python-and-nltk/&#34; target=&#34;_blank&#34;&gt;following this link&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I am glad to know that, with the aid of naive Bayes, NLTK could distinguish ‘like’ and ‘does not like’ into &amp;lsquo;positive&amp;rsquo; and &amp;lsquo;negative&amp;rsquo; respectively. I am wondering how R behaves compared with it. The method below employed the procedures depicted in the following figure.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://weblab.com.cityu.edu.hk/blog/chengjun/files/2012/03/overview-of-sentiment-analysis-using-nltk.png&#34; alt=&#34;original author: Laurent Luce&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure created by &lt;a href=&#34;http://www.laurentluce.com/posts/twitter-sentiment-analysis-using-python-and-nltk/&#34; target=&#34;_blank&#34;&gt;Laurent Luce&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Load in the data first.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Python script
import nltk

pos_tweets = [(&#39;I love this car&#39;, &#39;positive&#39;),
    (&#39;This view is amazing&#39;, &#39;positive&#39;),
    (&#39;I feel great this morning&#39;, &#39;positive&#39;),
    (&#39;I am so excited about the concert&#39;, &#39;positive&#39;),
    (&#39;He is my best friend&#39;, &#39;positive&#39;)]

neg_tweets = [(&#39;I do not like this car&#39;, &#39;negative&#39;),
    (&#39;This view is horrible&#39;, &#39;negative&#39;),
    (&#39;I feel tired this morning&#39;, &#39;negative&#39;),
    (&#39;I am not looking forward to the concert&#39;, &#39;negative&#39;),
    (&#39;He is my enemy&#39;, &#39;negative&#39;)]

tweets = []
for (words, sentiment) in pos_tweets + neg_tweets:
    words_filtered = [e.lower() for e in words.split() if len(e) &amp;gt;= 3]
    tweets.append((words_filtered, sentiment))

test_tweets = [
    ([&#39;feel&#39;, &#39;happy&#39;, &#39;this&#39;, &#39;morning&#39;], &#39;positive&#39;),
    ([&#39;larry&#39;, &#39;friend&#39;], &#39;positive&#39;),
    ([&#39;not&#39;, &#39;like&#39;, &#39;that&#39;, &#39;man&#39;], &#39;negative&#39;),
    ([&#39;house&#39;, &#39;not&#39;, &#39;great&#39;], &#39;negative&#39;),
    ([&#39;your&#39;, &#39;song&#39;, &#39;annoying&#39;], &#39;negative&#39;)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then we need to get the unique word list as the features for classification.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# get the word lists of tweets
def get_words_in_tweets(tweets):
    all_words = []
    for (words, sentiment) in tweets:
        all_words.extend(words)
    return all_words

# get the unique word from the word list
def get_word_features(wordlist):
    wordlist = nltk.FreqDist(wordlist)
    word_features = wordlist.keys()
    return word_features

word_features = get_word_features(get_words_in_tweets(tweets))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To create a classifier, we need to decide what features are relevant. To do that, we first need a feature extractor.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def extract_features(document):
    document_words = set(document)
    features = {}
    for word in word_features:
      features[&#39;contains(%s)&#39; % word] = (word in document_words)
    return features
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then, we can build up the training set and create the classifier:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;training_set = nltk.classify.util.apply_features(extract_features, tweets)

classifier = nltk.NaiveBayesClassifier.train(training_set)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You may want to know how to define the &amp;lsquo;train&amp;rsquo; method in NLTK here:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def train(labeled_featuresets, estimator=nltk.probability.ELEProbDist):
    # Create the P(label) distribution
    label_probdist = estimator(label_freqdist)
    # Create the P(fval|label, fname) distribution
    feature_probdist = {}
    return NaiveBayesClassifier(label_probdist, feature_probdist)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, we can use the naive bayes method to train the data. Have a look:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tweet_positive = &#39;Larry is my friend&#39;
tweet_negative = &#39;Larry is not my friend&#39;

print classifier.classify(extract_features(tweet_positive.split()))
# &amp;gt; positive
print classifier.classify(extract_features(tweet_negative.split()))
# &amp;gt; negative
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Don&amp;rsquo;t be too positive, let&amp;rsquo;s try another example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tweet_negative2 = &#39;Your song is annoying&#39;
print classifier.classify(extract_features(tweet_negative2.split()))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, we will classify the test_tweets and calculate the recall accuracy.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def classify_tweet(tweet):
    return \
        classifier.classify(extract_features(tweet)) # nltk.word_tokenize(tweet)

total = accuracy = float(len(test_tweets))

for tweet in test_tweets:
    if classify_tweet(tweet[0]) != tweet[1]:
        accuracy -= 1

print(&#39;Total accuracy: %f%% (%d/20).&#39; % (accuracy / total * 100, accuracy))
# 0.8
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Of course, this is only the starting point. In the future, I will use the same data set, to so &lt;a href=&#34;http://chengjun.github.io/en/2014/04/sentiment-analysis-with-machine-learning-in-R/&#34; target=&#34;_blank&#34;&gt;how to do it in R&lt;/a&gt; using naive Bayes and beyond.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
