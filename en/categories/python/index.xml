<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Python on </title>
    <link>https://chengjun.github.io/en/categories/python/index.xml</link>
    <description>Recent content in Python on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2016 Cheng-Jun Wang</copyright>
    <atom:link href="/en/categories/python/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>iching：一个用来算卦的python包</title>
      <link>https://chengjun.github.io/en/post/cn/2015-07-04-iching-python/</link>
      <pubDate>Sat, 04 Jul 2015 00:00:00 +0000</pubDate>
      
      <guid>https://chengjun.github.io/en/post/cn/2015-07-04-iching-python/</guid>
      <description>&lt;iframe src=&#34;http://nbviewer.ipython.org/github/chengjun/iching/blob/master/iching_intro.ipynb&#34; scrolling=&#34;no&#34; width=&#34;700&#34; height=&#34;6500&#34;&gt;&lt;/iframe&gt;
</description>
    </item>
    
    <item>
      <title>打包发布python软件包</title>
      <link>https://chengjun.github.io/en/post/cn/2015-02-22-distribute-python-package/</link>
      <pubDate>Sun, 22 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>https://chengjun.github.io/en/post/cn/2015-02-22-distribute-python-package/</guid>
      <description>&lt;p&gt;我们经常写一些程序碎片，却很少有动力把它们整合起来。前段时间写了一个爬取并可视化谷歌学术网的python程序。今天想不如把它整合一下，虽然非常简单（只有一个函数）。主要参考python官网的&lt;a href=&#34;https://packaging.python.org/en/latest/distributing.html#uploading-your-project-to-pypi&#34; target=&#34;_blank&#34;&gt;发布指南&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;##注册
于是首先来到pypi网站注册。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://pypi.python.org/pypi?%3Aaction=submit_form&#34; target=&#34;_blank&#34;&gt;https://pypi.python.org/pypi?%3Aaction=submit_form&lt;/a&gt;
记下用户名chengjun和密码W4&lt;/p&gt;

&lt;p&gt;##填写软件包信息
《指南》推荐直接在线填写 &lt;a href=&#34;https://pypi.python.org/pypi?%3Aaction=submit_form&#34; target=&#34;_blank&#34;&gt;https://pypi.python.org/pypi?%3Aaction=submit_form&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;##打包和发布工具
先要安装两个包：twine和wheel。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;pip install wheel
pip install twine
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;##整理项目文件夹
找项目实例（&lt;a href=&#34;https://github.com/pypa/sampleproject）下载下来，修改其中的部分内容即可。详见指南，或者自己摸索即可。&#34; target=&#34;_blank&#34;&gt;https://github.com/pypa/sampleproject）下载下来，修改其中的部分内容即可。详见指南，或者自己摸索即可。&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;##打包发布
1.在window环境下，使用cmd，转换工作路径到项目文件夹。
2. 主要参考 &lt;a href=&#34;https://github.com/pypa/twine打包发布：&#34; target=&#34;_blank&#34;&gt;https://github.com/pypa/twine打包发布：&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#Create some distributions in the normal way:
$ python setup.py sdist bdist_wheel

#Upload with twine:
$ twine upload dist/*
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;我使用上传的时候出错（typeError），于是直接使用打包好的zip文件（在dist子文件夹当中）手工上传到pypi。注意，每次上传到pypi需要修改一次setup.py中的版本号，并重新打包才可上传。如此而已，比我想象当中要速度快得多、简单的多。&lt;/p&gt;

&lt;p&gt;这里是我刚刚打包发布的一个可视化谷歌学术网络的python软件包：&lt;a href=&#34;https://pypi.python.org/pypi/scholarNetwork/&#34; target=&#34;_blank&#34;&gt;https://pypi.python.org/pypi/scholarNetwork/&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>表达、检索、练习——写给Python的初学者</title>
      <link>https://chengjun.github.io/en/post/cn/2015-02-22-fresh-python/</link>
      <pubDate>Sun, 22 Feb 2015 00:00:00 +0000</pubDate>
      
      <guid>https://chengjun.github.io/en/post/cn/2015-02-22-fresh-python/</guid>
      <description>&lt;p&gt;三年前，我写过一篇小日志，介绍如何从零开始学习R语言。后来我的工作越来越多的使用python，于是摸爬滚打自己探索了挺久的。身边也越来越多的人问起新手如何从零学习python的问题。我想练习、检索、表达这三点依旧是关键，只不过顺序稍有不同。很多人说，学习python，你来推荐一些资料吧。我想了想，资料还不是关键，关键是自身。&lt;/p&gt;

&lt;p&gt;##看书还是上手？
以前我们学语言，比如C语言或者Basic语言，首先讲的都是字符、数字、列表、逻辑符等。几乎所有的编程书都会这么讲，所以很多人会觉得看书没有什么新意。不过我觉得看书还是必须的，重点就是要去掌握这些基本的东西。&lt;/p&gt;

&lt;p&gt;当然了，掌握这些并不能帮助我们完成手上的工作。是的，并不能！总有一些细节的地方你必须去hack现有的代码。于是乎就有了另外一种学习语言的哲学：干中学（learn by doing）。基本的逻辑就是不断摸索，硬着头皮上。这种风格非常强悍，虽然刚开始的时候容易犯非常浅显的毛病，但是却是真正学语言的不二法门。&lt;/p&gt;

&lt;p&gt;##表达
到底看书还是上手呢？我主张先明确自己的问题是什么。做研究的人都知道，我们往往对于自己所面临的问题并不明确。写程序、学语言也是这个样子。首先要明确地表达出来。这是表达的第一重意思。&lt;/p&gt;

&lt;p&gt;##检索
当你问题明确之后，就可以去检索了。去哪里检索？书中、网上，不拘于形式。重要的是解决问题。这个过程中，我们带着问题读书、上网、提问，能够培养我们独立思考的能力。&lt;/p&gt;

&lt;p&gt;互联网的发展，使得很多时候我们并不需要真正去创造什么，只要检索一下，总能找到好的代码，修改以下就能解决自己的问题。我觉得挺好。这符合我们学习语言的初衷。有个说法是十年学会编程，但是使用编程一个月就够了。&lt;/p&gt;

&lt;p&gt;既然可以看书，有什么推荐的吗？我推荐Beginning Python这本书，虽然我是从A Byte of Python开始看的。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A Byte of Python &lt;a href=&#34;http://book.douban.com/subject/5948760/&#34; target=&#34;_blank&#34;&gt;http://book.douban.com/subject/5948760/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Beginning Python （Python 基础教程） &lt;a href=&#34;http://book.douban.com/subject/3205338/&#34; target=&#34;_blank&#34;&gt;http://book.douban.com/subject/3205338/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Hello World！Computer Programming for Kids and Other Beginners 与孩子一起学编程 &lt;a href=&#34;http://book.douban.com/subject/5338024/&#34; target=&#34;_blank&#34;&gt;http://book.douban.com/subject/5338024/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;How to Think Like a Computer Scientist: Learning with Python &lt;a href=&#34;http://book.douban.com/subject/1481058/&#34; target=&#34;_blank&#34;&gt;http://book.douban.com/subject/1481058/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;21 Recipes for Mining Twitter &lt;a href=&#34;http://book.douban.com/subject/5988563/&#34; target=&#34;_blank&#34;&gt;http://book.douban.com/subject/5988563/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Mining the Social Web &lt;a href=&#34;http://book.douban.com/subject/5391582/&#34; target=&#34;_blank&#34;&gt;http://book.douban.com/subject/5391582/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Toby Segaran (2007) Programming Collective Intelligence Building Smart Web 2.0 Applications. O&amp;rsquo;Reilly Media&lt;/li&gt;
&lt;li&gt;Python公开课 中文课程 &lt;a href=&#34;http://www.imooc.com/view/177&#34; target=&#34;_blank&#34;&gt;http://www.imooc.com/view/177&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;##练习
有了问题和思路之后，重要的就是练习了。不要怕麻烦，经常动手写东西。这个是不二法门。&lt;/p&gt;

&lt;p&gt;还有一些琐碎的东西，如下：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;我觉得学python上手很重要，选一个好的IDE很重要，对于windows用户我推荐winpython，使用集成于其中的spyder编程很方便，不需要指定python的路径，安装第三方包也很方便。&lt;/li&gt;
&lt;li&gt;每天都接触点Python,写写博客。&lt;/li&gt;
&lt;li&gt;既然使用Python了，那么google就是你最好的朋友！用英文检索。&lt;/li&gt;
&lt;li&gt;不要错过Github。上传你的代码。便于保存和分享。python的精神是开放，开源。&lt;/li&gt;
&lt;li&gt;很多时候，最大的问题是你不知道自己面临的问题：我的经验是用英文一句话说出你的问题。然后借助搜索引擎。你一般都能找到答案。Python的email list和stackoverflow中有很多想要的答案。&lt;/li&gt;
&lt;li&gt;实在找不到解决方案，不要过多寄希望于身边的朋友，stackoverflow上有更合适的回答你问题的人！去那里提问。&lt;/li&gt;
&lt;li&gt;熟悉一个package。经常阅读package的文档。&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Learning basic epidemic models with Python</title>
      <link>https://chengjun.github.io/en/post/en/2013-03-14-learn-basic-epidemic-models-with-python/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chengjun.github.io/en/post/en/2013-03-14-learn-basic-epidemic-models-with-python/</guid>
      <description>

&lt;p&gt;The epidemic model is another intellectual source for information diffusion research. The first known mathematical model of epidemiology is formulated by Daniel Bernoulli (1760) when he studied the mortality rates in order to eradicate the smallpox. However, it was not until the early twentieth century that deterministic modeling of epidemiology started. Ross (1911) developed differential equation models of epidemics in 1911. Later, Kermack and McKendrick (1927) found the epidemic threshold and they argued that the density of susceptible must exceed a critical value to make the outbreak of an epidemic happen.&lt;/p&gt;

&lt;p&gt;The mathematical models developed by epidemic research help clarify assumptions, variables, and parameters for diffusion research, lead to useful concepts (e.g., threshold, reproduction number), supply an experimental tool for testing theoretical conjectures, and forecast epidemic spreading in the future (Hethcote, 2009). Although epidemic models are simplifications of reality, they help us refine our understandings about the logic of diffusion beneath social realities (disease transmission, information diffusion through networks, and adoption of new technologies or behaviors). To understand the epidemic models in a better way, I will briefly review the basic epidemic models: SI, SIR, SIS, and the applications in networks.&lt;/p&gt;

&lt;h3 id=&#34;si-model&#34;&gt;SI model&lt;/h3&gt;

&lt;p&gt;The SI model is the simplest possible model of infection. In the SI model, there are only two phases in the SI epidemic spreading process: Susceptible and Infectious.  Let S be the proportion of the population that are susceptible. Let I be the proportion of the population that are infectious. At the initial time, the proportion of people who are infected is x0, the proportion of people who are susceptible is S0. β is the transmission rate, and it incorporates the encounter rate between susceptible and infectious individuals together with the probability of transmission. Consider a “closed population” with no births, deaths, or migrations, and assume the mixing is homogeneous (e.g., the susceptible individuals are uniformly spread in a geographic area, and the probability of contracting the infection is uniformly the same for all actors (T. G. Lewis, 2011)), yielding βSI as the transmission term. Thus, the equation for SI model is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        dS/dt= -βSI
        dI/( dt)= βSI
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Given every individual in the system must be either susceptible or infected, I + S = 1. Thus, the equations above can be transformed to:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        dI/dt=βI(1-I)   
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To solve this differential equation, we can get the cumulative growth curve as a function of time:&lt;/p&gt;

&lt;p&gt;$$I[t]= \frac{x&lt;em&gt;{0} e^{\beta t }}{1-x&lt;/em&gt;{0}+ x_{0} e^{\beta t }}$$&lt;/p&gt;

&lt;p&gt;Interestingly, this is a logistic growth featured by its S-shaped curve. $$$x_{0}$$ is the initial value of I[t]. The curve grows exponentially shortly after the system is infected, and then saturates as the number of susceptible shrinks which makes it harder to find the next victims. Thus, it could be used to model the classic diffusion of innovations.
In the naive model of SI, once one is infected, it is always infectious. However, this is not realistic for many situations of disease spreading. For many diseases, people recover after a certain time because their immune systems act to fight with the diseases. There is usually a status of recovery denoted by R. Let γ denote the removal or recovery rate. Usually, researchers are more interested in its reciprocal (1/γ) which determines the average infectious period.&lt;/p&gt;

&lt;h3 id=&#34;sir-model&#34;&gt;SIR model&lt;/h3&gt;

&lt;p&gt;There are two stages of the dynamics of the SIR model. In the first stage, susceptible individuals become infected by the infectious ones with who they contact. Similar to the SI model, β is the transmission rate between individuals; In the second stage, infected individuals recover at the average rate γ. Given the premise that underlying epidemiological rates are constant, the differential equations of simple SIR model (with no births, deaths, or migrations) are:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;       dS/dt= -βSI
       dI/( dt)= βSI- γI
       dR/dt= βI
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However, the differential equations above could not be analytically solved. In practice, researchers can evaluate SIR model numerically, as it is showed in the figure below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://weblab.com.cityu.edu.hk/blog/chengjun/files/2013/03/p2.1-SIR.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# -*- coding: utf-8 -*-

###################################
### Written by Ilias Soumpasis    #
### ilias.soumpasis@ucd.ie (work) #
### ilias.soumpasis@gmail.com     #
###################################

import scipy.integrate as spi
import numpy as np
import pylab as pl

beta=1.4247
gamma=0.14286
TS=1.0
ND=70.0
S0=1-1e-6
I0=1e-6
INPUT = (S0, I0, 0.0)

def diff_eqs(INP,t):
    &#39;&#39;&#39;The main set of equations&#39;&#39;&#39;
    Y=np.zeros((3))
    V = INP
    Y[0] = - beta * V[0] * V[1]
    Y[1] = beta * V[0] * V[1] - gamma * V[1]
    Y[2] = gamma * V[1]
    return Y   # For odeint

t_start = 0.0; t_end = ND; t_inc = TS
t_range = np.arange(t_start, t_end+t_inc, t_inc)
RES = spi.odeint(diff_eqs,INPUT,t_range)

print RES

#Ploting
pl.plot(RES[:,0], &#39;-bs&#39;, label=&#39;Susceptibles&#39;)  # I change -g to g--  # RES[:,0], &#39;-g&#39;,
pl.plot(RES[:,2], &#39;-g^&#39;, label=&#39;Recovereds&#39;)  # RES[:,2], &#39;-k&#39;,
pl.plot(RES[:,1], &#39;-ro&#39;, label=&#39;Infectious&#39;)
pl.legend(loc=0)
pl.title(&#39;SIR epidemic without births or deaths&#39;)
pl.xlabel(&#39;Time&#39;)
pl.ylabel(&#39;Susceptibles, Recovereds, and Infectious&#39;)
pl.savefig(&#39;2.1-SIR-high.png&#39;, dpi=900) # This does, too
pl.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;sis-model&#34;&gt;SIS model&lt;/h3&gt;

&lt;p&gt;Another extension of the SI model is the one that allows for reinfection.  If infected individuals are not immune to the diseases after their recovery, they can be infected more than once. The most simple model that captures this features is the SIS model. There are only two states: susceptible and infected, and infected individuals become susceptible after recovery. The differential equations for the simple SIS epidemic model are:
           dS/dt= γI-βSI
          dI/( dt)= βSI- γI&lt;/p&gt;

&lt;p&gt;Given S + I = 1, the differential equations have the solution:&lt;/p&gt;

&lt;p&gt;$$I[t]=(1-\frac{\gamma}{\beta}) \frac{C e^{(\beta - \gamma)t}}{1 + C e^{(\beta - \gamma)t}}$$&lt;/p&gt;

&lt;p&gt;C is the integration constant in the form of $$C=\frac{ \beta x&lt;em&gt;{0}}{\beta-\gamma-\beta x&lt;/em&gt;{0}} $$.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://weblab.com.cityu.edu.hk/blog/chengjun/files/2013/03/p2.1-SIS.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# -*- coding: utf-8 -*-

import scipy.integrate as spi
import numpy as np
import pylab as pl

beta=1.4247
gamma=0.14286
I0=1e-6
ND=70
TS=1.0
INPUT = (1.0-I0, I0)

def diff_eqs(INP,t):
    &#39;&#39;&#39;The main set of equations&#39;&#39;&#39;
    Y=np.zeros((2))
    V = INP
    Y[0] = - beta * V[0] * V[1] + gamma * V[1]
    Y[1] = beta * V[0] * V[1] - gamma * V[1]
    return Y   # For odeint

t_start = 0.0; t_end = ND; t_inc = TS
t_range = np.arange(t_start, t_end+t_inc, t_inc)
RES = spi.odeint(diff_eqs,INPUT,t_range)

print RES

#Ploting
pl.plot(RES[:,0], &#39;-bs&#39;, label=&#39;Susceptibles&#39;)
pl.plot(RES[:,1], &#39;-ro&#39;, label=&#39;Infectious&#39;)
pl.legend(loc=0)
pl.title(&#39;SIS epidemic without births or deaths&#39;)
pl.xlabel(&#39;Time&#39;)
pl.ylabel(&#39;Susceptibles and Infectious&#39;)
pl.savefig(&#39;2.5-SIS-high.png&#39;, dpi=900) # This does increase the resolution.
pl.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;One important contribution of epidemic models is the threshold phenomenon of epidemic diffusion existing in SIR model. The threshold of SIR model asks what factors determine whether an epidemic occur or fail. As the first step of analyzing the threshold of SIR model, the differential equation of SIR (dI/dt= βSI- βI) can be rewritten in the form:&lt;/p&gt;

&lt;p&gt;dI/( dt)=(βS - β)I&lt;/p&gt;

&lt;p&gt;If dI/dt is smaller than 0, the contagion will soon wither and die out. Thus, as a boundary condition, $$(\beta S- \gamma)$$ should be larger than 0, and S should be larger than γ/β. This is the threshold phenomenon (Kermack &amp;amp; McKendarick, 1927). Based on the rationales above, if the initial fraction of susceptible (S(0)) is less than γ/β, the infection would not be able to start the invasion in the population. Here  γ/β is defined as basic reproductive ratio $$R_{0}$$.&lt;/p&gt;

&lt;p&gt;To summarize, “for an infectious disease with an average infectious period give by 1/γ and a transmission rate β, its basic reproductive ratio R0 is determined by γ/β. In a closed population, an infection with a specified R0 can invade only if there is a threshold fraction of susceptible greater than 1/γ ” (Keeling &amp;amp; Rohani, 2011, p. 21).&lt;/p&gt;

&lt;p&gt;###The Other Extentions?
In the section above, I mainly focus on the deterministic models of epidemics. However, despite the many advantages of deterministic models, it can be difficult to include realistic population networks, to incorporate realistic probability distributions for the time spent in the infectious period, and to assess the probability of an outbreak. Thus, the stochastic epidemic simulations, such as stochastic differential equations, Markov Chain Monte Carlo (MCMC), and agent based modeling, have been used to remedy the defect.&lt;/p&gt;

&lt;p&gt;Network epidemic models have also been developed to investigate the widespread and rapid propagations (e.g., the contagion of computer virus) through a network. Typically, the network epidemic is brought about by adjacent nodes through propagations along one or more links. Network epidemic models consider the topology of the network as well as infection rate, death rate, and state transitions. This line of research is interested in the following questions: under what conditions will an initial outbreak spread to a nontrivial portion of the population? What percentage of the population will eventually become infected? What is the effect of immunization policies? For example, Pastor-Satorras et al. (2001a, 2001b) study the spreading of epidemics in complex networks using the mean-field method for network SIS model. Their findings indicate that in exponential networks (e.g., random graph network, small-world network), there is the usual epidemic threshold below which there is no prevalence of epidemic. Yet, on a wide range of scale-free networks, there is an absence of an epidemic threshold, which implies that scale-free networks are prone to the spreading of epidemics, as well as other spreading phenomena, e.g., information diffusion. Based on this rationale of the absence of epidemic threshold, network scientists expect online information diffuse to a great proportion of the population.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Parsing Githubarchive Data using Python</title>
      <link>https://chengjun.github.io/en/post/cn/2016-06-18-parsing-githubarchive-json/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chengjun.github.io/en/post/cn/2016-06-18-parsing-githubarchive-json/</guid>
      <description>&lt;p&gt;Githubarchive整理了github的历史数据，每一个小时一个数据文件，我下载了2011-2014年四年的数据，结果发现2012年的多数数据存在错误换行的问题，需要清洗。一般而言，正确的情况是一个字典格式的数据占一行，错误的情况就会是所有的字典格式的数据合并为了一行。似乎是缺乏换行符造成的。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://stackoverflow.com/questions/10432432/yajl-parse-error-with-githubarchive-org-json-stream-in-python#&#34; target=&#34;_blank&#34;&gt;http://stackoverflow.com/questions/10432432/yajl-parse-error-with-githubarchive-org-json-stream-in-python#&lt;/a&gt; 这里详细记录了这个问题。&lt;/p&gt;

&lt;p&gt;按照这个帖子，我尝试了很多方法，yajl和ijson，但是都不能很优雅的解决我的问题。不妨采用暴力的方法。&lt;/p&gt;

&lt;p&gt;rirwin利用了{和}出现的偶数关系来分割字符串，但是这种方法容易遗漏数据且速度较慢。&lt;/p&gt;

&lt;p&gt;仔细思考这个问题就是：Parse multiple json objects that are in one line。 搜索之，发现了&lt;a href=&#34;http://stackoverflow.com/questions/36967236/parse-multiple-json-objects-that-are-in-one-line&#34; target=&#34;_blank&#34;&gt;http://stackoverflow.com/questions/36967236/parse-multiple-json-objects-that-are-in-one-line&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;其中，Francesco的解决方法比较高效：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;f = &#39;/Users/chengjun/百度云同步盘/githubarchive/2012-03-10-22.json.gz&#39;
f = gzip.open(f, &#39;rb&#39;)
f = f.readline()
r = re.split(&#39;(\{.*?\})(?= *\{)&#39;, f)
accumulator = &#39;&#39;
res = []
for subs in r:
    accumulator += subs
    try:
        res.append(json.loads(accumulator))
        accumulator = &#39;&#39;
    except:
        pass
len(res)
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Out [29]: 1270&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;基于这种方法，可以写一个函数来实现对于数据的正确读取。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#f = &#39;/Users/chengjun/百度云同步盘/test.json&#39;
#f = open(f)

f = &#39;/Users/chengjun/百度云同步盘/githubarchive/2012-06-01-15.json.gz&#39;
f = gzip.open(f, &#39;rb&#39;)
files = f.readlines()
length = len(files)
if  length &amp;gt; 1:
    print &#39;Correct: &#39; + str(length)
else:
    f2 = files[0]
    r = re.split(&#39;(\{.*?\})(?= *\{)&#39;, f2)
    r = [i for i in r if i] # delete the &#39;&#39;
    accumulator = &#39;&#39;
    acts = []
    for subs in r:
        accumulator += subs
        try:
            acts.append(json.loads(accumulator))
            accumulator = &#39;&#39;
        except Exception, e:
            print e
            pass
    print &#39;Wrong: &#39; + str(len(acts))
&lt;/code&gt;&lt;/pre&gt;

&lt;blockquote&gt;
&lt;p&gt;Out [30]: Wrong: 4491&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>Scraping New York Times &amp; The Guardian using Python</title>
      <link>https://chengjun.github.io/en/post/en/2012-04-23-scraping-newyork-times-with-python/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chengjun.github.io/en/post/en/2012-04-23-scraping-newyork-times-with-python/</guid>
      <description>&lt;p&gt;I have read the blog post about Scraping New York Times Articles with R. It’s great. I want to reproduce the work with python.
First, we should learn about nytimes article search api.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://developer.nytimes.com/docs/article_search_api/&#34; target=&#34;_blank&#34;&gt;http://developer.nytimes.com/docs/article_search_api/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Second, we need to register and get the key which will be used in python script.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://developer.nytimes.com/apps/register&#34; target=&#34;_blank&#34;&gt;http://developer.nytimes.com/apps/register&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# !/usr/bin/env python
# -*- coding: UTF-8  -*-
# Scraping New York Times using python
# 20120421@ Canberra
# chengjun wang

import json
import urllib2

&#39;&#39;&#39;
About the api and the key, see the links above.
&#39;&#39;&#39;

&#39;&#39;&#39;step 1: input query information&#39;&#39;&#39;
apiUrl=&#39;http://api.nytimes.com/svc/search/v1/article?format=json&#39;
query=&#39;query=occupy+wall+street&#39;                            # set the query word here
apiDate=&#39;begin_date=20110901&amp;amp;end_date=20120214&#39;             # set the date here
fields=&#39;fields=body%2Curl%2Ctitle%2Cdate%2Cdes_facet%2Cdesk_facet%2Cbyline&#39;
offset=&#39;offset=0&#39;
key=&#39;api-key=c2c5b91680.......2811165&#39;  # input your key here

&#39;&#39;&#39;step 2: get the number of offset/pages&#39;&#39;&#39;
link=[apiUrl, query, apiDate, fields, offset, key]
ReqUrl=&#39;&amp;amp;&#39;.join(link)
jstr = urllib2.urlopen(ReqUrl).read()  # t = jstr.strip(&#39;()&#39;)
ts = json.loads( jstr )
number=ts[&#39;total&#39;] #  the number of queries  # query=ts[&#39;tokens&#39;] # result=ts[&#39;results&#39;]
print number
seq=range(number/9)  # this is not a good way
print seq

&#39;&#39;&#39;step 3: crawl the data and dump into csv&#39;&#39;&#39;
import csv
addressForSavingData= &amp;quot;D:/Research/Dropbox/tweets/wapor_assessing online opinion/News coverage of ows/nyt.csv&amp;quot;
file = open(addressForSavingData,&#39;wb&#39;) # save to csv file
for i in seq:
    nums=str(i)
    offsets=&#39;&#39;.join([&#39;offset=&#39;, nums]) # I made error here, and print is a good way to test
    links=[apiUrl, query, apiDate, fields, offsets, key]
    ReqUrls=&#39;&amp;amp;&#39;.join(links)
    print &amp;quot;*_____________*&amp;quot;, ReqUrls
    jstrs = urllib2.urlopen(ReqUrls).read()
    t = jstrs.strip(&#39;()&#39;)
    tss= json.loads( t )  # error no joson object
    result = tss[&#39;results&#39;]
    for ob in result:
        title=ob[&#39;title&#39;]  # body=ob[&#39;body&#39;]   # body,url,title,date,des_facet,desk_facet,byline
        print title
        url=ob[&#39;url&#39;]
        date=ob[&#39;date&#39;] # desk_facet=ob[&#39;desk_facet&#39;]  # byline=ob[&#39;byline&#39;] # some author names don&#39;t exist
        w = csv.writer(file,delimiter=&#39;,&#39;,quotechar=&#39;|&#39;, quoting=csv.QUOTE_MINIMAL)
        w.writerow((date, title, url)) # write it out
file.close()
pass
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;see the result:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://weblab.com.cityu.edu.hk/blog/chengjun/files/2012/04/nyt1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Similarly, you can crawl the article data from The Guardian. See the link below.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://explorer.content.guardianapis.com/#/?format=json&amp;amp;order-by=newest&#34; target=&#34;_blank&#34;&gt;http://explorer.content.guardianapis.com/#/?format=json&amp;amp;order-by=newest&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;After you have registered you app and got the key, we can work on the python script.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# !/usr/bin/env python
# -*- coding: UTF-8 -*-
# Scraping The Guardian using Python
# 20120421@ Canberra
# chengjun wang

import json
import urllib2

&#39;&#39;&#39;

http://content.guardianapis.com/search?q=occupy+wall+street&amp;amp;from-date=2011-09-01&amp;amp;to-date=2012-02-14&amp;amp;page=2

&amp;amp;page-size=3&amp;amp;format=json&amp;amp;show-fields=all&amp;amp;use-date=newspaper-edition&amp;amp;api-key=m....g33gzq
&#39;&#39;&#39;

&#39;&#39;&#39;step 1: input query information&#39;&#39;&#39;
apiUrl=&#39;http://content.guardianapis.com/search?q=occupy+wall+street&#39; # set the query word here
apiDate=&#39;from-date=2011-09-01&amp;amp;to-date=2011-10-14&#39;           # set the date here
apiPage=&#39;page=2&#39;   # set the page
apiNum=10       # set the number of articles in one page
apiPageSize=&#39;&#39;.join([&#39;page-size=&#39;,str(apiNum)])
fields=&#39;format=json&amp;amp;show-fields=all&amp;amp;use-date=newspaper-edition&#39;
key=&#39;api-key=mudfuj...g33gzq&#39; # input your key here

&#39;&#39;&#39;step 2: get the number of offset/pages&#39;&#39;&#39;
link=[apiUrl, apiDate, apiPage, apiPageSize, fields, key]
ReqUrl=&#39;&amp;amp;&#39;.join(link)
jstr = urllib2.urlopen(ReqUrl).read() # t = jstr.strip(&#39;()&#39;)
ts = json.loads( jstr )
number=ts[&#39;response&#39;][&#39;total&#39;] # the number of queries # query=ts[&#39;tokens&#39;] # result=ts[&#39;results&#39;]
print number
seq=range(number/(apiNum-1)) # this is not a good way
print seq

&#39;&#39;&#39;step 3: crawl the data and dump into csv&#39;&#39;&#39;
import csv
addressForSavingData= &amp;quot;D:/Research/Dropbox/tweets/wapor_assessing online opinion/News coverage of ows/guardian.csv&amp;quot;
file = open(addressForSavingData,&#39;wb&#39;) # save to csv file
for i in seq:
    nums=str(i+1)
    apiPages=&#39;&#39;.join([&#39;page=&#39;, nums]) # I made error here, and print is a good way to test
    links= [apiUrl, apiDate, apiPages, apiPageSize, fields, key]
    ReqUrls=&#39;&amp;amp;&#39;.join(links)
    print &amp;quot;*_____________*&amp;quot;, ReqUrls
    jstrs = urllib2.urlopen(ReqUrls).read()
    t = jstrs.strip(&#39;()&#39;)
    tss= json.loads( t )
    result = tss[&#39;response&#39;][&#39;results&#39;]
    for ob in result:
        title=ob[&#39;webTitle&#39;].encode(&#39;utf-8&#39;) # body=ob[&#39;body&#39;]  # body,url,title,date,des_facet,desk_facet,byline
        print title
        section=ob[&amp;quot;sectionName&amp;quot;].encode(&#39;utf-8&#39;)
        url=ob[&#39;webUrl&#39;]
        date=ob[&#39;fields&#39;][&#39;newspaperEditionDate&#39;] # date=ob[&#39;webPublicationDate&#39;] # byline=ob[&#39;fields&#39;][&#39;byline&#39;]
        w = csv.writer(file,delimiter=&#39;,&#39;,quotechar=&#39;|&#39;, quoting=csv.QUOTE_MINIMAL)
        w.writerow((date, title, section, url)) # write it out
file.close()
pass
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Scraping data from Sina Weibo using Python</title>
      <link>https://chengjun.github.io/en/post/en/2014-03-16-scraping-weibo-using-python/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chengjun.github.io/en/post/en/2014-03-16-scraping-weibo-using-python/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://weblab.com.cityu.edu.hk/blog/chengjun/files/2012/09/33.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;weibo-oauth2-0&#34;&gt;Weibo Oauth2.0&lt;/h3&gt;

&lt;p&gt;I would like to introduce you how to use python to scrape tweets from Sina Weibo in this post.&lt;/p&gt;

&lt;h4 id=&#34;automatically-get-authorization-by-oauth2-0&#34;&gt;Automatically get authorization by oauth2.0&lt;/h4&gt;

&lt;p&gt;First, following this webpage to set your app, especially to get the app key, app secret, and set the callback url.  See an &lt;a href=&#34;http://blog.laisky.us/2012/01/278/&#34; target=&#34;_blank&#34;&gt;introduction&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Second, following this &lt;a href=&#34;http://www.how2dns.com/blog/?p=538&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt;, you can automatically get the code from the callback url.&lt;/p&gt;

&lt;p&gt;The following python script demonstrates how to automatically get authorization.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/usr/bin/env python
# -*- coding: utf8 -*-

from weibo import APIClient
import urllib2
import urllib
import sys
import time
from time import clock
import csv
import random

reload(sys)
sys.setdefaultencoding(&#39;utf-8&#39;)

&#39;&#39;&#39;Step 0 Login with OAuth2.0&#39;&#39;&#39;
if __name__ == &amp;quot;__main__&amp;quot;:
    APP_KEY = &#39;663...&#39; # app key
    APP_SECRET = &#39;2fc....&#39; # app secret
    CALLBACK_URL = &#39;https://api.weibo.com/oauth2/default.html&#39; # set callback url exactly like this!
    AUTH_URL = &#39;https://api.weibo.com/oauth2/authorize&#39;
    USERID = &#39;w...4&#39; # your weibo user id
    PASSWD = &#39;w....&#39; #your pw

    client = APIClient(app_key=APP_KEY, app_secret=APP_SECRET, redirect_uri=CALLBACK_URL)
    referer_url = client.get_authorize_url()
    print &amp;quot;referer url is : %s&amp;quot; % referer_url

    cookies = urllib2.HTTPCookieProcessor()
    opener = urllib2.build_opener(cookies)
    urllib2.install_opener(opener)

    postdata = {&amp;quot;client_id&amp;quot;: APP_KEY,
                &amp;quot;redirect_uri&amp;quot;: CALLBACK_URL,
                &amp;quot;userId&amp;quot;: USERID,
                &amp;quot;passwd&amp;quot;: PASSWD,
                &amp;quot;isLoginSina&amp;quot;: &amp;quot;0&amp;quot;,
                &amp;quot;action&amp;quot;: &amp;quot;submit&amp;quot;,
                &amp;quot;response_type&amp;quot;: &amp;quot;code&amp;quot;,
                }
    headers = {&amp;quot;User-Agent&amp;quot;: &amp;quot;Mozilla/5.0 (Windows NT 6.1; rv:11.0) Gecko/20100101 Firefox/11.0&amp;quot;,
                &amp;quot;Host&amp;quot;: &amp;quot;api.weibo.com&amp;quot;,
                &amp;quot;Referer&amp;quot;: referer_url
            }

    req  = urllib2.Request(
       url = AUTH_URL,
       data = urllib.urlencode(postdata),
       headers = headers
       )
    try:
        resp = urllib2.urlopen(req)
        print &amp;quot;callback url is : %s&amp;quot; % resp.geturl()
        code = resp.geturl()[-32:]
        print &amp;quot;code is : %s&amp;quot; %  code
    except Exception, e:
        print e

r = client.request_access_token(code)
access_token1 = r.access_token # The token return by sina
expires_in = r.expires_in

print &amp;quot;access_token=&amp;quot; ,access_token1
print &amp;quot;expires_in=&amp;quot; ,expires_in   # access_token lifetime by second. http://open.weibo.com/wiki/OAuth2/access_token

&amp;quot;&amp;quot;&amp;quot;save the access token&amp;quot;&amp;quot;&amp;quot;
client.set_access_token(access_token1, expires_in)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;get-the-number-of-retweets&#34;&gt;Get the number of retweets&lt;/h3&gt;

&lt;p&gt;Step 1. Assume that you have had a list of tweet ids, you want to get the number of repsots. Thus, you can have the distribution of the size of diffusion.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&#39;&#39;&#39; Step 1 Get the number of reposts&#39;&#39;&#39;
&amp;quot;&amp;quot;&amp;quot;get the user ids&amp;quot;&amp;quot;&amp;quot;
dataReader = csv.reader(open(&#39;C:/Python27/weibo/sampledRtIds2.csv&#39;, &#39;r&#39;), delimiter=&#39;,&#39;, quotechar=&#39;|&#39;)
ids = []
for row in dataReader:
    ids.append(int(row[0]))  # modify the number to get the diffusers&#39; ids

file = open(&amp;quot;C:/Python27/weibo/repostsRT300000m2.csv&amp;quot;,&#39;wb&#39;) # save to csv file

start = clock()
print start

for seqNum in range(1500, 2999):
    id = ids[(0 + 100*seqNum) : (100+100*seqNum)]
    id = str(id).strip(&#39;[]&#39;).replace(&#39;L&#39;, &#39;&#39;)
    rate = client.get.account__rate_limit_status()
    sleep_time = rate.reset_time_in_seconds + 300
    remaining_ip_hits = rate.remaining_ip_hits
    remaining_user_hits = rate.remaining_user_hits
    if remaining_ip_hits &amp;gt;= 10 and remaining_user_hits &amp;gt;= 5:
        rtc = client.get.statuses__count(ids = id) # mid, 100
        for n in range(0, len(rtc)): # 0-99
            mid = rtc[n][&#39;id&#39;]
            reposts = rtc[n][&#39;reposts&#39;]
            comments = rtc[n][&#39;comments&#39;]
            attitudes = rtc[n][&#39;attitudes&#39;]
            timePass = clock()-start
            if round(timePass) % 10 == 0:
                print mid, reposts, len(rtc), &amp;quot;I have been working for %s seconds&amp;quot; % round(timePass)
            print &amp;gt;&amp;gt;file, &amp;quot;%s,%s,%s,%s&amp;quot; % (mid, reposts, comments, attitudes)
    elif remaining_ip_hits &amp;lt; 10 or remaining_user_hits &amp;lt; 5:
        print &amp;quot;Python will sleep %s seconds&amp;quot; % sleep_time
        time.sleep(sleep_time+60)

file.close()
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;get-the-list-of-diffusers&#34;&gt;Get the list of diffusers&lt;/h3&gt;

&lt;p&gt;Step 2. If you want to step further and get the list of diffusers for a list of weibos. Thus, you will know how many reposts or retweets have been deleted by the website.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# &#39;&#39;&#39;Step 2 Get the diffusers&#39;&#39;&#39;
&amp;quot;&amp;quot;&amp;quot;read ids&amp;quot;&amp;quot;&amp;quot;
dataReader = csv.reader(open(&#39;C:/Python27/weibo/repostsSample3.csv&#39;, &#39;r&#39;), delimiter=&#39;,&#39;, quotechar=&#39;|&#39;)
ids = []
for row in dataReader:
    ids.append(int(row[1]))  # get the number to get the mid

addressForSavingData= &amp;quot;C:/Python27/weibo/diffsersSave.csv&amp;quot;
file = open(addressForSavingData,&#39;wb&#39;) # save to csv file

start = clock()
print start

lenid = len(ids) # lenid = 8 # test with the first two cases

for n in range(0, lenid+1):  # the 78 should be 77 here
    rate = client.get.account__rate_limit_status()
    sleep_time = rate.reset_time_in_seconds + 300
    remaining_ip_hits = rate.remaining_ip_hits
    remaining_user_hits = rate.remaining_user_hits
    if remaining_ip_hits &amp;gt;= 10 and remaining_user_hits &amp;gt;= 3:
        if reposts[n]%200 == 0:
            pages = reposts[n]/200
        else:
            pages = reposts[n]/200 + 1
        try:
            for pageNum in range(1, pages + 1):
                r = client.get.statuses__repost_timeline(id = ids[n], page = pageNum, count = 200)
                if len(r) == 0:
                    pass
                else:
                    m = int(len(r[&#39;reposts&#39;]))
                    for i in range(0, m):
                        &amp;quot;&amp;quot;&amp;quot;1.1 reposts&amp;quot;&amp;quot;&amp;quot;
                        mid = r[&#39;reposts&#39;][i].id
                        text = r[&#39;reposts&#39;][i].text.replace(&amp;quot;,&amp;quot;, &amp;quot;&amp;quot;)
                        created = r[&#39;reposts&#39;][i].created_at
                        &amp;quot;&amp;quot;&amp;quot;1.2 reposts.user&amp;quot;&amp;quot;&amp;quot;
                        user = r[&#39;reposts&#39;][i].user
                        user_id = user.id
                        user_name = user.name
                        user_province = user.province
                        user_city = user.city
                        user_gender = user.gender
                        user_url = user.url
                        user_followers = user.followers_count
                        user_bifollowers = user.bi_followers_count
                        user_friends = user.friends_count
                        user_statuses = user.statuses_count
                        user_created = user.created_at
                        user_verified = user.verified
                        &amp;quot;&amp;quot;&amp;quot;2.1 retweeted_status&amp;quot;&amp;quot;&amp;quot;
                        rts = r[&#39;reposts&#39;][i].retweeted_status
                        rts_mid = rts.id
                        rts_text = rts.text.replace(&amp;quot;,&amp;quot;, &amp;quot;&amp;quot;)
                        rts_created = rts.created_at
                        &amp;quot;&amp;quot;&amp;quot;2.2 retweeted_status.user&amp;quot;&amp;quot;&amp;quot;
                        rtsuser_id = rts.user.id
                        rtsuser_name = rts.user.name
                        rtsuser_province = rts.user.province
                        rtsuser_city = rts.user.city
                        rtsuser_gender = rts.user.gender
                        rtsuser_url = rts.user.url
                        rtsuser_followers = rts.user.followers_count
                        rtsuser_bifollowers = rts.user.bi_followers_count
                        rtsuser_friends = rts.user.friends_count
                        rtsuser_statuses = rts.user.statuses_count
                        rtsuser_created = rts.user.created_at
                        rtsuser_verified = rts.user.verified
                        timePass = clock()-start
                        if round(timePass) % 10 == 0:
                            print mid, rts_mid, &amp;quot;I have been working for %s seconds&amp;quot; % round(timePass)
                            time.sleep( random.randrange(3, 9, 1) )  # To avoid http error 504 gateway time-out
                        print &amp;gt;&amp;gt;file, &amp;quot;%s,&#39;%s&#39;,&#39;%s&#39;,%s,&#39;%s&#39;,%s,%s,%s,&#39;%s&#39;,%s,%s,%s,&#39;%s&#39;,%s,%s,&#39;%s&#39;,%s,&#39;%s&#39;,%s,%s,%s,&#39;%s&#39;,%s,%s,%s,%s,%s&amp;quot;  % (mid, created, text, # 3 # &amp;quot;%s,%s,|%s|,%s,|%s|,%s,%s,%s,|%s|,%s,%s,%s,%s,%s,%s,%s,%s,|%s|,%s,%s,%s,|%s|,%s,%s,%s,%s,%s&amp;quot; % (mid, created, text, # 3
                                            user_id, user_name, user_province, user_city, user_gender,  # 5 --&amp;gt; 5
                                            user_url, user_followers, user_friends, user_statuses, user_created, user_verified,  # rts_text, # 6 --&amp;gt; 9
                                            rts_mid, rts_created, # 2
                                            rtsuser_id, rtsuser_name, rtsuser_province, rtsuser_city, rtsuser_gender, # 5 --&amp;gt; 18
                                            rtsuser_url, rtsuser_followers, rtsuser_friends, rtsuser_statuses, rtsuser_created, rtsuser_verified)  # 6  --&amp;gt; 22
        except Exception, e:
            print &amp;gt;&amp;gt; sys.stderr, &#39;Encountered Exception:&#39;, e, ids[n]
            time.sleep(120)
            pass
    elif remaining_ip_hits &amp;lt; 10 or remaining_user_hits &amp;lt; 3:
        print &amp;quot;Python will sleep %s seconds&amp;quot; % sleep_time
        time.sleep(sleep_time+60)

file.close()
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;get-the-following-relationships&#34;&gt;Get the following relationships&lt;/h3&gt;

&lt;p&gt;Step 3. Now, you may want to get the social graph for all the diffusers.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&#39;&#39;&#39;Step 3 Get the social graph&#39;&#39;&#39;
&amp;quot;&amp;quot;&amp;quot;read ids&amp;quot;&amp;quot;&amp;quot;
dataReader = csv.reader(open(&#39;C:/Python27/weibo/SocialGraphIdsForStepThree.csv&#39;, &#39;r&#39;), delimiter=&#39;,&#39;, quotechar=&#39;|&#39;)
ids = []
for row in dataReader:
    ids.append(int(row[0]))  # get the number to get the mid

ids = ids[188648:697060]

addressForSavingData= &amp;quot;C:/Python27/weibo/socialgraphSave142_2.csv&amp;quot;
file = open(addressForSavingData,&#39;wb&#39;) # save to csv file

addressForSavingError = &amp;quot;C:/Python27/weibo/socialgraphSaveError142_2.csv&amp;quot;
errorlog = open(addressForSavingError,&#39;w&#39;)
errorlog.close()

start = clock()
print start

for id in ids:
    try:
        rate = client.get.account__rate_limit_status()
        sleep_time = rate.reset_time_in_seconds + 300
        remaining_ip_hits = rate.remaining_ip_hits
        remaining_user_hits = rate.remaining_user_hits
        if remaining_ip_hits &amp;gt;= 10 and remaining_user_hits &amp;gt;= 3:
            cursor = -1
            fids=[]
            while cursor != 0:
                response = client.get.friendships__friends__ids(uid=id, count= 5000, cursor=cursor)  # the biggest count is 5000
                fids    += response.ids
                cursor = response.next_cursor # previousCursor = response.previous_cursor
                timePass = clock()-start
                if round(timePass) % 10 == 0:
                    print id, &amp;quot;I have been working for %s seconds&amp;quot; % round(timePass)
                    # time.sleep( 0.01 * random.randrange(0, 5, 1) )  # To avoid http error 504 gateway time-out
                if cursor == 0:
                    totalNum = response.total_number
                    for fid in fids:
                        print &amp;gt;&amp;gt;file, &amp;quot;%s,%s,%s&amp;quot;  % (id, fid, totalNum)
                    break
        elif remaining_ip_hits &amp;lt; 10 or remaining_user_hits &amp;lt; 3:
            print &amp;quot;Python will sleep %s seconds&amp;quot; % sleep_time
            time.sleep(sleep_time+60)
    except Exception, e:
        print &amp;gt;&amp;gt;sys.stderr, &#39;Encountered Exception:&#39;, e, id
        errorlog = open(addressForSavingError, &#39;a&#39;)
        print &amp;gt;&amp;gt;errorlog, &amp;quot;%s,%s&amp;quot;  % (id, e)
        errorlog.close()
        print &#39;When the error happens, the id is:&#39;, id
        time.sleep(60)
        pass

file.close()
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;get-the-diffusion-network&#34;&gt;Get the diffusion network&lt;/h3&gt;

&lt;p&gt;Step 4. Given the collected data of retweets, we can get the diffusion path by parsing the text of weibo.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import re
import sys
from time import clock

reload(sys)
sys.setdefaultencoding(&#39;utf-8&#39;)

&#39;&#39;&#39;
Convert &amp;quot;Thu Aug 04 11:39:32 +0800 2011&amp;quot; to the ISO format: YYYY-MM-DD H:M:S
Refer to: http://stackoverflow.com/questions/15727510/using-python-regex-to-identify-retweeters-from-tweets-with-chinese-characters
&#39;&#39;&#39;

file = open(&amp;quot;D:/chengjun/New/repostsReSampleClean.csv&amp;quot;, &#39;r&#39;)
lines = file.readlines()

addressForSavingData= &amp;quot;D:/chengjun/New/diffusion_path6.csv&amp;quot;  
file = open(addressForSavingData,&#39;wb&#39;) # save to csv file

addressForSavingError = &amp;quot;D:/chengjun/New/Error.csv&amp;quot;  
errorlog = open(addressForSavingError,&#39;w&#39;)
errorlog.close

start = clock()  
print start

for line in lines:
    list = line.split(&#39;,&#39;)
    rtsmid = list[15].strip()  #rmid
    userName = list[5].strip().replace(&amp;quot;&#39;&amp;quot;, &amp;quot;&amp;quot;) # username
    submitterName = list[18].strip().replace(&amp;quot;&#39;&amp;quot;, &amp;quot;&amp;quot;)
    tweet = list[3].replace(&#39;,&#39;,&#39;&#39;)
    RTpattern = r&#39;&#39;&#39;//?@(\w+)&#39;&#39;&#39;
    rt = re.findall(RTpattern, tweet.decode(&amp;quot;utf-8&amp;quot;), re.UNICODE)
    if rt == None or len(rt)==0:
        target = userName
        source = submitterName
        print &amp;gt;&amp;gt;file, &amp;quot;%s,%s,%s&amp;quot;  % (rtsmid, source, target)
    elif rt != None and len(rt) != 0:
        rt.insert(0, userName) #
        for i in xrange(len(rt) - 1):
            target = rt[i].encode(&#39;utf-8&#39;)
            source = rt[i + 1].encode(&#39;utf-8&#39;)
            print &amp;gt;&amp;gt;file, &amp;quot;%s,%s,%s&amp;quot;  % (rtsmid, source, target)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Scraping tweets using Twitter stream API</title>
      <link>https://chengjun.github.io/en/post/en/2013-01-20-scraping-tweets-from-twitter/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chengjun.github.io/en/post/en/2013-01-20-scraping-tweets-from-twitter/</guid>
      <description>&lt;p&gt;I want to randomly sample twitter streams. Thus, i turn to the &lt;a href=&#34;https://dev.twitter.com/docs/api/streaming&#34; target=&#34;_blank&#34;&gt;stream api of twitter&lt;/a&gt;.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The set of streaming APIs offered by Twitter give developers low latency access to Twitter&amp;rsquo;s global stream of Tweet data. A proper implementation of a streaming client will be pushed messages indicating Tweets and other events have occurred, without any of the overhead associated with polling a REST endpoint.[&lt;a href=&#34;https://dev.twitter.com/docs/api/streaming&#34; target=&#34;_blank&#34;&gt;from Twitter&lt;/a&gt;]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;http://weblab.com.cityu.edu.hk/blog/chengjun/files/2013/01/Picture1-300x188.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Of course, as the first step, you should register the stream api on Twitter to get the consumer key, consumer secret, access key, and access secret.&lt;/p&gt;

&lt;p&gt;With the help of &lt;a href=&#34;https://github.com/tweepy/tweepy&#34; target=&#34;_blank&#34;&gt;tweepy package of Python&lt;/a&gt;, I tried the following scripts. So far it works pretty well.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Twitter API Crawler
# -*- coding: utf-8 -*-

&#39;&#39;&#39;
Author: chengjun wang
Email: wangchj04@gmail.com
Hong Kong, 2013/01/20
&#39;&#39;&#39;
import sys
import tweepy
import codecs
from time import clock

&#39;&#39;&#39;OAuth Authentication&#39;&#39;&#39;
consumer_key=&amp;quot;xcEI4sb...fi6AzBQ&amp;quot;
consumer_secret=&amp;quot;5nfeG8...jUX8nU2pafr4hU&amp;quot;
access_token=&amp;quot;37595783-Fazh...8fPaH5IaTlz7y&amp;quot;
access_token_secret=&amp;quot;fyqUf5...YijKwvQe3I&amp;quot;

auth1 = tweepy.OAuthHandler(consumer_key, consumer_secret)
auth1.set_access_token(access_token, access_token_secret)
api = tweepy.API(auth1)

&#39;&#39;&#39;
# Note: Had you wanted to perform the full OAuth dance instead of using
# an access key and access secret, you could have uses the following
# four lines of code instead of the previous line that manually set the
# access token via auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET).
# auth_url = auth.get_authorization_url(signin_with_twitter=True)
# webbrowser.open(auth_url)
# verifier = raw_input(&#39;PIN: &#39;).strip()
# auth.get_access_token(verifier)
&#39;&#39;&#39;

file = open(&amp;quot;C:/Python27/twitter/mydata6.csv&amp;quot;,&#39;wb&#39;) # save to csv file

print api.me().name # api.update_status(&#39;Updating using OAuth authentication via Tweepy!&#39;)

start = clock()
print start

&#39;&#39;&#39;Specify the stream&#39;&#39;&#39;
class StreamListenerChengjun(tweepy.StreamListener):
    def on_status(self, status):
        try:
            tweet = status.text.encode(&#39;utf-8&#39;)
            tweet = tweet.replace(&#39;\n&#39;, &#39;\\n&#39;)
            user = status.author.screen_name.encode(&#39;utf-8&#39;)
            userid = status.author.id
            time = status.created_at
            source = status.source
            tweetid = status.id
            timePass = clock()-start
            if timePass%60==0:
                print &amp;quot;I have been working for&amp;quot;, timePass, &amp;quot;seconds.&amp;quot;
            if not (&#39;RT @&#39; in tweet) :  # Exclude re-tweets
                print &amp;gt;&amp;gt;file, &amp;quot;%s,%s,%s,%s,|%s|,%s&amp;quot; % (userid, user, time, tweetid, tweet, source)

        except Exception, e:
            print &amp;gt;&amp;gt; sys.stderr, &#39;Encountered Exception:&#39;, e
            pass
    def on_error(self, status_code):
        print &#39;Error: &#39; + repr(status_code)
        return True # False to stop
    def on_delete(self, status_id, user_id):
        &amp;quot;&amp;quot;&amp;quot;Called when a delete notice arrives for a status&amp;quot;&amp;quot;&amp;quot;
        print &amp;quot;Delete notice for %s. %s&amp;quot; % (status_id, user_id)
        return
    def on_limit(self, track):
        &amp;quot;&amp;quot;&amp;quot;Called when a limitation notice arrvies&amp;quot;&amp;quot;&amp;quot;
        print &amp;quot;!!! Limitation notice received: %s&amp;quot; % str(track)
        return
    def on_timeout(self):
        print &amp;gt;&amp;gt; sys.stderr, &#39;Timeout...&#39;
        time.sleep(10)
        return True

&#39;&#39;&#39;Link the tube with tweet stream&#39;&#39;&#39;
streamTube = tweepy.Stream(auth=auth1, listener=StreamListenerChengjun(), timeout= 300)  # https://github.com/tweepy/tweepy/issues/83 # setTerms = [&#39;good&#39;, &#39;goodbye&#39;, &#39;goodnight&#39;, &#39;good morning&#39;] # streamer.filter(track = setTerms)
streamTube.sample()

file.close()
pass

timePass = time.clock()-start
print timePass
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Sentiment analysis with Python</title>
      <link>https://chengjun.github.io/en/post/en/2012-03-19-sentiment-analysi-with-python/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chengjun.github.io/en/post/en/2012-03-19-sentiment-analysi-with-python/</guid>
      <description>&lt;p&gt;Learning To Do Sentiment Analysis Using Python &amp;amp; NLTK&lt;/p&gt;

&lt;p&gt;This is my first try to learn sentiment analysis using python. You can find the original post by Laurent Luce &lt;a href=&#34;http://www.laurentluce.com/posts/twitter-sentiment-analysis-using-python-and-nltk/&#34; target=&#34;_blank&#34;&gt;following this link&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I am glad to know that, with the aid of naive Bayes, NLTK could distinguish ‘like’ and ‘does not like’ into &amp;lsquo;positive&amp;rsquo; and &amp;lsquo;negative&amp;rsquo; respectively. I am wondering how R behaves compared with it. The method below employed the procedures depicted in the following figure.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://weblab.com.cityu.edu.hk/blog/chengjun/files/2012/03/overview-of-sentiment-analysis-using-nltk.png&#34; alt=&#34;original author: Laurent Luce&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure created by &lt;a href=&#34;http://www.laurentluce.com/posts/twitter-sentiment-analysis-using-python-and-nltk/&#34; target=&#34;_blank&#34;&gt;Laurent Luce&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Load in the data first.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Python script
import nltk

pos_tweets = [(&#39;I love this car&#39;, &#39;positive&#39;),
    (&#39;This view is amazing&#39;, &#39;positive&#39;),
    (&#39;I feel great this morning&#39;, &#39;positive&#39;),
    (&#39;I am so excited about the concert&#39;, &#39;positive&#39;),
    (&#39;He is my best friend&#39;, &#39;positive&#39;)]

neg_tweets = [(&#39;I do not like this car&#39;, &#39;negative&#39;),
    (&#39;This view is horrible&#39;, &#39;negative&#39;),
    (&#39;I feel tired this morning&#39;, &#39;negative&#39;),
    (&#39;I am not looking forward to the concert&#39;, &#39;negative&#39;),
    (&#39;He is my enemy&#39;, &#39;negative&#39;)]

tweets = []
for (words, sentiment) in pos_tweets + neg_tweets:
    words_filtered = [e.lower() for e in words.split() if len(e) &amp;gt;= 3]
    tweets.append((words_filtered, sentiment))

test_tweets = [
    ([&#39;feel&#39;, &#39;happy&#39;, &#39;this&#39;, &#39;morning&#39;], &#39;positive&#39;),
    ([&#39;larry&#39;, &#39;friend&#39;], &#39;positive&#39;),
    ([&#39;not&#39;, &#39;like&#39;, &#39;that&#39;, &#39;man&#39;], &#39;negative&#39;),
    ([&#39;house&#39;, &#39;not&#39;, &#39;great&#39;], &#39;negative&#39;),
    ([&#39;your&#39;, &#39;song&#39;, &#39;annoying&#39;], &#39;negative&#39;)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then we need to get the unique word list as the features for classification.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# get the word lists of tweets
def get_words_in_tweets(tweets):
    all_words = []
    for (words, sentiment) in tweets:
        all_words.extend(words)
    return all_words

# get the unique word from the word list
def get_word_features(wordlist):
    wordlist = nltk.FreqDist(wordlist)
    word_features = wordlist.keys()
    return word_features

word_features = get_word_features(get_words_in_tweets(tweets))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To create a classifier, we need to decide what features are relevant. To do that, we first need a feature extractor.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def extract_features(document):
    document_words = set(document)
    features = {}
    for word in word_features:
      features[&#39;contains(%s)&#39; % word] = (word in document_words)
    return features
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then, we can build up the training set and create the classifier:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;training_set = nltk.classify.util.apply_features(extract_features, tweets)

classifier = nltk.NaiveBayesClassifier.train(training_set)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You may want to know how to define the &amp;lsquo;train&amp;rsquo; method in NLTK here:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def train(labeled_featuresets, estimator=nltk.probability.ELEProbDist):
    # Create the P(label) distribution
    label_probdist = estimator(label_freqdist)
    # Create the P(fval|label, fname) distribution
    feature_probdist = {}
    return NaiveBayesClassifier(label_probdist, feature_probdist)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, we can use the naive bayes method to train the data. Have a look:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tweet_positive = &#39;Larry is my friend&#39;
tweet_negative = &#39;Larry is not my friend&#39;

print classifier.classify(extract_features(tweet_positive.split()))
# &amp;gt; positive
print classifier.classify(extract_features(tweet_negative.split()))
# &amp;gt; negative
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Don&amp;rsquo;t be too positive, let&amp;rsquo;s try another example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tweet_negative2 = &#39;Your song is annoying&#39;
print classifier.classify(extract_features(tweet_negative2.split()))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, we will classify the test_tweets and calculate the recall accuracy.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def classify_tweet(tweet):
    return \
        classifier.classify(extract_features(tweet)) # nltk.word_tokenize(tweet)

total = accuracy = float(len(test_tweets))

for tweet in test_tweets:
    if classify_tweet(tweet[0]) != tweet[1]:
        accuracy -= 1

print(&#39;Total accuracy: %f%% (%d/20).&#39; % (accuracy / total * 100, accuracy))
# 0.8
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Of course, this is only the starting point. In the future, I will use the same data set, to so &lt;a href=&#34;http://chengjun.github.io/en/2014/04/sentiment-analysis-with-machine-learning-in-R/&#34; target=&#34;_blank&#34;&gt;how to do it in R&lt;/a&gt; using naive Bayes and beyond.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>scholarNetwork: Visualizing Google Scholar Network</title>
      <link>https://chengjun.github.io/en/post/en/2015-02-22-scholarNetwork/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chengjun.github.io/en/post/en/2015-02-22-scholarNetwork/</guid>
      <description>

&lt;p&gt;#scholarNetwork&lt;/p&gt;

&lt;p&gt;Developed by Cheng-Jun Wang &amp;amp; Lingfei Wu&lt;/p&gt;

&lt;p&gt;Cheng-Jun Wang wangchj04@gmail.com
Lingfei Wu wlf850927@gmail.com&lt;/p&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;scholarNetwork is a python package for crawling and visualizing the co-author network of Google Scholar.&lt;/p&gt;

&lt;p&gt;To use it, you must have access to Google Scholar, and you would also install beautifulsoup4 and networkx during the installation process.&lt;/p&gt;

&lt;h2 id=&#34;install&#34;&gt;Install&lt;/h2&gt;

&lt;p&gt;Install from pypi using pip or easy_install&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;     pip install scholarNetwork
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;or&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;     easy_install scholarNetwork
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;##Use&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    from scholarNetwork import scholarNetwork
    import matplotlib.pyplot as plt
    import networkx as nx

    ## The seed of crawler
    seed = &#39;https://scholar.google.nl/citations?user=nNdt_G8AAAAJ&amp;amp;hl=en&amp;amp;oe=ASCII&#39;
    ## How many nodes do you want to visulize? Always start with a small one.
    Nmax = 21
    ## Get the graph g
    g = scholarNetwork.getGraph(seed, Nmax)

    ## plot the network
    pos=nx.spring_layout(g) #setup the layout

    nx.draw(g, pos, node_shape = &#39;o&#39;,
            edge_color = &#39;gray&#39;, width = 0.5,
            with_labels = True, arrows = True)
    plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://chengjun.qiniudn.com/example.png&#34; alt=&#34;1&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Of course, you can get a much larger graph to see what&amp;rsquo;s happening around you, just to change the value of Nmax. However, you have to be patient to wait for much longer time.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://chengjun.qiniudn.com/ego300large.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>使用Python快速分割数据的方法</title>
      <link>https://chengjun.github.io/en/post/cn/2014-08-31-fast-split-data-with-python/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chengjun.github.io/en/post/cn/2014-08-31-fast-split-data-with-python/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://chengjun.qiniudn.com/longcat.PNG&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;分割数据最慢的过程其实是打开和关闭一个文件，因此尽量减少这种操作可以飞速的提升分割数据的速度。之前在&lt;a href=&#34;http://stackoverflow.com/questions/519633/lazy-method-for-reading-big-file-in-python?lq=1&#34; target=&#34;_blank&#34;&gt;stackoverflow上看到一种方法&lt;/a&gt;非常高效，放在这里研究一下。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt; from collections import defaultdict

 path = &#39;D:/chengjun/Sina Weibo/DepthOverTime/&#39;

 #define a function 
 def splitData(f):
     #using dict to &#39;classify&#39; rows
     E = defaultdict(lambda:[]) 
     for line in f:
         lists = line.strip().split(&#39;,&#39;)
         rtmid = lists[0]
         file_save = path + &#39;single_weibo/&#39;+rtmid
         E[file_save].append(line)
     for key in E.keys():
         try:
             with open(key,&#39;a&#39;) as p:
                 for record in E[key]:
                     p.write(record+&amp;quot;\n&amp;quot;)
         except:
             pass
 # start to read in data by chunks
 bigfile = open(path + &#39;diffusion_path_date2552.csv&#39;)
 chunkSize = 100000000
 chunk = bigfile.readlines(chunkSize)
 while chunk:
     splitData(chunk)
     chunk = bigfile.readlines(chunkSize)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;上面这段代码有两个地方导致非常高效：&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;使用dict来将相同的key的行整理到一起， 以便一次将吞进来的某一个key下面的数据全部写入硬盘&lt;/li&gt;
&lt;li&gt;每次使用readlines读入足够多的行，充分发挥内存的作用&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
  </channel>
</rss>
