<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Python on </title>
    <link>https://chengjunwang.com/en/categories/python/index.xml</link>
    <description>Recent content in Python on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2016 Cheng-Jun Wang</copyright>
    <atom:link href="/en/categories/python/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Learning basic epidemic models with Python</title>
      <link>https://chengjunwang.com/en/post/en/2013-03-14-learn-basic-epidemic-models-with-python/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chengjunwang.com/en/post/en/2013-03-14-learn-basic-epidemic-models-with-python/</guid>
      <description>

&lt;p&gt;The epidemic model is another intellectual source for information diffusion research. The first known mathematical model of epidemiology is formulated by Daniel Bernoulli (1760) when he studied the mortality rates in order to eradicate the smallpox. However, it was not until the early twentieth century that deterministic modeling of epidemiology started. Ross (1911) developed differential equation models of epidemics in 1911. Later, Kermack and McKendrick (1927) found the epidemic threshold and they argued that the density of susceptible must exceed a critical value to make the outbreak of an epidemic happen.&lt;/p&gt;

&lt;p&gt;The mathematical models developed by epidemic research help clarify assumptions, variables, and parameters for diffusion research, lead to useful concepts (e.g., threshold, reproduction number), supply an experimental tool for testing theoretical conjectures, and forecast epidemic spreading in the future (Hethcote, 2009). Although epidemic models are simplifications of reality, they help us refine our understandings about the logic of diffusion beneath social realities (disease transmission, information diffusion through networks, and adoption of new technologies or behaviors). To understand the epidemic models in a better way, I will briefly review the basic epidemic models: SI, SIR, SIS, and the applications in networks.&lt;/p&gt;

&lt;h3 id=&#34;si-model&#34;&gt;SI model&lt;/h3&gt;

&lt;p&gt;The SI model is the simplest possible model of infection. In the SI model, there are only two phases in the SI epidemic spreading process: Susceptible and Infectious.  Let S be the proportion of the population that are susceptible. Let I be the proportion of the population that are infectious. At the initial time, the proportion of people who are infected is x0, the proportion of people who are susceptible is S0. β is the transmission rate, and it incorporates the encounter rate between susceptible and infectious individuals together with the probability of transmission. Consider a “closed population” with no births, deaths, or migrations, and assume the mixing is homogeneous (e.g., the susceptible individuals are uniformly spread in a geographic area, and the probability of contracting the infection is uniformly the same for all actors (T. G. Lewis, 2011)), yielding βSI as the transmission term. Thus, the equation for SI model is:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        dS/dt= -βSI
        dI/( dt)= βSI
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Given every individual in the system must be either susceptible or infected, I + S = 1. Thus, the equations above can be transformed to:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        dI/dt=βI(1-I)   
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To solve this differential equation, we can get the cumulative growth curve as a function of time:&lt;/p&gt;

&lt;p&gt;$$I[t]= \frac{x&lt;em&gt;{0} e^{\beta t }}{1-x&lt;/em&gt;{0}+ x_{0} e^{\beta t }}$$&lt;/p&gt;

&lt;p&gt;Interestingly, this is a logistic growth featured by its S-shaped curve. $$$x_{0}$$ is the initial value of I[t]. The curve grows exponentially shortly after the system is infected, and then saturates as the number of susceptible shrinks which makes it harder to find the next victims. Thus, it could be used to model the classic diffusion of innovations.
In the naive model of SI, once one is infected, it is always infectious. However, this is not realistic for many situations of disease spreading. For many diseases, people recover after a certain time because their immune systems act to fight with the diseases. There is usually a status of recovery denoted by R. Let γ denote the removal or recovery rate. Usually, researchers are more interested in its reciprocal (1/γ) which determines the average infectious period.&lt;/p&gt;

&lt;h3 id=&#34;sir-model&#34;&gt;SIR model&lt;/h3&gt;

&lt;p&gt;There are two stages of the dynamics of the SIR model. In the first stage, susceptible individuals become infected by the infectious ones with who they contact. Similar to the SI model, β is the transmission rate between individuals; In the second stage, infected individuals recover at the average rate γ. Given the premise that underlying epidemiological rates are constant, the differential equations of simple SIR model (with no births, deaths, or migrations) are:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;       dS/dt= -βSI
       dI/( dt)= βSI- γI
       dR/dt= βI
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;However, the differential equations above could not be analytically solved. In practice, researchers can evaluate SIR model numerically, as it is showed in the figure below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://weblab.com.cityu.edu.hk/blog/chengjun/files/2013/03/p2.1-SIR.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# -*- coding: utf-8 -*-

###################################
### Written by Ilias Soumpasis    #
### ilias.soumpasis@ucd.ie (work) #
### ilias.soumpasis@gmail.com     #
###################################

import scipy.integrate as spi
import numpy as np
import pylab as pl

beta=1.4247
gamma=0.14286
TS=1.0
ND=70.0
S0=1-1e-6
I0=1e-6
INPUT = (S0, I0, 0.0)

def diff_eqs(INP,t):
    &#39;&#39;&#39;The main set of equations&#39;&#39;&#39;
    Y=np.zeros((3))
    V = INP
    Y[0] = - beta * V[0] * V[1]
    Y[1] = beta * V[0] * V[1] - gamma * V[1]
    Y[2] = gamma * V[1]
    return Y   # For odeint

t_start = 0.0; t_end = ND; t_inc = TS
t_range = np.arange(t_start, t_end+t_inc, t_inc)
RES = spi.odeint(diff_eqs,INPUT,t_range)

print RES

#Ploting
pl.plot(RES[:,0], &#39;-bs&#39;, label=&#39;Susceptibles&#39;)  # I change -g to g--  # RES[:,0], &#39;-g&#39;,
pl.plot(RES[:,2], &#39;-g^&#39;, label=&#39;Recovereds&#39;)  # RES[:,2], &#39;-k&#39;,
pl.plot(RES[:,1], &#39;-ro&#39;, label=&#39;Infectious&#39;)
pl.legend(loc=0)
pl.title(&#39;SIR epidemic without births or deaths&#39;)
pl.xlabel(&#39;Time&#39;)
pl.ylabel(&#39;Susceptibles, Recovereds, and Infectious&#39;)
pl.savefig(&#39;2.1-SIR-high.png&#39;, dpi=900) # This does, too
pl.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;sis-model&#34;&gt;SIS model&lt;/h3&gt;

&lt;p&gt;Another extension of the SI model is the one that allows for reinfection.  If infected individuals are not immune to the diseases after their recovery, they can be infected more than once. The most simple model that captures this features is the SIS model. There are only two states: susceptible and infected, and infected individuals become susceptible after recovery. The differential equations for the simple SIS epidemic model are:
           dS/dt= γI-βSI
          dI/( dt)= βSI- γI&lt;/p&gt;

&lt;p&gt;Given S + I = 1, the differential equations have the solution:&lt;/p&gt;

&lt;p&gt;$$I[t]=(1-\frac{\gamma}{\beta}) \frac{C e^{(\beta - \gamma)t}}{1 + C e^{(\beta - \gamma)t}}$$&lt;/p&gt;

&lt;p&gt;C is the integration constant in the form of $$C=\frac{ \beta x&lt;em&gt;{0}}{\beta-\gamma-\beta x&lt;/em&gt;{0}} $$.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://weblab.com.cityu.edu.hk/blog/chengjun/files/2013/03/p2.1-SIS.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# -*- coding: utf-8 -*-

import scipy.integrate as spi
import numpy as np
import pylab as pl

beta=1.4247
gamma=0.14286
I0=1e-6
ND=70
TS=1.0
INPUT = (1.0-I0, I0)

def diff_eqs(INP,t):
    &#39;&#39;&#39;The main set of equations&#39;&#39;&#39;
    Y=np.zeros((2))
    V = INP
    Y[0] = - beta * V[0] * V[1] + gamma * V[1]
    Y[1] = beta * V[0] * V[1] - gamma * V[1]
    return Y   # For odeint

t_start = 0.0; t_end = ND; t_inc = TS
t_range = np.arange(t_start, t_end+t_inc, t_inc)
RES = spi.odeint(diff_eqs,INPUT,t_range)

print RES

#Ploting
pl.plot(RES[:,0], &#39;-bs&#39;, label=&#39;Susceptibles&#39;)
pl.plot(RES[:,1], &#39;-ro&#39;, label=&#39;Infectious&#39;)
pl.legend(loc=0)
pl.title(&#39;SIS epidemic without births or deaths&#39;)
pl.xlabel(&#39;Time&#39;)
pl.ylabel(&#39;Susceptibles and Infectious&#39;)
pl.savefig(&#39;2.5-SIS-high.png&#39;, dpi=900) # This does increase the resolution.
pl.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;One important contribution of epidemic models is the threshold phenomenon of epidemic diffusion existing in SIR model. The threshold of SIR model asks what factors determine whether an epidemic occur or fail. As the first step of analyzing the threshold of SIR model, the differential equation of SIR (dI/dt= βSI- βI) can be rewritten in the form:&lt;/p&gt;

&lt;p&gt;dI/( dt)=(βS - β)I&lt;/p&gt;

&lt;p&gt;If dI/dt is smaller than 0, the contagion will soon wither and die out. Thus, as a boundary condition, $$(\beta S- \gamma)$$ should be larger than 0, and S should be larger than γ/β. This is the threshold phenomenon (Kermack &amp;amp; McKendarick, 1927). Based on the rationales above, if the initial fraction of susceptible (S(0)) is less than γ/β, the infection would not be able to start the invasion in the population. Here  γ/β is defined as basic reproductive ratio $$R_{0}$$.&lt;/p&gt;

&lt;p&gt;To summarize, “for an infectious disease with an average infectious period give by 1/γ and a transmission rate β, its basic reproductive ratio R0 is determined by γ/β. In a closed population, an infection with a specified R0 can invade only if there is a threshold fraction of susceptible greater than 1/γ ” (Keeling &amp;amp; Rohani, 2011, p. 21).&lt;/p&gt;

&lt;p&gt;###The Other Extentions?
In the section above, I mainly focus on the deterministic models of epidemics. However, despite the many advantages of deterministic models, it can be difficult to include realistic population networks, to incorporate realistic probability distributions for the time spent in the infectious period, and to assess the probability of an outbreak. Thus, the stochastic epidemic simulations, such as stochastic differential equations, Markov Chain Monte Carlo (MCMC), and agent based modeling, have been used to remedy the defect.&lt;/p&gt;

&lt;p&gt;Network epidemic models have also been developed to investigate the widespread and rapid propagations (e.g., the contagion of computer virus) through a network. Typically, the network epidemic is brought about by adjacent nodes through propagations along one or more links. Network epidemic models consider the topology of the network as well as infection rate, death rate, and state transitions. This line of research is interested in the following questions: under what conditions will an initial outbreak spread to a nontrivial portion of the population? What percentage of the population will eventually become infected? What is the effect of immunization policies? For example, Pastor-Satorras et al. (2001a, 2001b) study the spreading of epidemics in complex networks using the mean-field method for network SIS model. Their findings indicate that in exponential networks (e.g., random graph network, small-world network), there is the usual epidemic threshold below which there is no prevalence of epidemic. Yet, on a wide range of scale-free networks, there is an absence of an epidemic threshold, which implies that scale-free networks are prone to the spreading of epidemics, as well as other spreading phenomena, e.g., information diffusion. Based on this rationale of the absence of epidemic threshold, network scientists expect online information diffuse to a great proportion of the population.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Scraping New York Times &amp; The Guardian using Python</title>
      <link>https://chengjunwang.com/en/post/en/2012-04-23-scraping-newyork-times-with-python/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chengjunwang.com/en/post/en/2012-04-23-scraping-newyork-times-with-python/</guid>
      <description>&lt;p&gt;I have read the blog post about Scraping New York Times Articles with R. It’s great. I want to reproduce the work with python.
First, we should learn about nytimes article search api.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://developer.nytimes.com/docs/article_search_api/&#34; target=&#34;_blank&#34;&gt;http://developer.nytimes.com/docs/article_search_api/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Second, we need to register and get the key which will be used in python script.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://developer.nytimes.com/apps/register&#34; target=&#34;_blank&#34;&gt;http://developer.nytimes.com/apps/register&lt;/a&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# !/usr/bin/env python
# -*- coding: UTF-8  -*-
# Scraping New York Times using python
# 20120421@ Canberra
# chengjun wang

import json
import urllib2

&#39;&#39;&#39;
About the api and the key, see the links above.
&#39;&#39;&#39;

&#39;&#39;&#39;step 1: input query information&#39;&#39;&#39;
apiUrl=&#39;http://api.nytimes.com/svc/search/v1/article?format=json&#39;
query=&#39;query=occupy+wall+street&#39;                            # set the query word here
apiDate=&#39;begin_date=20110901&amp;amp;end_date=20120214&#39;             # set the date here
fields=&#39;fields=body%2Curl%2Ctitle%2Cdate%2Cdes_facet%2Cdesk_facet%2Cbyline&#39;
offset=&#39;offset=0&#39;
key=&#39;api-key=c2c5b91680.......2811165&#39;  # input your key here

&#39;&#39;&#39;step 2: get the number of offset/pages&#39;&#39;&#39;
link=[apiUrl, query, apiDate, fields, offset, key]
ReqUrl=&#39;&amp;amp;&#39;.join(link)
jstr = urllib2.urlopen(ReqUrl).read()  # t = jstr.strip(&#39;()&#39;)
ts = json.loads( jstr )
number=ts[&#39;total&#39;] #  the number of queries  # query=ts[&#39;tokens&#39;] # result=ts[&#39;results&#39;]
print number
seq=range(number/9)  # this is not a good way
print seq

&#39;&#39;&#39;step 3: crawl the data and dump into csv&#39;&#39;&#39;
import csv
addressForSavingData= &amp;quot;D:/Research/Dropbox/tweets/wapor_assessing online opinion/News coverage of ows/nyt.csv&amp;quot;
file = open(addressForSavingData,&#39;wb&#39;) # save to csv file
for i in seq:
    nums=str(i)
    offsets=&#39;&#39;.join([&#39;offset=&#39;, nums]) # I made error here, and print is a good way to test
    links=[apiUrl, query, apiDate, fields, offsets, key]
    ReqUrls=&#39;&amp;amp;&#39;.join(links)
    print &amp;quot;*_____________*&amp;quot;, ReqUrls
    jstrs = urllib2.urlopen(ReqUrls).read()
    t = jstrs.strip(&#39;()&#39;)
    tss= json.loads( t )  # error no joson object
    result = tss[&#39;results&#39;]
    for ob in result:
        title=ob[&#39;title&#39;]  # body=ob[&#39;body&#39;]   # body,url,title,date,des_facet,desk_facet,byline
        print title
        url=ob[&#39;url&#39;]
        date=ob[&#39;date&#39;] # desk_facet=ob[&#39;desk_facet&#39;]  # byline=ob[&#39;byline&#39;] # some author names don&#39;t exist
        w = csv.writer(file,delimiter=&#39;,&#39;,quotechar=&#39;|&#39;, quoting=csv.QUOTE_MINIMAL)
        w.writerow((date, title, url)) # write it out
file.close()
pass
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;see the result:&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://weblab.com.cityu.edu.hk/blog/chengjun/files/2012/04/nyt1.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Similarly, you can crawl the article data from The Guardian. See the link below.&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://explorer.content.guardianapis.com/#/?format=json&amp;amp;order-by=newest&#34; target=&#34;_blank&#34;&gt;http://explorer.content.guardianapis.com/#/?format=json&amp;amp;order-by=newest&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;After you have registered you app and got the key, we can work on the python script.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# !/usr/bin/env python
# -*- coding: UTF-8 -*-
# Scraping The Guardian using Python
# 20120421@ Canberra
# chengjun wang

import json
import urllib2

&#39;&#39;&#39;

http://content.guardianapis.com/search?q=occupy+wall+street&amp;amp;from-date=2011-09-01&amp;amp;to-date=2012-02-14&amp;amp;page=2

&amp;amp;page-size=3&amp;amp;format=json&amp;amp;show-fields=all&amp;amp;use-date=newspaper-edition&amp;amp;api-key=m....g33gzq
&#39;&#39;&#39;

&#39;&#39;&#39;step 1: input query information&#39;&#39;&#39;
apiUrl=&#39;http://content.guardianapis.com/search?q=occupy+wall+street&#39; # set the query word here
apiDate=&#39;from-date=2011-09-01&amp;amp;to-date=2011-10-14&#39;           # set the date here
apiPage=&#39;page=2&#39;   # set the page
apiNum=10       # set the number of articles in one page
apiPageSize=&#39;&#39;.join([&#39;page-size=&#39;,str(apiNum)])
fields=&#39;format=json&amp;amp;show-fields=all&amp;amp;use-date=newspaper-edition&#39;
key=&#39;api-key=mudfuj...g33gzq&#39; # input your key here

&#39;&#39;&#39;step 2: get the number of offset/pages&#39;&#39;&#39;
link=[apiUrl, apiDate, apiPage, apiPageSize, fields, key]
ReqUrl=&#39;&amp;amp;&#39;.join(link)
jstr = urllib2.urlopen(ReqUrl).read() # t = jstr.strip(&#39;()&#39;)
ts = json.loads( jstr )
number=ts[&#39;response&#39;][&#39;total&#39;] # the number of queries # query=ts[&#39;tokens&#39;] # result=ts[&#39;results&#39;]
print number
seq=range(number/(apiNum-1)) # this is not a good way
print seq

&#39;&#39;&#39;step 3: crawl the data and dump into csv&#39;&#39;&#39;
import csv
addressForSavingData= &amp;quot;D:/Research/Dropbox/tweets/wapor_assessing online opinion/News coverage of ows/guardian.csv&amp;quot;
file = open(addressForSavingData,&#39;wb&#39;) # save to csv file
for i in seq:
    nums=str(i+1)
    apiPages=&#39;&#39;.join([&#39;page=&#39;, nums]) # I made error here, and print is a good way to test
    links= [apiUrl, apiDate, apiPages, apiPageSize, fields, key]
    ReqUrls=&#39;&amp;amp;&#39;.join(links)
    print &amp;quot;*_____________*&amp;quot;, ReqUrls
    jstrs = urllib2.urlopen(ReqUrls).read()
    t = jstrs.strip(&#39;()&#39;)
    tss= json.loads( t )
    result = tss[&#39;response&#39;][&#39;results&#39;]
    for ob in result:
        title=ob[&#39;webTitle&#39;].encode(&#39;utf-8&#39;) # body=ob[&#39;body&#39;]  # body,url,title,date,des_facet,desk_facet,byline
        print title
        section=ob[&amp;quot;sectionName&amp;quot;].encode(&#39;utf-8&#39;)
        url=ob[&#39;webUrl&#39;]
        date=ob[&#39;fields&#39;][&#39;newspaperEditionDate&#39;] # date=ob[&#39;webPublicationDate&#39;] # byline=ob[&#39;fields&#39;][&#39;byline&#39;]
        w = csv.writer(file,delimiter=&#39;,&#39;,quotechar=&#39;|&#39;, quoting=csv.QUOTE_MINIMAL)
        w.writerow((date, title, section, url)) # write it out
file.close()
pass
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Scraping data from Sina Weibo using Python</title>
      <link>https://chengjunwang.com/en/post/en/2014-03-16-scraping-weibo-using-python/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chengjunwang.com/en/post/en/2014-03-16-scraping-weibo-using-python/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://weblab.com.cityu.edu.hk/blog/chengjun/files/2012/09/33.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;weibo-oauth2-0&#34;&gt;Weibo Oauth2.0&lt;/h3&gt;

&lt;p&gt;I would like to introduce you how to use python to scrape tweets from Sina Weibo in this post.&lt;/p&gt;

&lt;h4 id=&#34;automatically-get-authorization-by-oauth2-0&#34;&gt;Automatically get authorization by oauth2.0&lt;/h4&gt;

&lt;p&gt;First, following this webpage to set your app, especially to get the app key, app secret, and set the callback url.  See an &lt;a href=&#34;http://blog.laisky.us/2012/01/278/&#34; target=&#34;_blank&#34;&gt;introduction&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Second, following this &lt;a href=&#34;http://www.how2dns.com/blog/?p=538&#34; target=&#34;_blank&#34;&gt;link&lt;/a&gt;, you can automatically get the code from the callback url.&lt;/p&gt;

&lt;p&gt;The following python script demonstrates how to automatically get authorization.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/usr/bin/env python
# -*- coding: utf8 -*-

from weibo import APIClient
import urllib2
import urllib
import sys
import time
from time import clock
import csv
import random

reload(sys)
sys.setdefaultencoding(&#39;utf-8&#39;)

&#39;&#39;&#39;Step 0 Login with OAuth2.0&#39;&#39;&#39;
if __name__ == &amp;quot;__main__&amp;quot;:
    APP_KEY = &#39;663...&#39; # app key
    APP_SECRET = &#39;2fc....&#39; # app secret
    CALLBACK_URL = &#39;https://api.weibo.com/oauth2/default.html&#39; # set callback url exactly like this!
    AUTH_URL = &#39;https://api.weibo.com/oauth2/authorize&#39;
    USERID = &#39;w...4&#39; # your weibo user id
    PASSWD = &#39;w....&#39; #your pw

    client = APIClient(app_key=APP_KEY, app_secret=APP_SECRET, redirect_uri=CALLBACK_URL)
    referer_url = client.get_authorize_url()
    print &amp;quot;referer url is : %s&amp;quot; % referer_url

    cookies = urllib2.HTTPCookieProcessor()
    opener = urllib2.build_opener(cookies)
    urllib2.install_opener(opener)

    postdata = {&amp;quot;client_id&amp;quot;: APP_KEY,
                &amp;quot;redirect_uri&amp;quot;: CALLBACK_URL,
                &amp;quot;userId&amp;quot;: USERID,
                &amp;quot;passwd&amp;quot;: PASSWD,
                &amp;quot;isLoginSina&amp;quot;: &amp;quot;0&amp;quot;,
                &amp;quot;action&amp;quot;: &amp;quot;submit&amp;quot;,
                &amp;quot;response_type&amp;quot;: &amp;quot;code&amp;quot;,
                }
    headers = {&amp;quot;User-Agent&amp;quot;: &amp;quot;Mozilla/5.0 (Windows NT 6.1; rv:11.0) Gecko/20100101 Firefox/11.0&amp;quot;,
                &amp;quot;Host&amp;quot;: &amp;quot;api.weibo.com&amp;quot;,
                &amp;quot;Referer&amp;quot;: referer_url
            }

    req  = urllib2.Request(
       url = AUTH_URL,
       data = urllib.urlencode(postdata),
       headers = headers
       )
    try:
        resp = urllib2.urlopen(req)
        print &amp;quot;callback url is : %s&amp;quot; % resp.geturl()
        code = resp.geturl()[-32:]
        print &amp;quot;code is : %s&amp;quot; %  code
    except Exception, e:
        print e

r = client.request_access_token(code)
access_token1 = r.access_token # The token return by sina
expires_in = r.expires_in

print &amp;quot;access_token=&amp;quot; ,access_token1
print &amp;quot;expires_in=&amp;quot; ,expires_in   # access_token lifetime by second. http://open.weibo.com/wiki/OAuth2/access_token

&amp;quot;&amp;quot;&amp;quot;save the access token&amp;quot;&amp;quot;&amp;quot;
client.set_access_token(access_token1, expires_in)
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;get-the-number-of-retweets&#34;&gt;Get the number of retweets&lt;/h3&gt;

&lt;p&gt;Step 1. Assume that you have had a list of tweet ids, you want to get the number of repsots. Thus, you can have the distribution of the size of diffusion.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&#39;&#39;&#39; Step 1 Get the number of reposts&#39;&#39;&#39;
&amp;quot;&amp;quot;&amp;quot;get the user ids&amp;quot;&amp;quot;&amp;quot;
dataReader = csv.reader(open(&#39;C:/Python27/weibo/sampledRtIds2.csv&#39;, &#39;r&#39;), delimiter=&#39;,&#39;, quotechar=&#39;|&#39;)
ids = []
for row in dataReader:
    ids.append(int(row[0]))  # modify the number to get the diffusers&#39; ids

file = open(&amp;quot;C:/Python27/weibo/repostsRT300000m2.csv&amp;quot;,&#39;wb&#39;) # save to csv file

start = clock()
print start

for seqNum in range(1500, 2999):
    id = ids[(0 + 100*seqNum) : (100+100*seqNum)]
    id = str(id).strip(&#39;[]&#39;).replace(&#39;L&#39;, &#39;&#39;)
    rate = client.get.account__rate_limit_status()
    sleep_time = rate.reset_time_in_seconds + 300
    remaining_ip_hits = rate.remaining_ip_hits
    remaining_user_hits = rate.remaining_user_hits
    if remaining_ip_hits &amp;gt;= 10 and remaining_user_hits &amp;gt;= 5:
        rtc = client.get.statuses__count(ids = id) # mid, 100
        for n in range(0, len(rtc)): # 0-99
            mid = rtc[n][&#39;id&#39;]
            reposts = rtc[n][&#39;reposts&#39;]
            comments = rtc[n][&#39;comments&#39;]
            attitudes = rtc[n][&#39;attitudes&#39;]
            timePass = clock()-start
            if round(timePass) % 10 == 0:
                print mid, reposts, len(rtc), &amp;quot;I have been working for %s seconds&amp;quot; % round(timePass)
            print &amp;gt;&amp;gt;file, &amp;quot;%s,%s,%s,%s&amp;quot; % (mid, reposts, comments, attitudes)
    elif remaining_ip_hits &amp;lt; 10 or remaining_user_hits &amp;lt; 5:
        print &amp;quot;Python will sleep %s seconds&amp;quot; % sleep_time
        time.sleep(sleep_time+60)

file.close()
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;get-the-list-of-diffusers&#34;&gt;Get the list of diffusers&lt;/h3&gt;

&lt;p&gt;Step 2. If you want to step further and get the list of diffusers for a list of weibos. Thus, you will know how many reposts or retweets have been deleted by the website.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# &#39;&#39;&#39;Step 2 Get the diffusers&#39;&#39;&#39;
&amp;quot;&amp;quot;&amp;quot;read ids&amp;quot;&amp;quot;&amp;quot;
dataReader = csv.reader(open(&#39;C:/Python27/weibo/repostsSample3.csv&#39;, &#39;r&#39;), delimiter=&#39;,&#39;, quotechar=&#39;|&#39;)
ids = []
for row in dataReader:
    ids.append(int(row[1]))  # get the number to get the mid

addressForSavingData= &amp;quot;C:/Python27/weibo/diffsersSave.csv&amp;quot;
file = open(addressForSavingData,&#39;wb&#39;) # save to csv file

start = clock()
print start

lenid = len(ids) # lenid = 8 # test with the first two cases

for n in range(0, lenid+1):  # the 78 should be 77 here
    rate = client.get.account__rate_limit_status()
    sleep_time = rate.reset_time_in_seconds + 300
    remaining_ip_hits = rate.remaining_ip_hits
    remaining_user_hits = rate.remaining_user_hits
    if remaining_ip_hits &amp;gt;= 10 and remaining_user_hits &amp;gt;= 3:
        if reposts[n]%200 == 0:
            pages = reposts[n]/200
        else:
            pages = reposts[n]/200 + 1
        try:
            for pageNum in range(1, pages + 1):
                r = client.get.statuses__repost_timeline(id = ids[n], page = pageNum, count = 200)
                if len(r) == 0:
                    pass
                else:
                    m = int(len(r[&#39;reposts&#39;]))
                    for i in range(0, m):
                        &amp;quot;&amp;quot;&amp;quot;1.1 reposts&amp;quot;&amp;quot;&amp;quot;
                        mid = r[&#39;reposts&#39;][i].id
                        text = r[&#39;reposts&#39;][i].text.replace(&amp;quot;,&amp;quot;, &amp;quot;&amp;quot;)
                        created = r[&#39;reposts&#39;][i].created_at
                        &amp;quot;&amp;quot;&amp;quot;1.2 reposts.user&amp;quot;&amp;quot;&amp;quot;
                        user = r[&#39;reposts&#39;][i].user
                        user_id = user.id
                        user_name = user.name
                        user_province = user.province
                        user_city = user.city
                        user_gender = user.gender
                        user_url = user.url
                        user_followers = user.followers_count
                        user_bifollowers = user.bi_followers_count
                        user_friends = user.friends_count
                        user_statuses = user.statuses_count
                        user_created = user.created_at
                        user_verified = user.verified
                        &amp;quot;&amp;quot;&amp;quot;2.1 retweeted_status&amp;quot;&amp;quot;&amp;quot;
                        rts = r[&#39;reposts&#39;][i].retweeted_status
                        rts_mid = rts.id
                        rts_text = rts.text.replace(&amp;quot;,&amp;quot;, &amp;quot;&amp;quot;)
                        rts_created = rts.created_at
                        &amp;quot;&amp;quot;&amp;quot;2.2 retweeted_status.user&amp;quot;&amp;quot;&amp;quot;
                        rtsuser_id = rts.user.id
                        rtsuser_name = rts.user.name
                        rtsuser_province = rts.user.province
                        rtsuser_city = rts.user.city
                        rtsuser_gender = rts.user.gender
                        rtsuser_url = rts.user.url
                        rtsuser_followers = rts.user.followers_count
                        rtsuser_bifollowers = rts.user.bi_followers_count
                        rtsuser_friends = rts.user.friends_count
                        rtsuser_statuses = rts.user.statuses_count
                        rtsuser_created = rts.user.created_at
                        rtsuser_verified = rts.user.verified
                        timePass = clock()-start
                        if round(timePass) % 10 == 0:
                            print mid, rts_mid, &amp;quot;I have been working for %s seconds&amp;quot; % round(timePass)
                            time.sleep( random.randrange(3, 9, 1) )  # To avoid http error 504 gateway time-out
                        print &amp;gt;&amp;gt;file, &amp;quot;%s,&#39;%s&#39;,&#39;%s&#39;,%s,&#39;%s&#39;,%s,%s,%s,&#39;%s&#39;,%s,%s,%s,&#39;%s&#39;,%s,%s,&#39;%s&#39;,%s,&#39;%s&#39;,%s,%s,%s,&#39;%s&#39;,%s,%s,%s,%s,%s&amp;quot;  % (mid, created, text, # 3 # &amp;quot;%s,%s,|%s|,%s,|%s|,%s,%s,%s,|%s|,%s,%s,%s,%s,%s,%s,%s,%s,|%s|,%s,%s,%s,|%s|,%s,%s,%s,%s,%s&amp;quot; % (mid, created, text, # 3
                                            user_id, user_name, user_province, user_city, user_gender,  # 5 --&amp;gt; 5
                                            user_url, user_followers, user_friends, user_statuses, user_created, user_verified,  # rts_text, # 6 --&amp;gt; 9
                                            rts_mid, rts_created, # 2
                                            rtsuser_id, rtsuser_name, rtsuser_province, rtsuser_city, rtsuser_gender, # 5 --&amp;gt; 18
                                            rtsuser_url, rtsuser_followers, rtsuser_friends, rtsuser_statuses, rtsuser_created, rtsuser_verified)  # 6  --&amp;gt; 22
        except Exception, e:
            print &amp;gt;&amp;gt; sys.stderr, &#39;Encountered Exception:&#39;, e, ids[n]
            time.sleep(120)
            pass
    elif remaining_ip_hits &amp;lt; 10 or remaining_user_hits &amp;lt; 3:
        print &amp;quot;Python will sleep %s seconds&amp;quot; % sleep_time
        time.sleep(sleep_time+60)

file.close()
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;get-the-following-relationships&#34;&gt;Get the following relationships&lt;/h3&gt;

&lt;p&gt;Step 3. Now, you may want to get the social graph for all the diffusers.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&#39;&#39;&#39;Step 3 Get the social graph&#39;&#39;&#39;
&amp;quot;&amp;quot;&amp;quot;read ids&amp;quot;&amp;quot;&amp;quot;
dataReader = csv.reader(open(&#39;C:/Python27/weibo/SocialGraphIdsForStepThree.csv&#39;, &#39;r&#39;), delimiter=&#39;,&#39;, quotechar=&#39;|&#39;)
ids = []
for row in dataReader:
    ids.append(int(row[0]))  # get the number to get the mid

ids = ids[188648:697060]

addressForSavingData= &amp;quot;C:/Python27/weibo/socialgraphSave142_2.csv&amp;quot;
file = open(addressForSavingData,&#39;wb&#39;) # save to csv file

addressForSavingError = &amp;quot;C:/Python27/weibo/socialgraphSaveError142_2.csv&amp;quot;
errorlog = open(addressForSavingError,&#39;w&#39;)
errorlog.close()

start = clock()
print start

for id in ids:
    try:
        rate = client.get.account__rate_limit_status()
        sleep_time = rate.reset_time_in_seconds + 300
        remaining_ip_hits = rate.remaining_ip_hits
        remaining_user_hits = rate.remaining_user_hits
        if remaining_ip_hits &amp;gt;= 10 and remaining_user_hits &amp;gt;= 3:
            cursor = -1
            fids=[]
            while cursor != 0:
                response = client.get.friendships__friends__ids(uid=id, count= 5000, cursor=cursor)  # the biggest count is 5000
                fids    += response.ids
                cursor = response.next_cursor # previousCursor = response.previous_cursor
                timePass = clock()-start
                if round(timePass) % 10 == 0:
                    print id, &amp;quot;I have been working for %s seconds&amp;quot; % round(timePass)
                    # time.sleep( 0.01 * random.randrange(0, 5, 1) )  # To avoid http error 504 gateway time-out
                if cursor == 0:
                    totalNum = response.total_number
                    for fid in fids:
                        print &amp;gt;&amp;gt;file, &amp;quot;%s,%s,%s&amp;quot;  % (id, fid, totalNum)
                    break
        elif remaining_ip_hits &amp;lt; 10 or remaining_user_hits &amp;lt; 3:
            print &amp;quot;Python will sleep %s seconds&amp;quot; % sleep_time
            time.sleep(sleep_time+60)
    except Exception, e:
        print &amp;gt;&amp;gt;sys.stderr, &#39;Encountered Exception:&#39;, e, id
        errorlog = open(addressForSavingError, &#39;a&#39;)
        print &amp;gt;&amp;gt;errorlog, &amp;quot;%s,%s&amp;quot;  % (id, e)
        errorlog.close()
        print &#39;When the error happens, the id is:&#39;, id
        time.sleep(60)
        pass

file.close()
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;get-the-diffusion-network&#34;&gt;Get the diffusion network&lt;/h3&gt;

&lt;p&gt;Step 4. Given the collected data of retweets, we can get the diffusion path by parsing the text of weibo.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import re
import sys
from time import clock

reload(sys)
sys.setdefaultencoding(&#39;utf-8&#39;)

&#39;&#39;&#39;
Convert &amp;quot;Thu Aug 04 11:39:32 +0800 2011&amp;quot; to the ISO format: YYYY-MM-DD H:M:S
Refer to: http://stackoverflow.com/questions/15727510/using-python-regex-to-identify-retweeters-from-tweets-with-chinese-characters
&#39;&#39;&#39;

file = open(&amp;quot;D:/chengjun/New/repostsReSampleClean.csv&amp;quot;, &#39;r&#39;)
lines = file.readlines()

addressForSavingData= &amp;quot;D:/chengjun/New/diffusion_path6.csv&amp;quot;  
file = open(addressForSavingData,&#39;wb&#39;) # save to csv file

addressForSavingError = &amp;quot;D:/chengjun/New/Error.csv&amp;quot;  
errorlog = open(addressForSavingError,&#39;w&#39;)
errorlog.close

start = clock()  
print start

for line in lines:
    list = line.split(&#39;,&#39;)
    rtsmid = list[15].strip()  #rmid
    userName = list[5].strip().replace(&amp;quot;&#39;&amp;quot;, &amp;quot;&amp;quot;) # username
    submitterName = list[18].strip().replace(&amp;quot;&#39;&amp;quot;, &amp;quot;&amp;quot;)
    tweet = list[3].replace(&#39;,&#39;,&#39;&#39;)
    RTpattern = r&#39;&#39;&#39;//?@(\w+)&#39;&#39;&#39;
    rt = re.findall(RTpattern, tweet.decode(&amp;quot;utf-8&amp;quot;), re.UNICODE)
    if rt == None or len(rt)==0:
        target = userName
        source = submitterName
        print &amp;gt;&amp;gt;file, &amp;quot;%s,%s,%s&amp;quot;  % (rtsmid, source, target)
    elif rt != None and len(rt) != 0:
        rt.insert(0, userName) #
        for i in xrange(len(rt) - 1):
            target = rt[i].encode(&#39;utf-8&#39;)
            source = rt[i + 1].encode(&#39;utf-8&#39;)
            print &amp;gt;&amp;gt;file, &amp;quot;%s,%s,%s&amp;quot;  % (rtsmid, source, target)
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Scraping tweets using Twitter stream API</title>
      <link>https://chengjunwang.com/en/post/en/2013-01-20-scraping-tweets-from-twitter/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chengjunwang.com/en/post/en/2013-01-20-scraping-tweets-from-twitter/</guid>
      <description>&lt;p&gt;I want to randomly sample twitter streams. Thus, i turn to the &lt;a href=&#34;https://dev.twitter.com/docs/api/streaming&#34; target=&#34;_blank&#34;&gt;stream api of twitter&lt;/a&gt;.&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;The set of streaming APIs offered by Twitter give developers low latency access to Twitter&amp;rsquo;s global stream of Tweet data. A proper implementation of a streaming client will be pushed messages indicating Tweets and other events have occurred, without any of the overhead associated with polling a REST endpoint.[&lt;a href=&#34;https://dev.twitter.com/docs/api/streaming&#34; target=&#34;_blank&#34;&gt;from Twitter&lt;/a&gt;]&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;http://weblab.com.cityu.edu.hk/blog/chengjun/files/2013/01/Picture1-300x188.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Of course, as the first step, you should register the stream api on Twitter to get the consumer key, consumer secret, access key, and access secret.&lt;/p&gt;

&lt;p&gt;With the help of &lt;a href=&#34;https://github.com/tweepy/tweepy&#34; target=&#34;_blank&#34;&gt;tweepy package of Python&lt;/a&gt;, I tried the following scripts. So far it works pretty well.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Twitter API Crawler
# -*- coding: utf-8 -*-

&#39;&#39;&#39;
Author: chengjun wang
Email: wangchj04@gmail.com
Hong Kong, 2013/01/20
&#39;&#39;&#39;
import sys
import tweepy
import codecs
from time import clock

&#39;&#39;&#39;OAuth Authentication&#39;&#39;&#39;
consumer_key=&amp;quot;xcEI4sb...fi6AzBQ&amp;quot;
consumer_secret=&amp;quot;5nfeG8...jUX8nU2pafr4hU&amp;quot;
access_token=&amp;quot;37595783-Fazh...8fPaH5IaTlz7y&amp;quot;
access_token_secret=&amp;quot;fyqUf5...YijKwvQe3I&amp;quot;

auth1 = tweepy.OAuthHandler(consumer_key, consumer_secret)
auth1.set_access_token(access_token, access_token_secret)
api = tweepy.API(auth1)

&#39;&#39;&#39;
# Note: Had you wanted to perform the full OAuth dance instead of using
# an access key and access secret, you could have uses the following
# four lines of code instead of the previous line that manually set the
# access token via auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET).
# auth_url = auth.get_authorization_url(signin_with_twitter=True)
# webbrowser.open(auth_url)
# verifier = raw_input(&#39;PIN: &#39;).strip()
# auth.get_access_token(verifier)
&#39;&#39;&#39;

file = open(&amp;quot;C:/Python27/twitter/mydata6.csv&amp;quot;,&#39;wb&#39;) # save to csv file

print api.me().name # api.update_status(&#39;Updating using OAuth authentication via Tweepy!&#39;)

start = clock()
print start

&#39;&#39;&#39;Specify the stream&#39;&#39;&#39;
class StreamListenerChengjun(tweepy.StreamListener):
    def on_status(self, status):
        try:
            tweet = status.text.encode(&#39;utf-8&#39;)
            tweet = tweet.replace(&#39;\n&#39;, &#39;\\n&#39;)
            user = status.author.screen_name.encode(&#39;utf-8&#39;)
            userid = status.author.id
            time = status.created_at
            source = status.source
            tweetid = status.id
            timePass = clock()-start
            if timePass%60==0:
                print &amp;quot;I have been working for&amp;quot;, timePass, &amp;quot;seconds.&amp;quot;
            if not (&#39;RT @&#39; in tweet) :  # Exclude re-tweets
                print &amp;gt;&amp;gt;file, &amp;quot;%s,%s,%s,%s,|%s|,%s&amp;quot; % (userid, user, time, tweetid, tweet, source)

        except Exception, e:
            print &amp;gt;&amp;gt; sys.stderr, &#39;Encountered Exception:&#39;, e
            pass
    def on_error(self, status_code):
        print &#39;Error: &#39; + repr(status_code)
        return True # False to stop
    def on_delete(self, status_id, user_id):
        &amp;quot;&amp;quot;&amp;quot;Called when a delete notice arrives for a status&amp;quot;&amp;quot;&amp;quot;
        print &amp;quot;Delete notice for %s. %s&amp;quot; % (status_id, user_id)
        return
    def on_limit(self, track):
        &amp;quot;&amp;quot;&amp;quot;Called when a limitation notice arrvies&amp;quot;&amp;quot;&amp;quot;
        print &amp;quot;!!! Limitation notice received: %s&amp;quot; % str(track)
        return
    def on_timeout(self):
        print &amp;gt;&amp;gt; sys.stderr, &#39;Timeout...&#39;
        time.sleep(10)
        return True

&#39;&#39;&#39;Link the tube with tweet stream&#39;&#39;&#39;
streamTube = tweepy.Stream(auth=auth1, listener=StreamListenerChengjun(), timeout= 300)  # https://github.com/tweepy/tweepy/issues/83 # setTerms = [&#39;good&#39;, &#39;goodbye&#39;, &#39;goodnight&#39;, &#39;good morning&#39;] # streamer.filter(track = setTerms)
streamTube.sample()

file.close()
pass

timePass = time.clock()-start
print timePass
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Sentiment analysis with Python</title>
      <link>https://chengjunwang.com/en/post/en/2012-03-19-sentiment-analysi-with-python/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chengjunwang.com/en/post/en/2012-03-19-sentiment-analysi-with-python/</guid>
      <description>&lt;p&gt;Learning To Do Sentiment Analysis Using Python &amp;amp; NLTK&lt;/p&gt;

&lt;p&gt;This is my first try to learn sentiment analysis using python. You can find the original post by Laurent Luce &lt;a href=&#34;http://www.laurentluce.com/posts/twitter-sentiment-analysis-using-python-and-nltk/&#34; target=&#34;_blank&#34;&gt;following this link&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I am glad to know that, with the aid of naive Bayes, NLTK could distinguish ‘like’ and ‘does not like’ into &amp;lsquo;positive&amp;rsquo; and &amp;lsquo;negative&amp;rsquo; respectively. I am wondering how R behaves compared with it. The method below employed the procedures depicted in the following figure.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://weblab.com.cityu.edu.hk/blog/chengjun/files/2012/03/overview-of-sentiment-analysis-using-nltk.png&#34; alt=&#34;original author: Laurent Luce&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure created by &lt;a href=&#34;http://www.laurentluce.com/posts/twitter-sentiment-analysis-using-python-and-nltk/&#34; target=&#34;_blank&#34;&gt;Laurent Luce&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Load in the data first.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# Python script
import nltk

pos_tweets = [(&#39;I love this car&#39;, &#39;positive&#39;),
    (&#39;This view is amazing&#39;, &#39;positive&#39;),
    (&#39;I feel great this morning&#39;, &#39;positive&#39;),
    (&#39;I am so excited about the concert&#39;, &#39;positive&#39;),
    (&#39;He is my best friend&#39;, &#39;positive&#39;)]

neg_tweets = [(&#39;I do not like this car&#39;, &#39;negative&#39;),
    (&#39;This view is horrible&#39;, &#39;negative&#39;),
    (&#39;I feel tired this morning&#39;, &#39;negative&#39;),
    (&#39;I am not looking forward to the concert&#39;, &#39;negative&#39;),
    (&#39;He is my enemy&#39;, &#39;negative&#39;)]

tweets = []
for (words, sentiment) in pos_tweets + neg_tweets:
    words_filtered = [e.lower() for e in words.split() if len(e) &amp;gt;= 3]
    tweets.append((words_filtered, sentiment))

test_tweets = [
    ([&#39;feel&#39;, &#39;happy&#39;, &#39;this&#39;, &#39;morning&#39;], &#39;positive&#39;),
    ([&#39;larry&#39;, &#39;friend&#39;], &#39;positive&#39;),
    ([&#39;not&#39;, &#39;like&#39;, &#39;that&#39;, &#39;man&#39;], &#39;negative&#39;),
    ([&#39;house&#39;, &#39;not&#39;, &#39;great&#39;], &#39;negative&#39;),
    ([&#39;your&#39;, &#39;song&#39;, &#39;annoying&#39;], &#39;negative&#39;)]
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then we need to get the unique word list as the features for classification.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;# get the word lists of tweets
def get_words_in_tweets(tweets):
    all_words = []
    for (words, sentiment) in tweets:
        all_words.extend(words)
    return all_words

# get the unique word from the word list
def get_word_features(wordlist):
    wordlist = nltk.FreqDist(wordlist)
    word_features = wordlist.keys()
    return word_features

word_features = get_word_features(get_words_in_tweets(tweets))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;To create a classifier, we need to decide what features are relevant. To do that, we first need a feature extractor.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def extract_features(document):
    document_words = set(document)
    features = {}
    for word in word_features:
      features[&#39;contains(%s)&#39; % word] = (word in document_words)
    return features
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then, we can build up the training set and create the classifier:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;training_set = nltk.classify.util.apply_features(extract_features, tweets)

classifier = nltk.NaiveBayesClassifier.train(training_set)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;You may want to know how to define the &amp;lsquo;train&amp;rsquo; method in NLTK here:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def train(labeled_featuresets, estimator=nltk.probability.ELEProbDist):
    # Create the P(label) distribution
    label_probdist = estimator(label_freqdist)
    # Create the P(fval|label, fname) distribution
    feature_probdist = {}
    return NaiveBayesClassifier(label_probdist, feature_probdist)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, we can use the naive bayes method to train the data. Have a look:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tweet_positive = &#39;Larry is my friend&#39;
tweet_negative = &#39;Larry is not my friend&#39;

print classifier.classify(extract_features(tweet_positive.split()))
# &amp;gt; positive
print classifier.classify(extract_features(tweet_negative.split()))
# &amp;gt; negative
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Don&amp;rsquo;t be too positive, let&amp;rsquo;s try another example:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;tweet_negative2 = &#39;Your song is annoying&#39;
print classifier.classify(extract_features(tweet_negative2.split()))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Now, we will classify the test_tweets and calculate the recall accuracy.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;def classify_tweet(tweet):
    return \
        classifier.classify(extract_features(tweet)) # nltk.word_tokenize(tweet)

total = accuracy = float(len(test_tweets))

for tweet in test_tweets:
    if classify_tweet(tweet[0]) != tweet[1]:
        accuracy -= 1

print(&#39;Total accuracy: %f%% (%d/20).&#39; % (accuracy / total * 100, accuracy))
# 0.8
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Of course, this is only the starting point. In the future, I will use the same data set, to so &lt;a href=&#34;http://chengjun.github.io/en/2014/04/sentiment-analysis-with-machine-learning-in-R/&#34; target=&#34;_blank&#34;&gt;how to do it in R&lt;/a&gt; using naive Bayes and beyond.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>scholarNetwork: Visualizing Google Scholar Network</title>
      <link>https://chengjunwang.com/en/post/en/2015-02-22-scholarNetwork/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://chengjunwang.com/en/post/en/2015-02-22-scholarNetwork/</guid>
      <description>

&lt;p&gt;#scholarNetwork&lt;/p&gt;

&lt;p&gt;Developed by Cheng-Jun Wang &amp;amp; Lingfei Wu&lt;/p&gt;

&lt;p&gt;Cheng-Jun Wang wangchj04@gmail.com
Lingfei Wu wlf850927@gmail.com&lt;/p&gt;

&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;scholarNetwork is a python package for crawling and visualizing the co-author network of Google Scholar.&lt;/p&gt;

&lt;p&gt;To use it, you must have access to Google Scholar, and you would also install beautifulsoup4 and networkx during the installation process.&lt;/p&gt;

&lt;h2 id=&#34;install&#34;&gt;Install&lt;/h2&gt;

&lt;p&gt;Install from pypi using pip or easy_install&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;     pip install scholarNetwork
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;or&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;     easy_install scholarNetwork
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;##Use&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;    from scholarNetwork import scholarNetwork
    import matplotlib.pyplot as plt
    import networkx as nx

    ## The seed of crawler
    seed = &#39;https://scholar.google.nl/citations?user=nNdt_G8AAAAJ&amp;amp;hl=en&amp;amp;oe=ASCII&#39;
    ## How many nodes do you want to visulize? Always start with a small one.
    Nmax = 21
    ## Get the graph g
    g = scholarNetwork.getGraph(seed, Nmax)

    ## plot the network
    pos=nx.spring_layout(g) #setup the layout

    nx.draw(g, pos, node_shape = &#39;o&#39;,
            edge_color = &#39;gray&#39;, width = 0.5,
            with_labels = True, arrows = True)
    plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://chengjun.qiniudn.com/example.png&#34; alt=&#34;1&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Of course, you can get a much larger graph to see what&amp;rsquo;s happening around you, just to change the value of Nmax. However, you have to be patient to wait for much longer time.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://chengjun.qiniudn.com/ego300large.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
