<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title></title>
    <link>https://chengjunwang.com/en/note/index.xml</link>
    <description>Recent content on </description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2016 Cheng-Jun Wang</copyright>
    <lastBuildDate>Sun, 01 Jan 2017 00:00:00 +0000</lastBuildDate>
    <atom:link href="/en/note/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title> Bit by Bit: Social Research in the Digital Age</title>
      <link>https://chengjunwang.com/en/note/2017-8-15-bitbybit/</link>
      <pubDate>Tue, 15 Aug 2017 12:00:00 +0000</pubDate>
      
      <guid>https://chengjunwang.com/en/note/2017-8-15-bitbybit/</guid>
      <description>

&lt;p&gt;authors: Matthew J. Salganik&lt;/p&gt;

&lt;h1 id=&#34;bit-by-bit-social-research-in-the-digital-age&#34;&gt;Bit by Bit: Social Research in the Digital Age&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;http://www.bitbybitbook.com/&#34; target=&#34;_blank&#34;&gt;http://www.bitbybitbook.com/&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&#34;preface&#34;&gt;Preface&lt;/h1&gt;

&lt;p&gt;For me, this book began in 2005, when I was working on my dissertation. I was running an online experiment, which I’ll tell you all about in Chapter 4, but now I’m going to tell you something that is not in any academic paper. And, it’s something that fundamentally changed how I think about research. One morning, when I checked the web-server, I discovered that overnight about 100 people from Brazil had participated in my experiment. This experience had a profound impact on me. At that time, I had friends who were running traditional lab experiments, and I knew how hard they had to work to recruit, supervise, and pay people to participate in their experiments; if they could run 10 people in a single day, that was good progress. But, with my online experiment, 100 people participated while I was sleeping. Doing your research while you are sleeping might sound too good to be true, but it isn’t. Changes in technology—specifically the transition from the analog age to the digital age—mean that we can now collect and analyze social data in new ways. This book is about doing social research in these new ways.&lt;/p&gt;

&lt;p&gt;This book is for two different communities. It is for social scientists that want to do more data science, and it is for data scientists that want to do more social science. I spend time in both of these communities, and this book is my attempt to bring their ideas together in a way that avoids the quirks and jargon of either. Given the communities that this book is for, it should go without saying that this book is not just for students and professors. I’ve worked some in government (at the US Census Bureau) and in the tech industry (at Microsoft Research), and I know that there is lots of exciting research happening outside of universities. So, if you think of what you are doing as social research, then this book is for you, no matter where you work or what kind of techniques you currently use.&lt;/p&gt;

&lt;p&gt;We are still in the early days of social research in the digital age, and I’ve seen some misunderstandings that are so fundamental and so common that it makes the most sense for me to address them here, in the preface. From data scientists, I’ve seen two common misunderstandings. The first is thinking that more data automatically solves problems. But, for social research that has not been my experience. In fact, for social research new types of data, as opposed to more of the same data, seems to be most helpful. The second misunderstanding that I’ve seen from data scientists is thinking that social science is just a bunch of fancy-talk wrapped around common sense. Of course, as a social scientist—more specifically as a sociologist—I don’t agree with that; I think that social science has a lot of to offer. Smart people have been working hard to understand human behavior for a long time, and it seems unwise to ignore the wisdom that has accumulated from this effort. My hope is that this book will offer you some of that wisdom in a way that is easy to understand.&lt;/p&gt;

&lt;p&gt;From social scientists, I’ve also seen two common misunderstandings. First, I’ve seen some people write-off the entire idea of social research using the tools of the digital age based on a few bad papers. If you are reading this book, you have probably already read a bunch of papers that uses social media data in ways that are banal or wrong (or both). I have too. However, it would be a serious mistake to conclude from these examples that all digital age social research is bad. In fact, you’ve probably also read a bunch of papers that use survey data in ways that are banal or wrong, but you don’t write-off all research using surveys. That’s because you know that there is great research done with survey data, and in this book, I’m going to show you that there is also great research done with the tools of the digital age.&lt;/p&gt;

&lt;p&gt;The second common misunderstanding that I’ve seen from social scientists is to confuse the present with the future. When assessing social research in the digital age—the research that I’m going to describe in this book—it is important to ask two distinction questions:&lt;/p&gt;

&lt;p&gt;How well does this style of research work now?
How well will this style of research work in the future as the data landscape changes and as researchers devote more attention to these problems?
Even though researchers are trained to answer the first question, for this book, I think the second question is more important. That is, even though social research in the digital age has not yet produced massive, paradigm-changing intellectual contributions, the rate of improvement of digital age research is incredibly rapid. It is this rate of change, more than the current level, that makes digital age research so exciting to me.&lt;/p&gt;

&lt;p&gt;Even though that last paragraph seemed to offer you potential riches at some unspecified time in the future, my goal in this book is not to sell you on any particular type of research. I don’t personally own shares in Twitter, Facebook, Google, Microsoft, Apple or any other tech company (although, for the sake of full disclosure, I have worked at or received research funding from Microsoft, Google, and Facebook). If you are happy with the research that you are already doing: great, keep doing what you are doing. But, if you have a sense that the digital age means that new and different things are possible, then I’d like to show you those possibilities. Thus, throughout the book my goal is to remain a credible narrator, telling you about all the exciting new stuff that is possible, while guiding you away from a few pitfalls that I’ve seen others fall into. I hope that this will help improve your research and help you better evaluate the research of others.&lt;/p&gt;

&lt;p&gt;As you might have noticed already, the tone of this book is a bit different from some other academic books. That’s intentional. This book emerged from a graduate seminar that I have taught at Princeton in the Department of Sociology, and I’d like this book to capture some of the the energy and excitement from that seminar. In particular, I want this book to have three characteristics: helpful, optimistic, and future-oriented.&lt;/p&gt;

&lt;p&gt;Helpful: My goal is to write a book that is helpful for you. Therefore, I’m going to write in an open and informal style. That’s because the most important thing that I want to convey is a certain way of thinking about social research. And, my experience from teaching suggests that the best way to convey this way of thinking is informally and with lots of examples.&lt;/p&gt;

&lt;p&gt;Optimistic: The two communities that this book engages—social scientists and data scientists—have very different styles. Data scientists are generally excited; they tend to see the glass as half full. Social scientists, on the other hand, are generally more critical; they tend to see the glass as half empty. In this book, I’m going to adopt the optimistic tone of a data scientist, even though my training is as a social scientist. So, when I present examples, I’m going to tell you what I love about these examples. And, when I do point out problems with the examples—and I will do this because no research is perfect—I’m going to try to point out these problems in a way is positive and optimistic. I’m not going to be critical for the sake of being critical. I’m going to be critical so that I can help you create more beautiful research.&lt;/p&gt;

&lt;p&gt;Future-oriented: I hope that this book will help you do social research using the digital systems that exist today and the digital systems that will be created in the future. I started doing this kind of research in 2003, and since then I’ve seen a lot of changes. I remember that when I was in graduate school people were very excited about using MySpace for social research. And, when I taught my first class on what I then called “web-based social research,” people were very excited about virtual worlds such as SecondLife. I’m sure that in the future much of what people are talking about today will seem silly and outdated. The trick to staying relevant in the face of this rapid change is abstraction. Therefore, this is not going to be a book that teaches you exactly how to use the Twitter API; instead, it is going to be a book that teaches you how to learn from digital traces (Chapter 2). This is not going to be a book that gives you step-by-step instructions for running experiments on Amazon Mechanical Turk; instead, it is going to teach you how to design and interpret experiments that rely on digital age infrastructure (Chapter 4). Through the use of abstraction, I hope this will be a timeless book on a timely topic.&lt;/p&gt;

&lt;p&gt;I think this is the most exciting time ever to be a social researcher, and I’m going to try to convey that excitement in a way that is precise. That is, it is time to move beyond vague generalities about the magical powers of new data. It is time to get specific.&lt;/p&gt;

&lt;h1 id=&#34;1-introduction&#34;&gt;1 Introduction&lt;/h1&gt;

&lt;h2 id=&#34;1-1-an-ink-blot&#34;&gt;1.1 An ink blot&lt;/h2&gt;

&lt;p&gt;In the summer of 2009, mobile phones were ringing all across Rwanda. In addition to the millions of calls between family, friends, and business associates, about 1,000 Rwandans received a call from Joshua Blumenstock and his colleagues. The researchers were studying wealth and poverty by conducting a survey of people who had been randomly sampled from a database of 1.5 million customers from Rwanda’s largest mobile phone provider. Blumenstock and colleagues asked the participants if they wanted to participate in a survey, explained the nature of the research to them, and then asked a series of questions about their demographic, social, and economic characteristics.&lt;/p&gt;

&lt;p&gt;Everything I have said up until now makes this sound like a traditional social science survey. But, what comes next is not traditional, at least not yet. They used the survey data to train a machine learning model to predict someone’s wealth from their call data, and then they used this model to estimate the wealth of all 1.5 million customers. Next, they estimated the place of residence of all 1.5 million customers by using the geographic information embedded in the call logs. Putting these two estimates together—the estimated wealth and the estimated place of residence—Blumenstock and colleagues were able to produce high-resolution estimates of the geographic distribution of wealth across Rwanda. In particular, they could produce an estimated wealth for each of Rwanda’s 2,148 cells, the smallest administrative unit in the country.&lt;/p&gt;

&lt;p&gt;It was impossible to validate these estimates because no one had ever produced estimates for such small geographic areas in Rwanda. But, when Blumenstock and colleagues aggregated their estimates to Rwanda’s 30 districts, they found that their estimates were similar to estimates from the Demographic and Health Survey, the gold standard of surveys in developing countries. Although these two approaches produced similar estimates in this case, the approach of Blumenstock and colleagues was about 10 times faster and 50 times cheaper than the traditional Demographic and Health Surveys. These dramatically faster and lower cost estimates create new possibilities for researchers, governments, and companies (Blumenstock, Cadamuro, and On 2015).&lt;/p&gt;

&lt;p&gt;In addition to developing a new methodology, this study is kind of like a Rorschach inkblot test; what people see depends on their background. Many social scientists see a new measurement tool that can be used to test theories about economic development. Many data scientists see a cool new machine learning problem. Many business people see a powerful approach for unlocking value in the digital trace data that they have already collected. Many privacy advocates see a scary reminder that we live in a time of mass surveillance. Many policy makers see a way that new technology can help create a better world. In fact, this study is all of those things, and that is why it is a window into the future of social research.&lt;/p&gt;

&lt;h2 id=&#34;1-2-welcome-to-the-digital-age&#34;&gt;1.2 Welcome to the digital age&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;The digital age is everywhere, it’s growing, and it changes what is possible for researchers.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The central premise of this book is that the digital age creates new opportunities for social research. Researchers can now observe behavior, ask questions, run experiments, and collaborate in ways that were simply impossible in the quite recent past. Along with these new opportunities also come new risks; researchers can now harm people in ways that were impossible in the quite recent past. The source of these opportunities and risks is the transition from the analog age to the digital age. This transition did not happen all at once—like a light-switch turning on—and, in fact, the transition is not yet complete. But, by this point we’ve seen enough to know that something big is happening.&lt;/p&gt;

&lt;p&gt;One way to notice this transition is to look for changes in your daily life. Many things in your life that used to be analog are now digital. Maybe you used to use a camera with film and now you use a digital camera (which is probably part of your digital phone). Maybe you used to read a physical newspaper and now you read an online newspaper. Maybe you used to pay for things with physical cash and now you pay with a credit card. In each case, the transition from analog to digital means that more information is now being captured and stored digitally.&lt;/p&gt;

&lt;p&gt;In fact, when looked at in aggregate, the effects of the transition are astonishing. The amount of information in the world is rapidly increasing and more of that information is stored digitally, which facilitates analysis, transmission, and merging (Figure 1.1) (Hilbert and López 2011). All of this digital information has come to be called “big data.” In addition to this explosion of digital data, there is a parallel growth in our access to computing power (Figure 1.1) (Hilbert and López 2011). These trends—increasing digital information and increasing computing—show no sign of slowing down.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/34696d2461b0e919e3d278b9260a278d.png&#34; alt=&#34;fig1.1&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 1.1: Information storage capacity and computing power are increasing dramatically. Further, information storage is now almost exclusively digital (Hilbert and López 2011). These changes create incredible opportunities for social researchers.&lt;/p&gt;

&lt;p&gt;For the purposes of social research, I think the most important feature of the digital age is computers everywhere. Beginning as room-sized machines that were only available to governments and big companies, computers have been constantly shrinking in size and increasing in popularity. Each decade since the 1980s, we’ve seen a new kind of computing emerge: personal computers, laptops, smart phones, and now embedded processors (i.e., computers inside of devices such as cars, watches, and thermostats) (Waldrop 2016). Increasingly these ubiquotous computers do more than just calculate; they also sense, store, and transmit information.&lt;/p&gt;

&lt;p&gt;For researchers, the implications of computers everywhere are easiest to see online, an environment that is fully measured and amenable to experimentation. For example, an online store can easily collect incredibly precise data about the shopping and purchasing patterns of millions of customers. Further, an online store can easily randomize some customers to receive one shopping experience and others to receive another. This ability to randomize on top of tracking means that online stores can constantly run randomized controlled experiments. In fact, if you’ve ever bought anything from an online store your behavior has been tracked and you’ve almost certainly been a participant in an experiment, whether you knew it or not.&lt;/p&gt;

&lt;p&gt;This fully-measured-fully-randomizable world is not just happening online; it is increasingly happening everywhere. Physical stores already collect extremely detailed purchase data, and they are developing infrastructure to monitor customers shopping behavior and mix experimentation into routine business practice. In other words, when you think about the digital age you should not just think online, you should think everywhere. Digital age social research will involve people interacting in fully digital spaces and will involve people using digital devices in the physical world.&lt;/p&gt;

&lt;p&gt;In addition to enabling the measurement of behavior and randomization of treatments, the digital age has also enabled new ways for people to communicate. These new forms of communication allow researchers to run innovative surveys and to create mass collaboration with their colleagues and the general public.&lt;/p&gt;

&lt;p&gt;A skeptic might point out that none of these capabilities are really new. That is, in the past, there have been other major advances in peoples’ abilities to communicate (e.g., the telegraph (Gleick 2011)), and computers have been getting faster at roughly the same rate since the 1960s (Waldrop 2016). But, what this skeptic is missing is that at a certain point more of the same becomes something different (Halevy, Norvig, and Pereira 2009). Here’s an analogy that I like. If you can capture an image of a horse, then you have a photograph. And, if you can capture 24 images of a horse per second, then you have a movie. Of course, a movie is just a bunch of photos, but only a die hard skeptic would claim that photos and movies are the same.&lt;/p&gt;

&lt;p&gt;Researchers are in the process of making a transition akin to the transition from photography to cinematography. This transition does not mean that everything we have learned in the past should be ignored. Just as the principles of photography inform the principles of cinematography, the principles of social research in the past will inform the social research of the future. But, the transition also means that we should not continue doing the same thing. Rather, we must combine the approaches of the past with the capabilities of the present and future. For example, the research of Blumenstock and colleagues was a mixture of traditional survey research with what some might call data science. Both of those ingredients were necessary: neither the survey responses nor the phone records by themselves were enough. More generally, I think that increasingly social researchers will need to combine social science with data science in order to take advantage of the opportunities of the digital age. To continue only taking pictures when we could also be making movies would be a mistake.&lt;/p&gt;

&lt;h2 id=&#34;1-3-research-design&#34;&gt;1.3 Research design&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Research design is about connecting questions and answers.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This book is written for two audiences that have a lot to learn from each other. On the one hand, this book is for social scientists who have training and experience studying social behavior, but who are less familiar with the opportunities created by the digital age. On the other hand, this book is for another group of researchers who are very comfortable using the tools of the digital age, but who are new to studying social behavior. This second group resists an easy name, but I will call them data scientists. These data scientists—who often have training in fields such as computer science, information science, engineering, and physics—have been some of the earliest adopters of digital age social research, in part because they had the necessary data access and computational skills. Data scientists, however, have less training and experience studying social behavior. This book brings these two communities together to produce something richer and more interesting than either community could produce individually.&lt;/p&gt;

&lt;p&gt;The best way to create this powerful hybrid is not to focus on abstract social theory or fancy machine learning. The best place to start is research design. If you think of social research as the process of asking and answering questions about human behavior, then research design is the connective tissue; research design links questions and answers. Getting this connection right is the key to producing convincing research.&lt;/p&gt;

&lt;h2 id=&#34;1-4-themes-of-this-book&#34;&gt;1.4 Themes of this book&lt;/h2&gt;

&lt;h3 id=&#34;1-4-1-readymades-and-custommades&#34;&gt;1.4.1 Readymades and Custommades&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;Social research in the digital age will involve Readymades, Custommades, and powerful hybrids.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;One of the most famous urinals ever was purchased in 1917, and that urinal is, in some ways, similar to lots of social research in the digital age. The urinal in question was purchased by the French artist Marcel Duchamp. After purchasing it, Duchamp scribbled “R. Mutt 1917” on it and then named his creation Fountain (Figure 1.2). Although its initial reception for was lukewarm, Fountain has come to considered one of the most important pieces of modern art because it fundamentally changed how people think about art (Higgins 2004). Fountain is an example of a Readymade, where an artist sees something that already exists in the world then repurposes it as art.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/9782262673cccc279d9be93631b75f7c.png&#34; alt=&#34;fig1.2&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 1.2: Fountain by Marcel Duchamp. Fountain is an example of a Readymade, where an artist sees something that already exists in the world then creatively repurposes it for art. So far, a lot of social research in the digital age has involved repurposing data that created for some purpose other than research. Photo by Alfred Stiglitz, 1917. Source: Wikimedia Commons.&lt;/p&gt;

&lt;p&gt;Much of social research in the digital age has, so far, had a similar structure, although not quite with the same result. Researchers have realized that digital records created by governments and businesses for their own purposes—such as phone logs, digitized texts, and social media data—can be repurposed for social research (Lazer 2015). In other words, a lot of social research in the digital age has been a search for Data Readymades.&lt;/p&gt;

&lt;p&gt;However, just as most artists don’t walk around looking for Readymades, most social researchers in the past have not walked around looking for data that can be repurposed. Instead, rather than being data-driven, most successful social research in the past has been question-driven. That is, a researcher had a question and then found or created the data needed to answer that question. An artist who illustrates this other style of work is Michelangelo. He wanted to make a statue of David so he spent 3 years laboring with a block of marble to create his masterpiece (Figure 1.3). David is not a Readymade; it is a Custommade.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/0fdbd65a64e560d12bc60f9e01101018.png&#34; alt=&#34;fig1.3&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 1.3: David by Michaelangelo. David is an example of art that was intentionally created; it is a Custommade. This style contrasts with Readymades such as Fountain (Figure 1.2). Social research in the digital age will involve both Readymades and Custommades. Photo by Jörg Bittner Unna, 2008. Source: Wikimedia Commons.&lt;/p&gt;

&lt;p&gt;Social research in the digital age will involve both Duchamps and Michelangelos, both Readymades and Custommades. This book will explore these two approaches, and, more importantly, it will show how they can be combined into a powerful hybrid. For example, Joshua Blumenstock and colleagues were part Duchamp and part Michelangelo; they repurposed the mobile phone call data (a Readymade) and they created their own survey data (a Custommade). This blending of Readymades and Custommades is a pattern that you’ll see throughout this book.&lt;/p&gt;

&lt;h3 id=&#34;1-4-2-simplicity-over-complexity&#34;&gt;1.4.2 Simplicity over complexity&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;Complex research can never convince anyone of something surprising. If you care about changing minds, then your research should be simple.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Social research in the digital age is often associated with complexity, such as fancy algorithms and sophisticated computing. This is unfortunate because the most convincing social research is often the simplest. To be clear, simple research is not the same as easy research. In fact, it is often much harder to create simple research.&lt;/p&gt;

&lt;p&gt;The most important reason to prefer simple research is that it is the only way to create believable, unexpected results. For example, imagine that you have just conducted some research using an incredibly complex methodology. If your results match your expectation, then you will probably accept them. But, if your results are different from what you expected, you have two options: accept the unexpected results or doubt the complex methodology. My guess is that you are much more likely to doubt the complex methodology. This makes perfect sense, but it means that the more complex the methodology, the less likely it is to produce unexpected results that you will actually believe. At some point, methods can become so complex that the only results that you can believe are those that match you expectations. At that point, the research has lost something very important: research should be able to change your mind.&lt;/p&gt;

&lt;p&gt;The problem that I’ve just described is ever more severe once you start trying to change someone else’s mind. Imagine presenting an incredibly complex piece of research that has an unexpected result to someone else. That other person has not spent months writing your code and working through your data so when they are faced with the choice of accepting the unexpected result or doubting the complex methodology, they are almost certainly going to doubt the complex methodology. If you care about convincing someone else to change their mind, then your research needs to be simple.&lt;/p&gt;

&lt;p&gt;Simple research comes from a natural fit between question and data; in other words, good research design. Poor research design, however, leads to the ugly complexity that come from stretching your data to a question for which they are not well suited. This book focuses on two approaches to create a natural fit between question and data. First, this book will help you ask realistic questions of your data. Second, this book will help you collect the right data to answer your question.&lt;/p&gt;

&lt;h3 id=&#34;1-4-3-ethics-everywhere&#34;&gt;1.4.3 Ethics everywhere&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;In the future, researchers will struggle less with what can be done and more with what should be done.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In the past, cost has been a dominant constraint on what researchers do. But, as you will see throughout this book, the cost of certain forms of research is plummeting. At essentially no cost, researchers can now secretly observe the behavior of millions of people, and can conduct massive experiments without the consent or even awareness of participants. In the future, therefore, researchers will struggle less with what can be done and more with what should be done. Chapter 6 will be entirely devoted to ethics, but I also integrate ethics into the other chapters as well. In the digital age, ethics will become an increasingly important consideration when researchers balance the trade-offs between various research approaches.&lt;/p&gt;

&lt;h2 id=&#34;1-5-outline-of-the-book&#34;&gt;1.5 Outline of the book&lt;/h2&gt;

&lt;p&gt;This book is organized around a progression through four broad research approaches: observing behavior, asking questions, running experiments, and creating mass collaboration. These four approaches were all used in some form 50 years ago, and I’m confident that they will all be used in some form 50 years from now. I’ve dedicated one chapter to each approach. Many of the chapters have a section devoted to further commentary, a technical or historical appendix, and activities that could be used in a class or for self-study. Because of these features, I’m going to keep the main text as simple as possible; you can refer to these other parts of the chapters if you would like more details and citations into the literature.&lt;/p&gt;

&lt;p&gt;In chapter 2 (Observing behavior), I will describe what and how researchers can learn from observing people’s behavior. In particular, I’ll focus on digital trace data and administrative data where the researcher had no role in the creation of the data. I’ll describe common features of this kind of data, and I’ll explain some research strategies that can be used to successfully learn from observed behavior.&lt;/p&gt;

&lt;p&gt;In chapter 3 (Asking questions), I will begin by showing what researchers can learn by moving beyond observing behavior and start interacting with people. In particular, I will argue that there is great value in doing survey research, even in a world awash with already existing digital data. I will review the traditional total survey error framework and use it to organize the developments that the digital age enables for survey research. In particular, I will show how the digital age can lead to big changes in sampling and interviewing. Finally, I’ll describe two strategies for combining survey data with digital trace data. Despite the pessimism that some survey researchers currently feel, I expect that the digital age will be the golden age of survey research.&lt;/p&gt;

&lt;p&gt;In chapter 4 (Running experiments), I will begin by showing what researchers can learn when they move beyond observing behavior and asking survey questions. In particular, I will show how randomized controlled experiments—where the researcher intervenes in the world in a very specific way—enable researchers to learn about causal relationships. I will compare the kinds of experiments that we could do in the past with the kinds that we can do now. With that background, I’ll describe the trade-offs involved in the two main strategies for conducting digital experiments. Finally, I’ll conclude with some design advice about how you can take advantage of the real power of digital experiments and describe some of responsibility that comes with that power.&lt;/p&gt;

&lt;p&gt;In chapter 5 (Creating mass collaboration), I will show how researchers can create mass collaborations—such as crowdsourcing and citizen science—in order to do social research. By describing successful mass collaboration projects and by providing a few key organizing principles, I hope to convince you of two things: first, that mass collaboration can be harnessed for social research, and second, that researchers who use mass collaboration will be able to solve problems that had previously seemed impossible. Although mass collaboration is often promoted as a way to save money, it is much more than that. Mass collaboration doesn’t just allow us to do research cheaper; it allows us to do research better.&lt;/p&gt;

&lt;p&gt;In chapter 6 (Ethics), I will argue that researchers have rapidly increasing power over participants, and that these capabilities are changing faster than our norms, rules, and laws. This combination—increasing power and lack of agreement about how that power should be used—leaves well-meaning researchers in a difficult situation. To address this problem, I will argue that researchers should adopt a principles-based approach. That is, researchers should evaluate their research through existing rules—which I will take as given—and through more general ethical principles. I’ll propose four established principles and two ethical frameworks that can help guide your decisions. Finally, I’ll describe and analyze some specific ethical challenges that I expect will confront researchers in the future, and I’ll offer practical tips for working in an area with unsettled ethics.&lt;/p&gt;

&lt;p&gt;Finally, in chapter 7 (The future), I will summarize three themes that recur across chapters and that will be particularly important in the future.&lt;/p&gt;

&lt;p&gt;Social research in the digital age will combine what we have done in the past with the very different capabilities of the future. Thus, social research will be shaped by both social scientists and data scientists. Each group has something to contribute, and each group has something to learn.&lt;/p&gt;

&lt;h2 id=&#34;2-1-introduction&#34;&gt;2.1 Introduction&lt;/h2&gt;

&lt;p&gt;In the analog age, collecting data about behavior—who does what when—was expensive, and therefore, relatively rare. Now, in the digital age, the behaviors of billions of people are recorded, stored, and analyzable. For example, every time you click on a website, make a call on your cell phone, or pay for something with your credit card, a digital record of your behavior is created and stored by a business. Because these data are a by-product of people’s every day actions, they are often called digital traces. In addition to these traces held by businesses, governments also have incredibly rich data about both people and businesses, data which is often digitized and analyzable. Together these business and government records are often called big data.&lt;/p&gt;

&lt;p&gt;The ever-rising flood of big data means that we have moved from a world where behavioral data was scarce to a world where behavioral data is plentiful. But, because these types data are relatively new, an unfortunate amount of research using them looks like scientists blindly chasing available data. This chapter, instead, offers a principled approach to understanding the different sources of data and how they can be used. This richer understanding should help you better match your research questions to appropriate sources of data. Or, if such existing sources are lacking, convince you to collect your own data using the ideas in future chapters.&lt;/p&gt;

&lt;p&gt;A first step to learning from big data is to realize that it is part of a broader category of data that has been used for social research for many years: observational data. Roughly, observational data is any data that results from observing a social system without intervening in some way. A crude way to think about it is that observational data is everything that does not involve talking with people (e.g., surveys, the topic of Chapter 3) or changing people’s environments (e.g., experiments, the topic of Chapter 4). Thus, in addition to business and government records, observational data also includes things like the text of newspaper articles and satellite photos.&lt;/p&gt;

&lt;p&gt;This chapter has three parts. First, in Section 2.2, I describe big data in more detail and clarify a fundamental difference between it and the data that have generally been used for social research in the past. Then, in Section 2.3, I describe ten common characteristics of big data sources. Understanding these characteristics enables us to quickly recognize the strengths and weaknesses of existing sources and will help us harness the new sources that will be created in the future. Finally, in Section 2.4, I describe three main research strategies that you can use to learn from observational data: counting things, forecasting things, and approximating an experiment.&lt;/p&gt;

&lt;h2 id=&#34;2-2-big-data&#34;&gt;2.2 Big data&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Big data are created and collected by governments for purposes other than research. Using this data for research, therefore, requires repurposing.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;An idealized view of social research imagines a scientist having an idea and then collecting data to test that idea. This style of research leads to a tight fit between research question and data, but it is limited because an individual researcher often do not have the resources needed to collect the data they need, such as large, rich, and nationally-representative data. Therefore, a lot of social research in the past has used large-scale social surveys, such as the General Social Survey (GSS), the American National Election Study (ANES), and Panel Study of Income Dynamics (PSID). These large-scale survey are generally run by a team of researchers and they are designed to create data that can be used by many researchers. Because of the goals of these large-scale surveys, great care is put into designing the data collection and preparing the resulting data for use by researchers. These data are by researchers and for researchers.&lt;/p&gt;

&lt;p&gt;Most social research using digital age sources, however, is fundamentally different. Instead of using data collected by researchers and for researchers, it uses data sources that were created and collected by businesses and governments for their own purposes such as making a profit, providing a service, or administering a law. These business and government data sources have come to be called big data. Doing research with big data is different than doing research with data that was originally created for research. Compare, for example, a social media website, such as Twitter, with a traditional public opinion survey such as the General Social Survey (GSS). Twitter’s main goals are to provide a service to its users and to make a profit. In the process of achieving these goals, Twitter creates data that might be useful for studying certain aspects of public opinion. But, unlike the General Social Survey (GSS), Twitter is not primarily focused on social research.&lt;/p&gt;

&lt;p&gt;The term big data is frustratingly vague, and it groups together many different things. For the purposes of social research, I think it is helpful to distinguish between two kinds of big data sources: government administrative records and business administrative records. Government administrative records are data that are created by governments as part of their routine activities. These kinds of records have been used by researchers in the past—such as demographers studying birth, marriage, and death records—but governments are increasingly collecting and releasing detailed records in analyzable forms. For example, the New York City government installed digital meters inside of every taxi in the city. These meters record all kinds of data about each taxi ride including the driver, the start time and location, the stop time and location, and the fare. In a study that I’ll tell later in this chapter, Henry Farber (2015) repurposed these data to address a fundamental debate in labor economics about the relationship between hourly wages and the number of hours worked.&lt;/p&gt;

&lt;p&gt;The second main type of big data for social research is business administrative records. These are data that business create and collect as part of their routine activities. These business administrative records are often called digital traces, and include things like search engine query logs, social media posts, and call records from mobile phones. Critically, these business administrative records are not just about online behavior. For example, stores that use check-out scanners are creating real-time measures of worker productivity. In a study that I’ll tell you about later in this chapter, Alexandre Mas and Enrico Moretti (2009) repurposed this supermarket check-out data to study how a workers’ productivity is impacted by the productivity of their peers.&lt;/p&gt;

&lt;p&gt;As both of these examples illustrate, the idea of repurposing is fundamental to learning from big data. In my experience, social scientists and data scientists approach to this repurposing very differently. Social scientists, who are accustomed to working with data designed for research, are quick to point out the problems with repurposed data while ignoring its strengths. On the other hand, data scientists are quick to point out the benefits of repurposed data while ignoring its weaknesses. Naturally, the best approach would be a hybrid. That is, researchers need to understand the characteristics of these new sources of data—both good and bad—and then figure out how to learn from them. And, that is the plan for the remainder of this chapter. Next, I will describe ten common characteristics of business and government administrative data&lt;/p&gt;

&lt;h2 id=&#34;2-3-common-characteristics-of-big-data&#34;&gt;2.3 Common characteristics of big data&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Big data sources tend to have ten characteristics; some are good for social research and some are bad.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;If researchers are going to learn from big data that they did not create or collect, then they must understand its general characteristics. Rather than taking a platform by platform approach (e.g., here’s what you need to know about Twitter, here’s what you need to know about Google search data, etc), I’m going to describe ten general characteristics of big data, characteristics that arise because the data was not created for the purpose of social research. By stepping back from the details of each particular system and looking at these general properties, researchers can quickly learn more about existing data sources and have a firm set of ideas to apply to future data sources.&lt;/p&gt;

&lt;p&gt;I find it helpful to group the characteristics into two categories:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;generally good for research: big, always-on, non-reactive&lt;/li&gt;
&lt;li&gt;generally bad for research: incomplete, inaccessible, non-representative, drifting, algorithmically confounded, inaccessible, dirty, and sensitive&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Broadly speaking, government administrative records are less non-representative, less algorithmically confounded, and less drifting. On the other hand, business administrative records tend to be larger and more always-on.&lt;/p&gt;

&lt;h3 id=&#34;2-3-1-characteristics-that-are-generally-good-for-research&#34;&gt;2.3.1 Characteristics that are generally good for research&lt;/h3&gt;

&lt;h4 id=&#34;2-3-1-1-big&#34;&gt;2.3.1.1 Big&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;Large datasets are a means to an end; they are not an end in themselves.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The first of the three good characteristics of big data is the most discussed: these are big data. These data sources can be big in three different ways: many people, lots of information per person, or many observations over time. Having a big dataset enables some specific types of research—measuring heterogeneity, studying rare events, detecting small differences, and making causal estimates from observational data. It also seems to lead to a specific type of sloppiness.&lt;/p&gt;

&lt;p&gt;The first thing for which size is particularly useful is moving beyond averages to make estimates for specific subgroups. For example, Gary King, Jennifer Pan, and Molly Roberts (2013) measured the probability that social media posts in China would be censored by the government. By itself this average probability of deletion is not very helpful for understanding why the government censors some posts but not others. But, because their dataset included 11 million posts, King and colleagues also produced estimates for the probability of censorship for posts on 85 separate categories (e.g., pornography, Tibet, and Traffic in Beijing). By comparing the probability of censorship for posts in different categories, they were able to understand more about how and why the government censors certain types of posts. With 11 thousand posts (rather than 11 million posts), they would not have been able to produce these category-specific estimates.&lt;/p&gt;

&lt;p&gt;Second, size is particularly useful for is studying of rare events. For example, Goel and colleagues (2015) wanted to study the different ways that tweets can go viral. Because large cascades of re-tweets are extremely rare—about one in a 3,000—they needed to study more than a billion tweets in order to find enough large cascades for their analysis.&lt;/p&gt;

&lt;p&gt;Third, large datasets enable researchers to detect small differences. In fact, much of the focus on big data in industry is about these small differences: reliably detecting the difference between 1% and 1.1% click-through rates on an ad can translate into millions of dollars in extra revenue. In some scientific settings, such small differences might not be particular important (even if they are statistically significant). But, in some policy settings, such small differences can become important when viewed in aggregate. For example, if there are two public health interventions and one is slightly more effective than the other, then switching to the more effective intervention could end up saving thousands of additional lives.&lt;/p&gt;

&lt;p&gt;Finally, large data sets greatly increase our ability to make causal estimates from observational data. Although large datasets don’t fundamentally change the problems with making causal inference from observational data, matching and natural experiments—two techniques that researchers have developed for making causal claims from observational data—both greatly benefit from large datasets. I’ll explain and illustrate this claim in greater detail later in this chapter when I describe research strategies.&lt;/p&gt;

&lt;p&gt;Although bigness is generally a good property when used correctly, I’ve noticed that bigness commonly leads to a conceptual error. For some reason, bigness seems to lead researchers to ignore how their data was generated. While bigness does reduce the need to worry about random error, it actually increases the need to worry about systematic errors, the kinds of errors that I’ll describe in more below that arise from biases in how data are created and collected. In a small dataset, both random error and systematic error can be important, but in a large dataset random error is can be averaged away and systematic error dominates. Researchers who don’t think about systematic error will end up using their large datasets to get a precise estimate of the wrong thing; they will be precisely inaccurate (McFarland and McFarland 2015).&lt;/p&gt;

&lt;h4 id=&#34;2-3-1-2-always-on&#34;&gt;2.3.1.2 Always-on&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;Always-on big data enables the study of unexpected events and real-time measurement.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Many big data systems are always-on; they are constantly collecting data. This always-on characteristic provides researchers with longitudinal data (i.e., data over time). Being always-on has two important implications for research.&lt;/p&gt;

&lt;p&gt;First, always-on data collection enables researchers to study unexpected events in ways that were not possible previously. For example, researchers interested in studying the Occupy Gezi protests in Turkey in the summer of 2013 would typically focus on the behavior of protesters during the event. Ceren Budak and Duncan Watts (2015) were able to do more by using the always-on nature of Twitter to study Twitter-using protesters before, during, and after the event. And, they were able to create a comparison group of non-participants (or participants who did not tweet about the protest) before, during, and after the event (Figure 2.1). In total their ex-post panel included the tweets of 30,000 people over two years. By augmenting the commonly used data from the protests with this other information, Budak and Watts were able to learn much more: they were able to estimate what kinds of people were more likely to participate in the Gezi protests and to estimate the changes in attitudes of participants and non-participants, both in the short-term (comparing pre-Gezi to during Gezi) and in the long-term (comparing pre-Gezi to post-Gezi).&lt;/p&gt;

&lt;p&gt;Figure 2.1: Design used by Budak and Watts (2015) to study the Occupy Gezi protests in Turkey in the summer of 2013. By using the always-on nature of Twitter, the researchers created what they called an ex-post panel that included about 30,000 people over two years. In contrast the typical study that focused on participants during the protests, the ex-post panel adds 1) data from participants before and after the event and 2) data from non-participants before, during, and after the event. This enriched data structure enabled Budak and Watts to estimate what kinds of people were more likely to participate in the Gezi protests and to estimate the changes in attitudes of participants and non-participants, both in the short-term (comparing pre-Gezi to during Gezi) and in the long-term (comparing pre-Gezi to post-Gezi).
Figure 2.1: Design used by Budak and Watts (2015) to study the Occupy Gezi protests in Turkey in the summer of 2013. By using the always-on nature of Twitter, the researchers created what they called an ex-post panel that included about 30,000 people over two years. In contrast the typical study that focused on participants during the protests, the ex-post panel adds 1) data from participants before and after the event and 2) data from non-participants before, during, and after the event. This enriched data structure enabled Budak and Watts to estimate what kinds of people were more likely to participate in the Gezi protests and to estimate the changes in attitudes of participants and non-participants, both in the short-term (comparing pre-Gezi to during Gezi) and in the long-term (comparing pre-Gezi to post-Gezi).&lt;/p&gt;

&lt;p&gt;It is true that some of these estimates could have been made without always-on data collection sources (e.g., long-term estimates of attitude change), although such data collection for 30,000 people would have been quite expensive. And, even given an unlimited budget, I can’t think of any other method that essentially allows researchers to travel back in time and directly observe participants behavior in the past. The closest alternative would be to collect retrospective reports of behavior, but these reports would be of limited granularity and questionable accuracy. Table 2.1 provides other examples of studies that use an always-on data source to study an unexpected event.&lt;/p&gt;

&lt;p&gt;Table 2.1: Studies of unexpected events using always-on big data sources.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Unexpected event&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Always-on data source&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Citation&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Occupy Gezi movement in Turkey&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Twitter&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Budak and Watts (2015)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Umbrella protests in Hong Kong&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Weibo&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Zhang (2016)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Shootings of police in New York City&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Stop-and-frisk&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;reports Legewie (2016)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Person joining ISIS&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Twitter&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Magdy, Darwish, and Weber (2016)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;September 11, 2001 attack&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;livejournal.com&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Cohn, Mehl, and Pennebaker (2004)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;September 11, 2001 attack&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;pager messages&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Back, Küfner, and Egloff (2010), Pury (2011), Back, Küfner, and Egloff (2011)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Second, always-on data collection enables researchers to produce real-time measurements, which can be important in settings where policy makers want to not just learn from existing behavior but also respond to it. For example, social media data can be used to guide responses to natural disasters (Castillo 2016).&lt;/p&gt;

&lt;p&gt;In conclusion, always-on data systems enable researchers to study unexpected events and provide real-time information to policy makers. I did not, however, propose that that always-on data systems enable researchers to track changes over long periods of time. That is because many big data systems are constantly changing—a process called drift (Section 2.3.2.4).&lt;/p&gt;

&lt;h4 id=&#34;2-3-1-3-non-reactive&#34;&gt;2.3.1.3 Non-reactive&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;Measurement is much less likely to change behavior in big data sources.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;One challenge of social research is that people can change their behavior when they know that they are being observed by researchers. Social scientists generally call this behavior change in response to researcher measurement reactivity (Webb et al. 1966). One aspect of big data that many researcher find promising is that participants are generally not aware that their data are being captured or they have become so accustomed to this data collection that it no longer changes their behavior. Because they are non-reactive, therefore, many sources of big data can be used to study behavior that has not been amendable to accurate measurement previously. For example, Stephens-Davidowitz (2014) used the prevalence of racist terms in search engine queries to measure racial animus in different regions of the United States. The non-reactive and big (see previous section) nature of search data enabled measurements that would be difficult using other methods, such as surveys.&lt;/p&gt;

&lt;p&gt;Non-reactivity, however, does not ensure that these data are somehow a direct reflect of people’s behavior or attitudes. For example, as one respondent told Newman et al. (2011), “It’s not that I don’t have problems, I’m just not putting them on Facebook.” In other words, even though some big data sources are non-reactive, they are not always free of social desirability bias, the tendency for people to want to present themselves in the best possible way. Further, as I’ll describe more below, these data sources are sometimes impacted by the goals of platform owners, a problem called algorithmic confounding (described more below).&lt;/p&gt;

&lt;p&gt;Although non-reactivity is advantageous for research, tracking people’s behavior without their consent and awareness raises ethical concerns discussed below and in detail in Chapter 6. A public backlash against increased digital surveillance could lead big data systems to become more reactive over time, and strong concern about digital surveillance could even lead some people to attempt to opt-out of big data systems completely, increasing concerns about non-representativity (described more below).&lt;/p&gt;

&lt;p&gt;These three good properties of big data for social research—big, always-on, and non-reactive—generally arise because these data sources were not created by researchers for research. Now, I’ll turn to the seven properties of big data sources that are bad for research. These features also tend to arise because this data was not created by researchers for research.&lt;/p&gt;

&lt;h3 id=&#34;2-3-2-characteristics-that-are-generally-bad-for-research&#34;&gt;2.3.2 Characteristics that are generally bad for research&lt;/h3&gt;

&lt;h4 id=&#34;2-3-2-1-incomplete&#34;&gt;2.3.2.1 Incomplete&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;No matter how “big” your “big data” it probably doesn’t have the information you want.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Most big data sources are incomplete, in the sense that they don’t have the information that you will want for your research. This is a common feature of data that were created for purposes other than research. Many social scientists have already had the experience of dealing with the incompleteness, such as an existing survey that didn’t ask the question you wanted. Unfortunately, the problems of incompleteness tend to be more extreme in big data. In my experience, big data tends to be missing three types of information useful for social research: demographics, behavior on other platforms, and data to operationalize theoretical constructs.&lt;/p&gt;

&lt;p&gt;All three of these forms of incompleteness are illustrated in a study by Gueorgi Kossinets and Duncan Watts (2006) about the evolution of the social network at a university. Kossinets and Watts started with the email logs from the university, which had precise information about who sent emails to whom at what time (the researchers did not have access to the content of the emails). These email records sound like an amazing dataset, but, they are—despite their size and granularity—fundamentally incomplete. For example, the email logs do not include data about the demographic characteristics of the students, such as gender and age. Further, the email logs do not include information about communication through other media, such as phone calls, text message, or face-to-face conversations. Finally, the email logs do not directly include information about relationships, the theoretical constructs in many existing theories. Later in the chapter, when I talk about research strategies, you’ll see how Kossinets and Watts solved these problems.&lt;/p&gt;

&lt;p&gt;Of three kinds of incompleteness, the problem of incomplete data to operationalize theoretical constructs is the hardest to solve, and in my experience, it is often accidentally overlooked by data scientists. Roughly, theoretical constructs are abstract ideas that social scientists study, but, unfortunately, these constructs cannot always be unambiguously defined and measured. For example, let’s imagine trying to empirically test the apparently simple claim that people who are more intelligent earn more money. In order to test this claim you would need to measure “intelligence.” But, what is intelligence? For example, Gardner (2011) argued that there are actually eight different forms of intelligence. And, are there procedures that could accurately measure any of these forms of intelligence? Despite enormous amounts of work by psychologists, these questions still don’t have unambiguous answers. Thus, even a relatively simple claim—people who are more intelligent earn more money—can be hard to assess empirically because it can be hard to operationalize theoretical constructs in data. Other examples of theoretical constructs that are important but hard to operationalize include “norms,” “social capital,” and “democracy.” Social scientists call the match between theoretical constructs and data construct validity (Cronbach and Meehl 1955). And, as this list of constructs suggests, construct validity is a problem that social scientists have struggled with for a very long time, even when they were working with data that was collected for the purpose of research. When working with data collected for purposes other than research, the problems of construct validity are even more challenging (Lazer 2015).&lt;/p&gt;

&lt;p&gt;When you are reading a research paper, one quick and useful way to assess concerns about construct validity is to take the main claim in the paper, which is usually expressed in terms of constructs, and re-express it in terms of the data used. For example, consider two hypothetical studies that claim to show that more intelligent people earn more money:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Study 1: people who score well on the Raven Progressive Matrices Test—a well studied test of analytic intelligence (Carpenter, Just, and Shell 1990)—have higher reported incomes on their tax returns&lt;/li&gt;
&lt;li&gt;Study 2: people on Twitter who used longer words are more likely to mention luxury brands&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In both cases, researchers could assert that they have shown that more intelligent people earn more money. But, in the first study the theoretical constructs are well operationalized by the data, and in the second they are not. Further, as this example illustrates, more data does not automatically solve problems with construct validity. You should doubt the results of Study 2 whether it involved a million tweets, a billion tweets, or a trillion tweets. For researchers not familiar with the idea of construct validity, Table 2.2 provides some examples of studies that have operationalized theoretical constructs using digital trace data.&lt;/p&gt;

&lt;p&gt;Table 2.2: Examples of digital traces that are used as measures of more abstract theoretical concepts. Social scientists call this match construct validity and it is a major challenge with using big data sources for social research (Lazer 2015).&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Digital trace&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Theoretical construct&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Citation&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;email logs from a university (meta-data only)&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Social relationships&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Kossinets and Watts (2006), Kossinets and Watts (2009), De Choudhury et al. (2010)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;social media posts on Weibo&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Civic engagement&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Zhang (2016)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;email logs from a firm (meta-data and complete text)&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Cultural fit in an organization&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Goldberg et al. (2015)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Although the problem of incomplete data for operationalizing theoretical constructs is pretty hard to solve, there are three common solutions to the problem of incomplete demographic information and incomplete information on behavior on other platforms. The first is to actually collect the data you need; I’ll tell you about an example of that in Chapter 3 when I tell you about surveys. Unfortunately, this kind of data collection is not always possible. The second main solution is to do what data scientists call user-attribute inference and what social scientists call imputation. In this approach, researchers use the information that they have on some people to infer attributes of other people. The third possible solution—the one used by Kossinets and Watts—was to combine multiple data sources. This process is sometimes called merging or record linkage. My favorite metaphor for this process was proposed in the very first paragraph of the very first paper ever written on record linkage (Dunn 1946):&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;“Each person in the world creates a Book of Life. This Book starts with birth and ends with death. Its pages are made up of records of the principle events in life. Record linkage is the name given to the process of assembling the pages of this book into a volume.”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This passage was written in 1946, and at that time, people were thinking that the Book of Life could include major life events like birth, marriage, divorce, and death. However, now that so much information about people is recorded, the Book of Life could be an incredibly detailed portrait, if those different pages (i.e., our digital traces), can be bound together. This Book of Life could be a great resource for researchers. But, the Book of Life could also be called a database of ruin (Ohm 2010), which could be used for all kinds of unethical purposes, as described more below when I talk about the sensitive nature of the information collected by big data sources below and in Chapter 6 (Ethics).&lt;/p&gt;

&lt;h4 id=&#34;2-3-2-2-inaccessible&#34;&gt;2.3.2.2 Inaccessible&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;Data held by businesses and governments are difficult for researchers to access.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In May 2014, the US National Security Agenda opened a data center in rural Utah that has an awkward name, the Intelligence Community Comprehensive National Cybersecurity Initiative Data Center. However, this data center, which has come to be known as the Utah Data Center, is reported to have astounding capabilities. One report alleges that the Utah Data Center is able to store and process all forms of communication including “the complete contents of private emails, cell phone calls, and Google searches, as well as all sorts of personal data trails—parking receipts, travel itineraries, bookstore purchases, and other digital `pocket litter’” (Bamford 2012). In addition to the raising concerns about the sensitive nature of much of the information captured in big data, which will be described more below, the Utah Data Center is an extreme example of a rich data source that is inaccessible to researchers. More generally, many sources of big data that would be useful to researchers are controlled and restricted by governments (e.g., tax data and educational data) and companies (e.g., queries to search engines and phone call meta-data). Therefore, these data will not be immediately available to researchers at universities, and most will not even be available to researchers in the governments or companies.&lt;/p&gt;

&lt;p&gt;In my experience, many researchers based at universities misunderstand the source of this inaccessibility. These data are not inaccessible because people at companies and governments are stupid, lazy, or uncaring. Rather, there are serious legal, technical, business, and ethical barriers that prevent data access. For example, some terms-of-service agreements for websites only allow data to be used by employees or to improve the service. So certain forms of data sharing could expose companies to legitimate lawsuits from customers. There are also substantial business risks to companies involved in sharing data. Try to imagine how the public would respond if personal search data accidentally leaked out from Google as part of a university research project. Such a data breach, if extreme, might even be an existential risk for the company. So Google—and most large companies—are very risk-averse about sharing data with researchers.&lt;/p&gt;

&lt;p&gt;In fact, almost everyone who is in a position to provide access to large amounts of data knows the story of Abdur Chowdhury. In 2006, when he was the head of AOL research, he intentionally released what he thought were anonymized search queries from 650,000 AOL users to the research community. As far as I can tell, Chowdhury and the researchers at AOL had good intentions and they thought that they had anonymized the data. But, they were wrong. It was quickly discovered that the data were not as anonymous as the researchers thought, and reporters from the New York Times were able to identify people in the dataset with ease (Barbaro and Zeller Jr 2006). Once these problems were discovered, Chowdhury removed the data from AOL’s website, but it was too late. The data had been reposted on other websites, and it will probably still be available when you are reading this book. Because of his attempt to share data with the research community, Chowdhury was fired, and AOL’s chief technology officer resigned (Hafner 2006). As this example shows, the benefits for specific individuals inside of companies to facilitate data access are pretty small and the worst-case scenario is terrible.&lt;/p&gt;

&lt;p&gt;Research can, however, gain access to data that is inaccessible to the general public. Governments have procedures that researchers can follow to apply for access, and as the examples later in this chapter show, researchers can occasionally gain access to corporate data. For example, Einav et al. (2015) partnered with a researcher at eBay to study the digital traces from online auctions. I’ll talk more about the research that came from this collaboration later in the chapter (Section 2.4.3.2), but I mention it now because it had all four of the ingredients that I see in successful partnerships: researcher interest, researcher capability, company interest, and company capability. In other words, Einav and colleagues were interested in and capable of studying online auctions. And, eBay was also. However, I’ve seen many possible collaboration fail because either the researcher or company lacked one of these ingredients.&lt;/p&gt;

&lt;p&gt;Even if you are able to develop a partnership with a business, however, there are some downsides for you. First, the questions that you can ask with the data with likely be limited; companies are unlikely to allow research that could make them look bad. Second, you will probably not be able to share your data with other researchers, which means that other researchers will not be able to verify and extend your results. Further, these partnerships can create at least the appearance of a conflict of interest, where people might think that your results were influenced by your partnerships. All of these downsides can be addressed, but it is important to be clear that working with data that is not accessible to everyone had both upsides and downsides.&lt;/p&gt;

&lt;p&gt;In summary, lots of big data is inaccessible to researchers. There are serious legal, technical, business, and ethical barriers that prevent data access, and these barriers will not go away. National governments generally have established procedures for enabling data access, but the process can be more ad hoc at the state and local levels. Also, in some cases, researchers can partner with companies to obtain data access, but this can create a variety of problems for researchers.&lt;/p&gt;

&lt;h4 id=&#34;2-3-2-3-non-representative&#34;&gt;2.3.2.3 Non-representative&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;Two sources of non-representativeness are different populations and different usage patterns.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Big data tend to be systematically biased in two main ways. This need not cause a problem for all kind of analysis, but for some analysis it can be a critical flaw.&lt;/p&gt;

&lt;p&gt;A first source of systematic bias is that the people captured are typically neither a complete universe of all people or a random sample from any specific population. For example, Americans on Twitter are not a random sample of Americans (Hargittai 2015). A second source of systematic bias is that many big data systems capture actions, and some people contribute many more actions than others. For example, some people on Twitter contribute hundreds of times more tweets than others. Therefore, the events on a specific platform can be ever more heavily reflective of certain subgroups than the platform itself.&lt;/p&gt;

&lt;p&gt;Normally researchers want to know a lot about the data that they have. But, given the non-representative nature of big data, it is helpful to also flip your thinking. You also need to know a lot about the data that you don’t have. This is especially true when the data that you don’t have are systematically different from the data that you do have. For example, if you have the call records from a mobile phone company in a developing countries, you should think not just about the people in your dataset, but also about the people who might be too poor to own a mobile phone. Further, in Chapter 3, we’ll learn about how weighting can enable researchers to make better estimates from non-representative data.&lt;/p&gt;

&lt;h4 id=&#34;2-3-2-4-drifting&#34;&gt;2.3.2.4 Drifting&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;Population drift, usage drift, and system drift make it hard to use big data source to study long-term trends.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;One of the great advantages of many big data sources are that they collect data over time. Social scientists call this kind of over-time data, longitudinal data. And, naturally, longitudinal data are very important for studying change. In order to reliably measure change, however, the measurement system itself must be stable. In the words of sociologist Otis Dudley Duncan, “if you want to measure change, don’t change the measure” (Fischer 2011).&lt;/p&gt;

&lt;p&gt;Unfortunately, many big data systems—especially business system that create and capture digital traces—are changing all the time, a process that I’ll call drift. In particular, these systems change in three main ways: population drift (change in who is using them), behavioral drift (change in how people are using them), and system drift (change in the system itself). The three sources of drift mean that any pattern in digital trace data could be caused by an important change in the world, or it could be caused by some form of drift.&lt;/p&gt;

&lt;p&gt;The first source of drift—population drift—is who is using the system, and this changes on long-time scales and short-time scales. For example, from 2008 to present the average age of people on social media has increased. In addition to these long-term trends, the people using a system at any moment varies. For example, during the US Presidential election of 2012 the proportion of tweets about politics that were written by women fluctuated from day to day (Diaz et al. 2016). Thus, what might appear to be a change in the mood of the Twitter-verse might actually just be changes in who is talking at any moment.&lt;/p&gt;

&lt;p&gt;In addition to changes in who is using a system, there are also changes in how the system is used. For example, during the Occupy Gezi Park protests in Istanbul, Turkey in 2013 protesters changed their use of hashtags as the protest evolved. Here’s how Zeynep Tufekci (2014) described the drift, which she was able to detect because she was observing behavior on Twitter and on the ground:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;“What had happened was that as soon as the protest became the dominant story, large numbers of people &amp;hellip; stopped using the hashtags except to draw attention to a new phenomenon &amp;hellip;. While the protests continued, and even intensified, the hashtags died down. Interviews revealed two reasons for this. First, once everyone knew the topic, the hashtag was at once superfluous and wasteful on the character-limited Twitter platform. Second, hashtags were seen only as useful for attracting attention to a particular topic, not for talking about it.”
Thus, researchers who were studying the protests by analyzing tweets with protest-related hashtags would have a distorted sense of what was happening because of this behavioral drift. For example, they might believe that the discussion of the protest decreased long before it actually decreased.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The third kind of drift is system drift. In this case, it is not the people changing or their behavior changing, but the system itself changing. For example, over time Facebook has increased the limit on the length of status updates. Thus, any longitudinal study of status updates will be vulnerable to artifacts caused by this change. System drift is closely related to problem called algorithmic confounding to which we now turn.&lt;/p&gt;

&lt;h4 id=&#34;2-3-2-5-algorithmically-confounded&#34;&gt;2.3.2.5 Algorithmically confounded&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;Behavior in found data is not natural, it is driven by the engineering goals of the systems.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Although many found data sources are non-reactive because people are not aware their data are being recorded (Section 2.3.1.3), researchers should not consider behavior in these online systems to be “naturally occurring” or “pure.” In reality, the digital systems that record behavior are highly engineered to induce specific behaviors such as clicking on ads or posting content. The ways that the goals of system designers can introduce patterns into data is called algorithmic confounding. Algorithmic confounding is relatively unknown to social scientists, but it is a major concern among careful data scientists. And, unlike some of the other problems with digital traces, algorithmic confounding is largely invisible.&lt;/p&gt;

&lt;p&gt;A relatively simple example of algorithmic confounding is the fact that on Facebook there are an anomalously high number of users with approximately 20 friends (Ugander et al. 2011). Scientists analyzing with this data without any understanding of how Facebook works could doubtlessly generate many stories about how 20 is some kind of magical social number. However, Ugander and his colleagues had a substantial understanding of the process that generated the data, and they knew that Facebook encouraged people with few connections on Facebook to make more friends until they reached 20 friends. Although Ugander and colleagues don’t say this in the paper, this policy was presumably created by Facebook in order to encourage new users to become more active. Without knowing about the existence of this policy, however, it is easy to draw the wrong conclusion from the data. In other words, the surprisingly high number of people with about 20 friends tells us more about Facebook than human behavior.&lt;/p&gt;

&lt;p&gt;More pernicious than this previous example where algorithmic confounding produced a quirky result that a careful researchers might investigate further, there is an even trickier version of algorithmic confounding that occurs when designers of online systems are aware of social theories and then bake these theories into the working of their systems. Social scientists call this performativity: when theories change the world in such a way that they bring the world more into line with the theory. In the cases of performative algorithmic confounding, the confounded nature of the data is likely invisible.&lt;/p&gt;

&lt;p&gt;One example of a pattern created by performativity is transitivity in online social networks. In the 1970s and 1980s, researchers repeatedly found that if you are friends with Alice and you are friends with Bob, then Bob and Alice are more likely to be friends with each other than two randomly chosen people. And, this very same pattern was found in the social graph on Facebook (Ugander et al. 2011). Thus, one might conclude that patterns of friendship on Facebook replicate patterns of offline friendships, at least in terms of transitivity. However, the magnitude of transitivity in the Facebook social graph is partially driven by algorithmic confounding. That is, data scientists at Facebook knew of the empirical and theoretical research about transitivity and then baked it into how Facebook works. Facebook has a “People You May Know” feature that suggests new friends, and one way that Facebook decides who to suggest to you is transitivity. That is, Facebook is more likely to suggest that you become friends with the friends of your friends. This feature thus has the effect of increasing transitivity in the Facebook social graph; in other words, the theory of transitivity brings the world into line with the predictions of the theory (Healy 2015). Thus, when big data sources appears to reproduce predictions of social theory, we must be sure that the theory itself was not baked into how the system worked.&lt;/p&gt;

&lt;p&gt;Rather than thinking of big data sources as observing people in a natural setting, a more apt metaphor is observing people in a casino. Casinos are highly engineered environments designed to induce certain behaviors, and a researchers would never expect that behavior in a casino would provide an unfettered window into human behavior. Of course, we could learn something about human behavior studying people in casinos—in fact a casino might be an ideal setting for studying the relationship between alcohol consumption and risk preferences—but if we ignored that the data was being created in a casino we might draw some bad conclusions.&lt;/p&gt;

&lt;p&gt;Unfortunately, dealing with algorithmic confounding is particularly difficult because many features of online systems are proprietary, poorly documented, and constantly changing. For example, as I’ll explain later in this chapter, algorithmic confounding was one possible explanation for the gradual break-down of Google Flu Trends (Section 2.4.2), but this claim was hard to assess because the inner workings of Google’s search algorithm are proprietary. The dynamic nature of algorithmic confounding is one form of system drift. Algorithmic confounding means that we should be cautious about any claim for human behavior that comes from a single digital system, no matter how big.&lt;/p&gt;

&lt;h4 id=&#34;2-3-2-6-dirty&#34;&gt;2.3.2.6 Dirty&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;Big data sources can be loaded with junk and spam.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Some researchers believe that big data sources, especially those from online sources, are pristine because they are collected automatically. In fact, people who have worked with big data sources know that they are frequently dirty. That is, they frequently include data that do not reflect real actions of interest to researchers. Many social scientists are already familiar with the process of cleaning large-scale social survey data, but cleaning big data sources is more difficult for two reasons: 1) they were not created by researchers for researchers and 2) researchers generally have less understanding of how they were created.&lt;/p&gt;

&lt;p&gt;The dangers of dirty digital trace data are illustrated by Back and colleagues’ (2010) study of the emotional response to the attacks of September 11, 2001. Researchers typically study the response to tragic events using retrospective data collected over months or even years. But, Back and colleagues found an always-on source of digital traces—the timestamped, automatically recorded messages from 85,000 American pagers—and this enabled the researchers to study emotional response on a much finer timescale. Back and colleagues created a minute-by-minute emotional timeline of September 11th by coding the emotional content of the pager messages by the percentage of words related to (1) sadness (e.g., crying, grief), (2) anxiety (e.g., worried, fearful), and (3) anger (e.g., hate, critical). They found that sadness and anxiety fluctuated throughout the day without a strong pattern, but that there was a striking increase in anger throughout the day. This research seems to be a wonderful illustration of the power of always-on data sources: using standard methods it would be impossible to have such a high-resolution timeline of the immediate response to an unexpected event.&lt;/p&gt;

&lt;p&gt;Just one year later, however, Cynthia Pury (2011) looked at the data more carefully. She discovered that a large number of the supposedly angry messages were generated by a single pager and they were all identical. Here’s what those supposedly angry messages said:&lt;/p&gt;

&lt;p&gt;“Reboot NT machine [name] in cabinet [name] at [location]:CRITICAL:[date and time]”
These messages were labeled angry because they included the word “CRITICAL”, which may generally indicate anger but does not in this case. Removing the messages generated by this single automated pager completely eliminates the apparent increase in anger over the course of the day (Figure 2.2). In other words, the main result in Back, Küfner, and Egloff (2010) was an artifact of one pager. As this example illustrates, relatively simple analysis of relatively complex and messy data has the potential to go seriously wrong.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/548d989d34644c5cd7daf88a9257a729.png&#34; alt=&#34;fig2.2&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 2.2: Estimated trends in anger over the course of September 11, 2001 based on 85,000 American pagers (Back, Küfner, and Egloff 2010; Pury 2011; Back, Küfner, and Egloff 2011). Originally, Back, Küfner, and Egloff (2010) reported a pattern of increasing anger throughout the day. However, most of these apparent angry messages were generated by a single pager that repeatedly sent out the following message: Reboot NT machine [name] in cabinet [name] at [location]:CRITICAL:[date and time]. With this message removed, the apparent increase in anger disappears (Pury 2011; Back, Küfner, and Egloff 2011). This figure is a reproduction of Fig 1B in Pury (2011).&lt;/p&gt;

&lt;p&gt;While dirty data that is created unintentionally—such as from one noisy pager—can be detected by a reasonably careful researcher, there are also some online systems that attract intentional spammers. These spammers actively generate fake data, and—often motivated by profit—work very hard to keep their spamming concealed. For example, political activity on Twitter seems to include at least some reasonably sophisticated spam, whereby some political causes are intentionally made to look more popular than they actual are (Ratkiewicz et al. 2011). Researchers working with data that may contain intentional spam face the challenge of convincing their audience that they have detected and removed relevant spam.&lt;/p&gt;

&lt;p&gt;Finally, what is considered dirty data can depend in subtle ways on your research questions. For example, many edits to Wikipedia are created by automated bots (Geiger 2014). If you are interested in the ecology of Wikipedia, then these bots are important. But, if you are interested in how humans contribute to Wikipedia, these edits made by these bots should be excluded.&lt;/p&gt;

&lt;p&gt;The best ways to avoid being fooled by dirty data are to understand how your data were created to perform simple exploratory analysis, such as making simple scatter plots.&lt;/p&gt;

&lt;h4 id=&#34;2-3-2-7-sensitive&#34;&gt;2.3.2.7 Sensitive&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;Some of the information that companies and governments have is sensitive.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Health insurance companies have detailed information about the medical care received by their customers. This information could be used for important research about health, but if it became public it could potentially lead to emotional harm (e.g., embarrassment) and economic harm (e.g., loss of employment). Far from distinctive, many big data sources have information that is sensitive. The sensitive nature of this information is part of the reason that big data sources are often inaccessible (described above).&lt;/p&gt;

&lt;p&gt;One way that researchers attempt to deal with this situation is to de-identify datasets that have sensitive information. But, as I will show in detail in Chapter 6 (Ethics) this approach seriously limited in ways that are not widely appreciated by both social scientists and data scientists.&lt;/p&gt;

&lt;p&gt;In conclusion, the big data sources of today (and tomorrow) generally have ten characteristics. Many of the good properties—big, always-on, and nonreactive—come from the fact in the digital age companies and governments are able to collect data at a scale that was not possible previously. And, many of the bad properties—incomplete, inaccessible, non-representative, drifting, algorithmically confounded, inaccessible, dirty, and sensitive—come from the fact that the data is not collected by researchers for researchers. Understanding these characteristics are a necessary first step to learning from big data. And, now we turn to research strategies we can use with this data.&lt;/p&gt;

&lt;h2 id=&#34;2-4-research-strategies&#34;&gt;2.4 Research strategies&lt;/h2&gt;

&lt;p&gt;Given these ten characteristics of big data sources and the inherent limitations of even perfectly observed data, what kind of research strategies are useful? That is, how can we learn when we don’t ask questions and don’t run experiments? It might seem that just watching people could not lead to interesting research, but that’s not the case.&lt;/p&gt;

&lt;p&gt;I see three main strategies for learning from observational data: counting things, forecasting things, and approximating experiments. I’ll describe each of these approaches—which could be called “research strategies” or “research recipes”—and I’ll illustrate them with examples. These strategies are neither mutually exclusive or exhaustive, but they do capture a lot of research with observational data.&lt;/p&gt;

&lt;p&gt;To foreshadow the claims that follow, counting things is most important when we are empirically adjudicating between predictions from different theories. Forecasting, and especially nowcasting, can be useful for policy makers. Finally, big data increases our ability to make causal estimates from observational data.&lt;/p&gt;

&lt;h4 id=&#34;2-4-1-counting-things&#34;&gt;2.4.1 Counting things&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;Simple counting can be interesting if you combine a good question with good data.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Although it is couched in sophisticated sounding language, lots of social research is really just counting things. In the age of big data, researchers can count more than ever, but that does not automatically mean that research should be focused on counting more and more stuff. Instead, if we are going to do good research with big data, we need to ask: what things are worth counting? This may seem like an entirely subjective matter, but there are some general patterns.&lt;/p&gt;

&lt;p&gt;Often students motivate their counting research by saying: I’m going to count something that no one has ever counted before. For example, a student might say, many people have studied migrants and many people have studied twins, but nobody has studied migrant twins. Motivation by absence does not usually lead to good research. Of course, there might be good reasons to study migrant twins, but the fact that they have not been studied before does not mean that they should be studied now. No one has ever counted the number of threads on the carpet in my office, but that does not automatically imply that this would be a good research project. Motivation by absence is kind of like saying: look, there’s a hole over there, and I’m going to work very hard to fill it up. But, not every hole needs to be filled.&lt;/p&gt;

&lt;p&gt;Instead of motivating by absence, I think that counting leads to good research in two situations, when the research is interesting or important (or ideally both). For example, measuring the rate of unemployment is important because it is an indicator of the economy that drives policy decisions. Generally, people have a pretty good sense of what is important. So, in the rest of this section, I’m going to provide three examples where counting is interesting. In each case, the researchers were not counting haphazardly, rather they were counting in very particular settings that revealed important insights into more general ideas about how social systems work. In other words, a lot of what makes these particular counting exercises interesting is not in the data itself, it comes from these more general ideas.&lt;/p&gt;

&lt;p&gt;Below I’ll present three examples on: 1) the working behavior of taxi drivers in New York (Section 2.4.1.1), 2) friendship formation by students (Section 2.4.1.2) and 3) social media censorship behavior of the Chinese government (Section 2.4.1.3). What these examples share is that they all show that counting big data can be used to test theoretical predictions. In some cases, big data sources enable you to do this counting relatively directly (as in the case of New York Taxis). In other cases, researchers will need to deal with incompleteness by merging data together and operationalizing theoretical constructs (as in the case of friendship formation); and in some cases researchers will need to collect their own observational data (as in the case of social media censorship). As I hope these examples show, for researchers who are able to ask interesting questions, big data holds great promise.&lt;/p&gt;

&lt;h4 id=&#34;2-4-1-1-taxis-in-new-york-city&#34;&gt;2.4.1.1 Taxis in New York City&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;A researcher used big data from taxi meters to study decision-making of taxi drivers in New York. These data was well suited for this research.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;One example of the simple power of counting the right thing comes from Henry Farber’s (2015) study of the behavior of New York City taxi drivers. Although this group might not sound inherently interesting it is a strategic research site for testing two competing theories in labor economics. For the purposes of Farber’s research, there are two important features about the work environment of taxi drivers: 1) their hourly wage fluctuates from day-to-day, based in part on factors like the weather and 2) the number of hours they work can fluctuate each day based on the driver’s decisions. These features lead to an interesting question about the relationship between hourly wages and hours worked. Neoclassical models in economics predict that taxi drivers would work more on days where they have higher hourly wages. Alternatively, models from behavioral economics predict exactly the opposite. If drivers set a particular income target—say \$100 per day—and work until that target is met, then drivers would end up working fewer hours on days that they are earning more. For example, if you were a target earner, you might end up working 4 hours on a good day (\$25 per hour) and 5 hours on a bad day (\$20 per hour). So, do drivers work more hours on days with higher hourly wages (as predicted by the neoclassical models) or more hours on days with lower hourly wages (as predicted by behavioral economic models)?&lt;/p&gt;

&lt;p&gt;To answer this question Farber obtained data on every taxi trip taken by New York City cabs from 2009 - 2013, data that are now publicly available. This data—which was collected by electronic meters that the city requires taxis to use—includes several pieces of information for each trip: start time, start location, end time, end location, fare, and tip (if the tip was paid with a credit card). In total, Farber’s data contained information on approximately 900 million trips taken during approximately 40 million shifts (a shift is roughly one day’s work for one driver). In fact, there was so much data, that Farber only used a random sample of it for his analysis. Using this taxi meter data, Farber found that most drivers work more on days when wages are higher, consistent with the neoclassical theory. In addition to this main finding, Farber was able to leverage the size of the data for a better understanding of heterogeneity and dynamics. Farber found that over time newer drivers gradually learn to work more hours on high wage days (e.g., they learn to behave as the neoclassical models predicts). And, new drivers who behave more like target earners are more likely to quit being a taxi driver. Both of these more subtle findings, which help explain the observed behavior of current drivers, were only possible because of the size of the dataset. They would have been impossible to detect in earlier studies that used paper trip sheets from a small number of taxi drivers over a short period of time (e.g., Camerer et al. (1997)).&lt;/p&gt;

&lt;p&gt;Farber’s study was close to a best-case for a study using big data. First, the data were not non-representative because the city required drivers to use digital meters. And, the data were not incomplete because the data that was collected by the city was pretty close to the data that Farber would have collected if he had the choice (one difference is that Farber would have wanted data on total wages—fares plus tips—but the city data only included tips paid by credit card). The key to Farber’s research was combining a good question with good data. The data alone are not enough.&lt;/p&gt;

&lt;h4 id=&#34;2-4-1-2-friendship-formation-among-students&#34;&gt;2.4.1.2 Friendship formation among students&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;Researchers used email logs and administrative records to understand friendship formation. This research requires dealing with the incompleteness of big data.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In many situations, researchers are not lucky enough to have everything that they want automatically collected in one place. Two common problems are incomplete information about the people and a mismatch between theoretical constructs and data. Both of these problems were addressed by Kossinets and Watts (2009) as part of their efforts to understand how social networks evolve.&lt;/p&gt;

&lt;p&gt;Roughly speaking, researchers think that social network evolution is driven by three features: 1) the structure of existing relationships 2) shared activities (e.g., dorms, classes) and 3) demographics. Understanding the interrelationships between these three factors requires longitudinal network data combined with information about individuals’ demographics and activities. Earlier studies had some of these features, but none had all three.&lt;/p&gt;

&lt;p&gt;Kossinets and Watts started their research by acquiring the email logs from a large university. However, these email logs alone were incomplete, they don’t include everything needed to understand the various factors driving network evolution. Therefore, Kossinets and Watts merged these email logs, with two other sources of information: demographic information collected by the university and information about shared activities (e.g., student residence information and a complete list of enrollment in courses). Once these three sources of information, each of which was incomplete, were merged together Kossinets and Watts had a powerful data structure for understanding network evolution.&lt;/p&gt;

&lt;p&gt;But, there was one final challenge that they had to overcome. Kossinets and Watts wanted to study how the social network in this university evolved so they needed a way to use the email logs into an estimate of who was connected to who at which time. As discussed in previously (Section 2.3.2.1), this kind of operationalization of theoretical constructs is a big challenge when using digital traces for social research. In the end, Kossinets and Watts decided that two people were considered connected at time  t if and only if they had exchanged emails (i emailed j and j emailed i) in the previous 60 days. These choices were not arbitrary; they were based on careful consideration of this empirical setting, and Kossinets and Watts checked that their results were robust to these choices. In general, if your operationalization involves choosing some specific cutoffs—say 60 days instead of 30 days or 90 days—it is a good idea to make sure that your results are not sensitive to this choice.&lt;/p&gt;

&lt;p&gt;Once Kossinets and Watts addressed the problem caused by incompleteness (e.g., missing demographic information, missing information about shared activity, and missing theoretical constructs), they had data that enabled them to understand the three main forces that can drive network evolution: 1) the structure of existing relationships 2) shared activities (e.g., dorms, classes) and 3) demographics. Consistent with earlier research, they found that people with similar demographics are more likely to form relationships. However, unlike earlier studies, they found that this pattern was strongly mitigated by the existing network structure and shared activities. In other words, the pattern that earlier researchers had seen was partially explained by data that earlier researchers did not have. Thus, by successfully dealing with the incompleteness of their data, Kossinets and Watts were able clarify the interaction of a variety of different factors that drive social network evolutions.&lt;/p&gt;

&lt;h4 id=&#34;2-4-1-3-censorship-of-social-media-by-the-chinese-government&#34;&gt;2.4.1.3 Censorship of social media by the Chinese government&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;Researchers scraped Chinese social media sites to study censorship. They dealt with incompleteness with latent-trait inference.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In addition to the big data used in the two previous examples, researchers can also collect their own observational data, as was wonderfully illustrated by Gary King, Jennifer Pan, and Molly Roberts’ (2013) research on censorship by the Chinese government.&lt;/p&gt;

&lt;p&gt;Social media posts in China are censored by an enormous state apparatus that is thought to include tens of thousands of people. Researchers and citizens, however, have little sense of how these censors decide what content should be deleted from the social media. Scholars of China actually have conflicting expectations about which kinds of posts are most likely to get deleted. Some think that censors focus on posts that are critical of the state while others think they focus on posts that encourage collective behavior, such as protests. Figuring out which of these expectations is correct has implications for how researchers understand China and other authoritarian governments that engage in censorship. Therefore, King and colleagues wanted to compare posts that were published and subsequently deleted to posts that were published and never deleted.&lt;/p&gt;

&lt;p&gt;Collecting these posts involved the amazing engineering feat of crawling more than 1,000 Chinese social media websites—each with different page layouts—finding relevant posts, and then revisiting these posts to see which were subsequently deleted. In addition to the normal engineering problems associated with large scale web-crawling, this project had the added challenge that it needed to be extremely fast because many censored posts are taken down in less than 24 hours. In other words, a slow crawler would miss lots of posts that were censored. Further, the crawlers had to do all this data collection while evading detection lest the social media websites block access or otherwise change their policies in response to the study.&lt;/p&gt;

&lt;p&gt;Once this massive engineering task was completed, King and colleagues had obtained about 11 million posts on 85 different topics that were pre-specified based on their expected level of sensitivity. For example, a topic of high sensitivity is Ai Weiwei, the dissident artist; a topic of middle sensitivity is appreciation and devaluation of the Chinese currency, and a topic of low sensitivity is the World Cup. Of these 11 million posts about 2 million had been censored, but posts on highly sensitive topics were censored only slightly more often than posts on middle and low sensitivity topics. In other words, Chinese censors are about as likely to censor a post that mentions Ai Weiwei as a post that mentions the World Cup. These findings did not match the simplistic idea that the government censors all posts on sensitive topics.&lt;/p&gt;

&lt;p&gt;This simple calculation of censorship rate by topic could be misleading, however. For example, the government might censor posts that are supportive of Ai Weiwei, but leave posts that are critical of him. In order to distinguish between posts more carefully, the researchers need to measure the sentiment of each post. Thus, one way to think about it is that the sentiment of each post in an important latent feature of each post. Unfortunately, despite much work, fully automated methods of sentiment detection using pre-existing dictionaries are still not very good in many situations (think back to the problems creating an emotional timeline of September 11, 2001 from Section 2.3.2.6). Therefore, King and colleagues needed a way to label their 11 million social media posts as to whether they were 1) critical of the state, 2) supportive of the state, or 3) irrelevant or factual reports about the events. This sounds like a massive job, but they solved it using a powerful trick; one that is common in data science but currently relatively rare in social science.&lt;/p&gt;

&lt;p&gt;First, in a step typically called pre-processing, the researchers converted the social media posts into a document-term matrix, where there was one row for each document and one column that recorded whether the post contained a specific word (e.g., protest, traffic, etc.). Next, a group of research assistants hand-labeled the sentiment of a sample of post. Then, King and colleagues used this hand-labeled data to estimate a machine learning model that could infer the sentiment of a post based on its characteristics. Finally, they used this machine learning model to estimate the sentiment of all 11 million posts. Thus, rather than manually reading and labeling 11 million posts (which would be logistically impossible), they manually labeled a small number of posts and then used what data scientists would call supervised learning to estimate the categories of all the posts. After completing this analysis, King and colleagues were able to conclude that, somewhat surprisingly, the probability of a post being deleted was unrelated to whether it was critical of the state or supportive of the state.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/e5af8b87bb9c85f434d8e1335cc4f954.png&#34; alt=&#34;fig2.3&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 2.3: Simplified schematic for the procedure used in King, Pan, and Roberts (2013) to estimating the sentiment of 11 million Chinese social media posts. First, in a step typically called pre-processing, the researchers converted the social media posts into a document-term matrix (see Grimmer and Stewart (2013) for more information). Second, the researchers hand-coded the sentiment of a small sample of posts. Third, the researchers trained a supervised learning model to classify the sentiment of posts. Fourth, the researchers used the supervised learning model to estimate the sentiment of all the posts. See King, Pan, and Roberts (2013), Appendix B for a more detailed description.&lt;/p&gt;

&lt;p&gt;In the end, King and colleagues discovered that only three types of posts were regularly censored: pornography, criticism of censors, and those that had collective action potential (i.e., the possibility of leading to large-scale protests). By observing a huge number of posts that were deleted and posts that were not deleted, King and colleagues were able to learn how the censors work just by watching and counting. In subsequent research, they actually directly intervened into the Chinese social media ecosystem by creating posts with systematically different content and measuring which get censored (King, Pan, and Roberts 2014). We will learn more about experimental approaches in Chapter 4. Further, foreshadowing a theme that will occur throughout the book, these latent-attribute inference problems—which can sometimes be solved with supervised learning—turn out to be very common in social research in the digital age. You will see pictures very similar to Figure 2.3 in Chapters 3 (Asking questions) and 5 (Creating mass collaboration); it is one of the few ideas that appears in multiple chapters.&lt;/p&gt;

&lt;p&gt;All three of these examples—the working behavior of taxi drivers in New York, friendship formation by students, and social media censorship behavior of the Chinese government—show that relatively simple counting of observational data can enable researchers to test theoretical predictions. In some cases, big data enables you to do this counting relatively directly (as in the case of New York Taxis). In other cases, researchers will need to collect their own observational data (as in the case of Chinese censorship); deal with incompleteness by merging data together (as in the case of network evolution); or performing some form of latent-trait inference (as in the case of Chinese censorship). As I hope these examples show, for researchers who are able to ask interesting questions, big holds great promise.&lt;/p&gt;

&lt;h4 id=&#34;2-4-2-forecasting-and-nowcasting&#34;&gt;2.4.2 Forecasting and nowcasting&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;Predicting the future is hard, but predicting the present is easier.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The second main strategy used by researchers with observational data is forecasting. Predicting the future is notoriously difficult, but it can be incredibly important for decision makers, whether they work in companies or governments.&lt;/p&gt;

&lt;p&gt;Kleinberg et al. (2015) offers two stories that clarify the importance of forecasting for certain policy problems. Imagine one policy maker, I’ll call her Anna, who is facing a drought and must decide whether to hire a shaman to do a rain dance to increase the chance of rain. Another policy maker, I’ll call him Bob, must decide whether to take an umbrella to work to avoid getting wet on the way home. Both Anna and Bob can make a better decision if they understand weather, but they need to know different things. Anna needs to understand whether the rain dance causes rain. Bob, on the other hand, does not need to understanding anything about causality; he just needs an accurate forecast. Social researchers often focus on what Kleinberg et al. (2015) call “rain dance–like” policy problems—those that focus on causality—and ignore “umbrella-like” policy problems that are focused on forecasting.&lt;/p&gt;

&lt;p&gt;I’d like to focus, however, on a special kind of forecasting called nowcasting—a term derived from combining “now” and “forecasting.” Rather than predicting the future, nowcasting attempts to predict the present (Choi and Varian 2012). In other words, nowcasting uses forecasting methods for problems of measurement. As such, it should be especially useful to governments who require timely and accurate measures about their countries. Nowcasting can be illustrated most clearly with the example of Google Flu Trends.&lt;/p&gt;

&lt;p&gt;Imagine that you are feeling a bit under the weather so you type “flu remedies” into a search engine, receive a page of links in response, and then follow one of them to a helpful webpage. Now imagine this activity being played out from the perspective of the search engine. Every moment, millions of queries are arriving from around the world, and this stream of queries—what Battelle (2006) has called the “database of intentions”— provides a constantly updated window into the collective global consciousness. However, turning this stream of information into a measurement of the prevalence of the flu is difficult. Simply counting up the number of queries for “flu remedies” might not work well. Not everyone who has the flu searches for flu remedies and not everyone who searchers for flu remedies has the flu.&lt;/p&gt;

&lt;p&gt;The important and clever trick behind Google Flu Trends was to turn a measurement problem into a forecasting problem. The U.S. Centers for Disease Control and Prevention (CDC) has an influenza monitoring system that collects information from doctors around the country. However, one problem with this CDC system is there is a two week reporting lag; the time it takes for the data arriving from doctors to be cleaned, processed, and published. But, when handling an emerging epidemic, public health offices don’t want to know how much influenza there was two weeks ago; they want to know how much influenza there is right now. In fact, in many other traditional sources of social data, there are gaps between waves of data collection and reporting lags. Most big data sources, on the other hand, are always-on (Section 2.3.1.2).&lt;/p&gt;

&lt;p&gt;Therefore, Jeremy Ginsberg and colleagues (2009) tried to predict the CDC flu data from the Google search data. This is an example of “predicting the present” because the researchers were trying to measure how much flu there is now by predicting future data from the CDC, future data that is measuring the present. Using machine learning, they searched through 50 million different search terms to see which are most predictive of the CDC flu data. Ultimately, they found a set of 45 different queries that seemed to be most predictive, and the results were quite good: they could use the search data to predict the CDC data. Based in part on this paper, which was published in Nature, Google Flu Trends became an often repeated success story about the power of big data.&lt;/p&gt;

&lt;p&gt;There are two important caveats to this apparent success, however, and understanding these caveats will help you evaluate and do forecasting and nowcasting. First, the performance of Google Flu Trends was actually not much better than a simple model that estimates the amount of flu based on a linear extrapolation from the two most recent measurements of flu prevalence (Goel et al. 2010). And, over some time periods Google Flu Trends was actually worse than this simple approach (Lazer et al. 2014). In other words, Google Flu Trends with all its data, machine learning, and powerful computing did not dramatically outperform a simple and easier to understand heuristic. This suggests that when evaluating any forecast or nowcast it is important to compare against a baseline.&lt;/p&gt;

&lt;p&gt;The second important caveat about Google Flu Trends is that its ability to predict the CDC flu data was prone to short-term failure and long-term decay because of drift and algorithmic confounding. For example, during the 2009 Swine Flu outbreak Google Flu Trends dramatically over-estimated the amount of influenza, probably because people tend to change their search behavior in response to widespread fear of a global pandemic (Cook et al. 2011; Olson et al. 2013). In addition to these short-term problems, the performance gradually decayed over time. Diagnosing the reasons for this long term decay are difficult because the Google search algorithms are proprietary, but it appears that in 2011 Google made changes that would suggest related search terms when people search for symptoms like “fever” and “cough” (it also seem that this feature is no longer active). Adding this feature is a totally reasonable thing to do if you are running a search engine business, and it had the effect of generating more health related searches. This was probably a success for the business, but it caused Google Flu Trends to over-estimate flu prevalence (Lazer et al. 2014).&lt;/p&gt;

&lt;p&gt;Fortunately, these problems with Google Flu Trends are fixable. In fact, using more careful methods, Lazer et al. (2014) and Yang, Santillana, and Kou (2015) were able to get better results. Going forward, I expect that nowcasting studies that combine big data with researcher collected data—that combine Duchamp-style Readymades with Michaelangelo-style Custommades—will enable policy makers to produce faster and more accurate measurements of the present and predictions of the future.&lt;/p&gt;

&lt;h3 id=&#34;2-4-3-approximating-experiments&#34;&gt;2.4.3 Approximating experiments&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;We can approximate experiments that we can’t do. Two approaches that especially benefit from the digital age are matching and natural experiments.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Many important scientific and policy questions are causal. Let’s consider, for example, the following question: what is the effect of a job training program on wages? One way to answer this question would be with a randomized controlled experiment where workers were randomly assigned to either receive training or not receive training. Then, researchers could estimate the effect of training for these participants by simply comparing the wages of people who received the training to those that did not receive it.&lt;/p&gt;

&lt;p&gt;The simple comparison is valid because of something that happens before the data was even collected: the randomization. Without randomization, the problem is much trickier. A researcher could compare the wages of people who voluntarily signed up for training to those who didn’t sign-up. That comparison would probably show that people who received training earned more, but how much of this is because of training and how much of this is because people that sign-up for training are different from those that don’t sign-up for training? In other words, is it fair to compare the wages of these two groups of people?&lt;/p&gt;

&lt;p&gt;This concern about fair comparisons leads some researchers to believe that it is impossible to make causal estimates without running an experiment. This claim goes too far. While it is true that experiments provide the strongest evidence for causal effects, there are other strategies that can provide valuable causal estimates. Instead of thinking that causal estimates are either easy (in the case of experiments) or impossible (in the case of passively observed data), it is better to think of the strategies for making causal estimates lying along a continuum from strongest to weakest (Figure 2.4). At the strongest end of the continuum are randomized controlled experiments. But, these are often difficult to do in social research because many treatments require unrealistic amounts of cooperation from governments or companies; quite simply there are many experiments that we cannot do. I will devote all of Chapter 4 to both the strengths and weaknesses of randomized controlled experiments, and I’ll argue that in some cases, there are strong ethical reasons to prefer observational to experimental methods.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/6bd9485a59999ed235e72a8b3ea38e85.png&#34; alt=&#34;fig2.4&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 2.4: Continuum of research strategies for estimated causal effects.&lt;/p&gt;

&lt;p&gt;Moving along the continuum, there are situations where researchers have not explicitly randomized. That is, researchers are attempting to learn experiment-like knowledge without actually doing an experiment; naturally, this is going to be tricky, but big data greatly improves our ability to make causal estimates in these situations.&lt;/p&gt;

&lt;p&gt;Sometimes there are settings where randomness in the world happens to create something like an experiment for researchers. These designs are called natural experiments, and they will be considered in detail in Section 2.4.3.1. Two features of big data sources—their always-on nature and their size—greatly enhances our ability to learn from natural experiments when they occur.&lt;/p&gt;

&lt;p&gt;Moving further away from randomized controlled experiments, sometimes there is not even an event in nature that we can use to approximate a natural experiment. In these settings, we can carefully construct comparisons within non-experimental data in an attempt to approximate an experiment. These designs are called matching, and they will be considered in detail in Section 2.4.3.2. Like natural experiments, matching is a design that also benefits from big data sources. In particular, the massive size—both in terms of number of cases and type of information per case—greatly facilitates matching. The key difference between natural experiments and matching is that in natural experiments the researcher knows the process through which treatment was assigned and believes it to be random.&lt;/p&gt;

&lt;p&gt;The concept of fair comparisons that motivated the desires to do experiments also underlies the two alternative approaches: natural experiments and matching. These approaches will enable you to estimate causal effects from passively observed data by discovering fair comparisons sitting inside of the data that you already have.&lt;/p&gt;

&lt;h4 id=&#34;2-4-3-1-natural-experiments&#34;&gt;2.4.3.1 Natural experiments&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;Natural experiments take advantage of random events in the world. random event + always-on data system = natural experiment&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The key to randomized controlled experiments enabling fair comparison is the randomization. However, occasionally something happens in the world that essentially assigns people randomly or nearly randomly to different treatments. One of clearest examples of the strategy of using natural experiments comes from the research of Angrist (1990) that measures the effect of military services on earnings.&lt;/p&gt;

&lt;p&gt;During the war in Vietnam, the United States increased the size of its armed forces through a draft. In order to decide which citizens would be called into service, the US government held a lottery. Every birthdate was represented on a piece of paper, and these papers were placed in a large glass jar. As shown in Figure 2.5, these slips of paper were drawn from the jar one at a time to determine the order that young men would be called to serve (young women were not subject to the draft). Based on the results, men born on September 14 were called first, men born on April 24 were called second, and so on. Ultimately, in this lottery, men born on 195 different days were called to service while men born on 171 days were not called.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/c55e0c91fe69f3958341962913e0cbe5.png&#34; alt=&#34;fig2.5&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 2.5: Congressman Alexander Pirnie (R-NY) drawing the first capsule for the Selective Service draft on December 1, 1969. Joshua Angrist (1990) combined the draft lottery with earnings data from the Social Security Administration to estimate the effect of military service on earnings. This is an example of research using a natural experiment. Source: Wikimedia Commons&lt;/p&gt;

&lt;p&gt;Although it might not be immediately apparent, a draft lottery has a critical similarity to a randomized controlled experiment: in both situations participants are randomly assigned to receive a treatment. In the case of the draft lottery, if we are interested in learning about the effects of draft-eligibility and military service on subsequent labor market earnings, we can compare outcomes for people whose birthdates were below the lottery cutoff (e.g., September 14, April 24, etc.) with the outcomes for people whose birthdays were after the cutoff (e.g., February 20, December 2, etc.).&lt;/p&gt;

&lt;p&gt;Given that this treatment of being drafted has been randomly assigned, we can then measure the effect of this treatment for any outcome that has been measured. For example, Angrist (1990) combined the information about who was randomly selected in the draft with earnings data that was collected by the Social Security Administration to conclude that the earnings of white veterans were about 15% less than the earnings of comparable non-veterans. Other researchers have used a similar trick as well. For example, Conley and Heerwig (2011) combined the information about who was randomly selected in the draft with household data collected from the 2000 Census and 2005 American Community Survey and found that so long after the draft, there was little long-term effect of military service on variety of outcomes such as housing tenure (owning versus renting) and residential stability (likelihood of having moved in previous five years).&lt;/p&gt;

&lt;p&gt;As this example illustrates, sometimes social, political, or natural forces create experiments or near-experiments that can be leveraged by researchers. Often natural experiments are the best way to estimate cause-and-effect relationships in settings where it is not ethical or practical to run randomized controlled experiments. They are an important strategy for discovering fair comparisons in non-experimental data. This research strategy can be summarized by this equation:&lt;/p&gt;

&lt;p&gt;random (or as if random) event+always-on data stream=natural experiment(2.1)&lt;/p&gt;

&lt;p&gt;However, the analysis of natural experiments can be quite tricky. For example, in the case of the Vietnam draft, not everyone who was draft-eligible ended up serving (there were a variety of exemptions). And, at the same time, some people who were not draft-eligible volunteered for service. It was as if in a clinical trial of a new drug, some people in the treatment group did not take their medicine and some of the people in the control group somehow received the drug. This problem, called two-sided noncompliance, as well as many other problems are described in greater detail in some of the recommended readings at the end of this chapter.&lt;/p&gt;

&lt;p&gt;The strategy of taking advantage of naturally occurring random assignment precedes the digital age, but the prevalence of big data makes this strategy much easier to use. Once you realize some treatment has been assigned randomly, big data sources can provide the outcome data that you need in order to compare the results for people in the treatment and control conditions. For example, in his study of the effects of the draft and military service, Angrist made use of earnings records from the Social Security Administration; without this outcome data, his study would not have been possible. In this case, the Social Security Administration is the always-on big data source. As more and more automatically collected data sources exist, we will have more outcome data that can measure the effects of changes created by exogenous variation.&lt;/p&gt;

&lt;p&gt;To illustrate this strategy in the digital age, let’s consider Mas and Moretti’s (2009) elegant research on the effect of peers on productivity. Although on the surface it might look different than Angrist’s study about the effects of the Vietnam Draft, in structure they both follow the pattern in eq. 2.1.&lt;/p&gt;

&lt;p&gt;Mas and Moretti measured how peers affect the productivity of workers. On the one hand, having a hard working peer might lead workers to increase their productivity because of peer pressure. Or, on the other hand, a hard working peer might lead other workers to slack off even more. The clearest way to study peer effects on productivity would be a randomized controlled experiment where workers are randomly assigned to shifts with workers of different productivity levels and then resulting productivity is measured for everyone. Researchers, however, do not control the schedule of workers in any real business, and so Mas and Moretti had to rely on a natural experiment which took place in a supermarket.&lt;/p&gt;

&lt;p&gt;Just like eq. 2.1, their study had two parts. First, they used the logs from the supermarket checkout system to have a precise, individual, and always-on measure of productivity: the number of items scanned per second. And, second, because of the way that scheduling was done at this supermarket, they have near random composition of peers. In other words, even though the scheduling of cashiers is not determined by a lottery, it was essentially random. In practice, the confidence we have in natural experiments frequently hinges on the plausibility of this “as-if” random claim. Taking advantage of this random variation, Mas and Moretti found that working with higher productivity peers increases productivity. Further, Mas and Moretti used the size and richness of their dataset to move beyond the estimation of cause-and-effect to explore two more important and subtle issues: heterogeneity of this effect (for which kinds of workers is the effect larger) and mechanism behind the effect (why does having high productivity peers lead to higher productivity). We will return to these two important issues—heterogeneity of treatment effects and mechanisms—in Chapter 5 when we discuss experiments in more detail.&lt;/p&gt;

&lt;p&gt;Generalizing from the studies on the effect of the Vietnam Draft on earnings and the study of the effect of peers on productivity, Table 2.3 summarizes other studies that have this exact same structure: using an always-on data source to measure the impact of some event. As Table 2.3 makes clear, natural experiments are everywhere if you just know how to look for them.&lt;/p&gt;

&lt;p&gt;Table 2.3: Examples of natural experiments using big data sources. All these studies follow the same basic recipe: random (or as if random) event + always-on data system. See Dunning (2012) for more examples.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Substantive focus&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Source of natural experiment&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Always-on data source&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Citation&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Peer effects on productivity&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;scheduling process&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;checkout data&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Mas and Moretti (2009)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Friendship formation&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;hurricanes&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Facebook&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Phan and Airoldi (2015)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Spread of emotions&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;rain&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Facebook&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Coviello et al. (2014)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Peer to peer economic transfers&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;earthquake&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;mobile money data&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Blumenstock, Fafchamps, and Eagle (2011)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Personal consumption behavior&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2013 US government shutdown&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;personal finance data&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Baker and Yannelis (2015)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Economic impact of recommender systems&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;various&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;browsing data at Amazon&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Sharma, Hofman, and Watts (2015)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Effect of stress on unborn babies&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;2006 Israel–Hezbollah war&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Birth records&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Torche and Shwed (2015)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Reading behavior on Wikipedia&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Snowden revelations&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Wikipedia logs&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Penney (2016)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;In practice, researchers use two different strategies for finding natural experiments, both of which can be fruitful. Some researchers start with the always-on data source and look for random events in the world; others start with random events in the world and look for data sources that capture their impact. Finally, notice that the strength of natural experiments comes not from the sophistication of the statistical analysis, but from the care in discovering a fair comparison created by a fortunate accident of history.&lt;/p&gt;

&lt;h4 id=&#34;2-4-3-2-matching&#34;&gt;2.4.3.2 Matching&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;Matching create fair comparisons by pruning away cases.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Fair comparisons can come from either randomized controlled experiments or natural experiments. But, there are many situations where you can’t run the ideal experiment and nature has not provided a natural experiment. In these settings, the best way to create a fair comparison is matching. In matching, the researcher looks through non-experimental data to create pairs of people that are similar except that one has received the treatment and one has not. In the process of matching, researchers are actually also pruning; that is, discarding cases where there are no obvious comparison. Thus, this method would be more accurately called matching-and-pruning, but I’ll stick with the traditional term: matching.&lt;/p&gt;

&lt;p&gt;A beautiful example of the power of matching strategies with massive non-experimental data sources come from the research on consumer behavior by Liran Einav and colleagues (2015). Einav and colleagues were interested in auctions taking place on eBay, and in describing their work, I’ll focus on one particular aspect: the effect of auction starting price on auction outcomes, such as the sale price or the probability of a sale.&lt;/p&gt;

&lt;p&gt;The most naive way to answer the question about the effect of starting price on sale price would be to simply calculate the final price for auctions with different starting prices. This approach would be fine if you simply want to predict the sale price of a given item that had been put on eBay with a given starting price. But, if your question is what is the effect of starting price on market outcomes this approach will not work because it is not based on fair comparisons; the auctions with lower starting prices might be quite different from auctions with higher starting prices (e.g., they might be for different types of goods or include different types of sellers).&lt;/p&gt;

&lt;p&gt;If you are already concerned about making fair comparisons, you might skip the naive approach and consider running a field experiment where you would sell a specific item—say, a golf club—with a fixed set of auction parameters—say, free shipping, auction open for two weeks, etc.—but with randomly set starting prices. By comparing the resulting market outcomes, this field experiment would offer a very clear measurement of the effect of starting price on sale price. But, this measurement would only apply to one particular product and set of auction parameters. The results might be different, for example, for different types of products. Without strong theory, it is difficult to extrapolate from this single experiment the full range of possible experiments that could have been run. Further, field experiments are sufficiently expensive that it would be infeasible to run enough of them up to cover the whole parameter space of products and auction types.&lt;/p&gt;

&lt;p&gt;In contrast to the naive approach and the experimental approach, Einav and colleagues take a third approach: matching. The main trick of their strategy is to discover things similar to field experiments that have already happened on eBay. For example, Figure 2.6 shows some of the 31 listings for exactly the same golf club—a Taylormade Burner 09 Driver—being sold by exactly the same seller—“budgetgolfer”. However, these listings have slightly different characteristics. Eleven of them offer the driver for a fixed price of $124.99, while the other 20 are auctions with different end dates. Also, the listings have different shipping fees, either $7.99 or $9.99. In other words, it is as if “budgetgolfer” is running experiments for the researchers.&lt;/p&gt;

&lt;p&gt;The listings of the Taylormade Burner 09 Driver being sold by “budgetgolfer” are one example of a matched set of listings, where the exact same item is being sold by the exact same seller but each time with slightly different characteristics. Within the massive logs of eBay there are literally hundreds of thousands of matched sets involving millions of listings. Thus, rather than comparing the final price for all auctions within a given starting price, Einav and colleagues make comparisons within matched sets. In order to combine results from the comparisons within these hundreds of thousands of matched sets, Einav and colleagues re-express the starting price and final price in terms of the reference value of each item (e.g., its average sale price). For example, if the Taylormade Burner 09 Driver has a reference value of $100 (based on its sales), then a starting price of $10 would be expressed as 0.1 and final price of $120 would be expressed as 1.2.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/72131fe0e2d1f1466aa0b8ae599c143c.png&#34; alt=&#34;fig2.6&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 2.6: An example of a matched set. This is the exact same golf club (a Taylormade Burner 09 Driver) being sold by the exact same person (budgetgolfer), but some of these sales were performed different conditions (e.g., different starting price). Figure taken from Einav et al. (2015).&lt;/p&gt;

&lt;p&gt;Recall that Einav and colleagues were interested in the effect of start price on auction outcomes. First, using linear regression they estimated that higher starting prices decrease the probability of a sale, and that higher starting prices increase the final sale price, conditional on a sale occurring. By themselves, these estimates—which are averaged over all products and assume a linear relationship between starting price and final outcomes—are not all that interesting. But, Einav and colleagues also use the massive size of their data to estimate a variety of more subtle findings. First, Einav and colleagues made these estimates separately for items of different prices and without using linear regression. They found that while the relationship between start price and probability of a sale is linear, the relationship between starting price and sale price is clearly non-linear (Figure 2.7). In particular, for starting prices between 0.05 and 0.85, the starting price has very little impact on sale price, a finding that was completed missed in the analysis that had assumed a linear relationship.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/6c4b3eaa480e9f5488e837434afa152e.png&#34; alt=&#34;fig2.7&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 2.7: Relationship between auction start price and probability of a sale (left panel) and sale price (right panel). There is roughly a linear relationship between start price and probability of sale, but there is a non-linear relationship between start price and sale price; for starting prices between 0.05 and 0.85, the starting price has very little impact on sale price. In both cases, the relationships are basically independent of item value. These graphs reproduce Fig 4a and 4b Einav et al. (2015).&lt;/p&gt;

&lt;p&gt;Second, rather than averaging over all items, Einav and colleagues also use the massive scale of their data to estimate the impact of starting price for 23 different categories of items (e.g, pet supplies, electronics, and sports memorabilia) (Figure 2.8). These estimates show that for more distinctive items—such as memorabilia—start price has a smaller effect on the probability of a sale and a larger effect on the final sale price. Further, for more commodified items—such as DVDs and video—the start price has almost no impact on the final price. In other words, an average that combines results from 23 different categories of items hides important information about the differences between these items.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/9d89fbd54de2f8d18e8d9f638bad3412.png&#34; alt=&#34;fig2.8&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 2.8: Results showed estimates from each category individually; the solid dot in the estimate for all categories pooled together, Table 11(Einav et al. 2015, Table 11). These estimates show that for more distinctive items—such as memorabilia—the start price has a smaller effect on the probability of a sale (x-axis) and a larger effect on the final sale price (y-axis).&lt;/p&gt;

&lt;p&gt;Even if you are not particularly interested in auctions on eBay, you have to admire the way that Figure 2.7 and Figure 2.8 offer a richer understanding of eBay than simple linear regression estimates that assume linear relationships and combine many different categories of items. These more subtle estimates illustrate the power of matching in massive data; these estimates would have been impossible without an enormous number of field experiments, which would have been prohibitively expensive.&lt;/p&gt;

&lt;p&gt;Of course, we should have less confidence in the results of any particular matching study than we would in the results of a comparable experiment. When assessing the results from any matching study, there are two important concerns. First, we have to remember that we can only ensure fair comparisons on things that were used for matching. In their main results, Einav and colleagues did exact matching on four characteristics: seller ID number, item category, item title, and subtitle. If the items were different in ways that were not used for matching, that could create an unfair comparison. For example, if “budgetgolfer” lowered prices for Taylormade Burner 09 Driver in the winter (when golf clubs are less popular), then it could appear that lower starting prices lead to lower final prices, when in fact this would be an artifact of seasonal variation in demand. In general, the best approach to this problem seems to be trying many different kinds of matching. For example, Einav and colleagues repeat their analysis where matched sets include items on sale within one year, within one month, and contemporaneously. Making the time window tighter decreases the number of matched sets, but reduces concerns about seasonal variation. Fortunately, they find that results are unchanged by these changes in matching criteria. In the matching literature, this type of concern is usually expressed in terms of observables and unobservables, but the key idea is really that researchers are only creating fair comparisons on the features used in matching.&lt;/p&gt;

&lt;p&gt;The second major concern when interpreting matching results is that they only apply to matched data; they do not apply to the cases that could not be matched. For example, by limiting their research to items that had multiple listings Einav and colleagues are focusing on professional and semi-professional sellers. Thus, when interpreting these comparisons we must remember that they only apply to this subset of eBay.&lt;/p&gt;

&lt;p&gt;Matching is a powerful strategy for finding fair comparisons in large datasets. To many social scientists, matching feels like second-best to experiments, but that is a belief that should be revised, slightly. Matching in massive data might be better than a small number of field experiments when: 1) heterogeneity in effects is important and 2) there are good observables for matching. Table 2.4 provides some other examples of how matching can be used with big data sources.&lt;/p&gt;

&lt;p&gt;Table 2.4: Examples of studies that use matching to find fair comparisons within digital traces.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Substantive focus&lt;/th&gt;
&lt;th align=&#34;center&#34;&gt;Big data source&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Citation&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Effect of shootings on police&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;violence    Stop-and-frisk records&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Legewie (2016)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Effect of September 11, 2001 on families and neighbors&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;voting records and donation records&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Hersh (2013)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Social contagion&lt;/td&gt;
&lt;td align=&#34;center&#34;&gt;Communication and product adoption data&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Aral, Muchnik, and Sundararajan (2009)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;In conclusion, naive approaches to estimating causal effects from non-experimental data are dangerous. However, strategies for making causal estimates lying along a continuum from strongest to weakest, and researchers can discover fair comparisons within non-experimental data. The growth of always-on, big data systems increases our ability to effectively use two existing methods: natural experiments and matching.&lt;/p&gt;

&lt;h2 id=&#34;2-5-conclusion&#34;&gt;2.5 Conclusion&lt;/h2&gt;

&lt;p&gt;Big data is everywhere, but using it and other forms of observational data for social research is difficult. In my experience there is something like a no free lunch property for research: if you don’t put in a lot of work collecting data, then you are probably going to have to put in a lot of work analyzing your data or in thinking about what is in an interesting question to ask of the data. Based on the ideas in this chapter, I think that there are three main ways that big data sources will be most valuable for social research:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;empirically adjudicating between competing theoretical predictions. Examples of this kind of work include Farber (2015) (New York Taxi drivers) and King, Pan, and Roberts (2013) (Censorship in China)&lt;/li&gt;
&lt;li&gt;improved social measurement for policy through nowcasting. An example of this kind of work is Ginsberg et al. (2009) (Google Flu Trends).&lt;/li&gt;
&lt;li&gt;estimating causal effects with natural experiments and matching. Examples of this kind of work. Mas and Moretti (2009) (peer effects on productivity) and Einav et al. (2015) (effect of starting price on auctions at eBay).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Many important questions in social research could be expressed as one of these three. However, these approaches generally require researchers to bring a lot to the data. What makes Farber (2015) interesting is the theoretical motivation for the measurement. This theoretical motivation comes from outside the data. Thus, for those who are good at asking certain types of research questions, big data sources can be very fruitful.&lt;/p&gt;

&lt;p&gt;Finally, rather than theory-driven empirical research (which has been the focus on this chapter), we can flip the script and create empirically-driven theorizing. That is, through the careful accumulation of empirical facts, patterns, and puzzles, we can build new theories.&lt;/p&gt;

&lt;p&gt;This alternative, data-first approach to theory is not new, and it was most forcefully articulated by Glaser and Strauss (1967) with their call for grounded theory. This data-first approach, however, does not imply “the end of theory,” as has been claimed by much of the journalism around research in the digital age (Anderson 2008). Rather, as the data environment changes, we must expect a re-balancing in the relationship between theory and data. In a world where data collection was expensive, it makes sense to only collect the data that theories suggest will be the most useful. But, in a world where enormous amounts of data are already available for free, it makes sense to also try a data-first approach (Goldberg 2015).&lt;/p&gt;

&lt;p&gt;As I have shown in this chapter, researchers can learn a lot by watching people. In the next three chapters, I’ll describe how we can learn more and different things if we tailor our data collection and interact with people more directly by asking them questions (Chapter 3), running experiments (Chapter 4), and even involving them in the research process directly (Chapter 5).&lt;/p&gt;

&lt;h2 id=&#34;technical-appendix&#34;&gt;Technical appendix&lt;/h2&gt;

&lt;p&gt;In the main text, I discussed making causal claims from non-experimental data using natural experiments and matching. In this appendix, I will introduce the potential outcomes model, and define more precisely the conditions that are required for causal inference from observational data. This chapter will draw on Morgan and Winship (2014) and Imbens and Rubin (2015).&lt;/p&gt;

&lt;h2 id=&#34;further-commentary&#34;&gt;Further commentary&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;This section is designed to be used as a reference, rather than to be read as a narrative.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Introduction (Section 2.1)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;One kind of observing that is not included in this chapter is ethnography. For more on ethnography in digital spaces see Boellstorff et al. (2012), and for more on ethnography in mixed digital and physical spaces see Lane (2016).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Big data (Section 2.2)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;When you are repurposing data, there are two mental tricks that can help you understand the possible problems that you might encounter. First, you can try to imagine the ideal dataset for your problem and the compare that to the dataset that you are using. How are they similar and how are they different? If you didn’t collect your data yourself, there are likely to be difference between what you want and what you have. But, you have to decide if these differences are minor or major.&lt;/p&gt;

&lt;p&gt;Second, remember that someone created and collected your data for some reason. You should try to understand their reasoning. This kind of reverse-engineering can help you identify possible problems and biases in your repurposed data.&lt;/p&gt;

&lt;p&gt;There is no single consensus definition of “big data”, but many definitions seem to focus on the 3 Vs: volume, variety, and velocity (e.g., Japec et al. (2015)). Rather than focusing on the characteristics of the data, my definition focuses more on why the data was created.&lt;/p&gt;

&lt;p&gt;My inclusion of government administrative data inside the category of big data is a bit unusually. Others who have made this case, include Legewie (2015), Connelly et al. (2016), and Einav and Levin (2014). For more about the value of government administrative data for research, see Card et al. (2010), Taskforce (2012), and Grusky, Smeeding, and Snipp (2015).&lt;/p&gt;

&lt;p&gt;For a view of administrative research from inside the government statistical system, particularly the US Census Bureau, see Jarmin and O’Hara (2016). For a book length treatment of the administrative records research at Statistics Sweden, see Wallgren and Wallgren (2007).&lt;/p&gt;

&lt;p&gt;In the chapter, I briefly compared a traditional survey such as the General Social Survey (GSS) to a social media data source such as Twitter. For a thorough and careful comparison between traditional surveys and social media data, see Schober et al. (2016).&lt;/p&gt;

&lt;p&gt;Common characteristics of big data (Section 2.3)
These 10 characteristics of big data have been described in a variety of different ways by a variety of different authors. Writing that influenced my thinking on these issues include: Lazer et al. (2009), Groves (2011), Howison, Wiggins, and Crowston (2011), boyd and Crawford (2012), Taylor (2013), Mayer-Schönberger and Cukier (2013), Golder and Macy (2014), Ruths and Pfeffer (2014), Tufekci (2014), Sampson and Small (2015), Lewis (2015), Lazer (2015), Horton and Tambe (2015), Japec et al. (2015), and Goldstone and Lupyan (2016).&lt;/p&gt;

&lt;p&gt;Throughout this chapter, I’ve used the term digital traces, which I think is relatively neutral. Another popular term for digital traces is digital footprints (Golder and Macy 2014), but as Hal Abelson, Ken Ledeen, and Harry Lewis (2008) point out, a more appropriate term is probably digital fingerprints. When you create footprints, you are aware of what is happening and your footprints cannot generally be traced to you personally. The same is not true for your digital traces. In fact, you are leaving traces all the time about which you have very little knowledge. And, although these traces don’t have your name on them, they can often be linked back to you. In other words, they are more like fingerprints: invisible and personally identifying.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Big&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For more on why large datasets, render statistical tests problematic, see Lin, Lucas, and Shmueli (2013) and McFarland and McFarland (2015). These issues should lead researchers to focus on practical significance rather than statistical significance.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Always-on&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;When considering always-on data, it is important to consider whether you are comparing the exact same people over time or whether you are comparing some changing group of people; see for example, Diaz et al. (2016).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Non-reactive&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;A classic book on non-reactive measures is Webb et al. (1966). The examples in the book pre-date the digital age, but they are still illuminating. For examples of people changing their behavior because of the presence of mass surveillance, see Penney (2016) and Brayne (2014).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Incomplete&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For more on record linkage, see Dunn (1946) and Fellegi and Sunter (1969) (historical) and Larsen and Winkler (2014) (modern). Similar approached have also been developed in computer science under the names such as data deduplication, instance identification, name matching, duplicate detection, and duplicate record detection (Elmagarmid, Ipeirotis, and Verykios 2007). There are also privacy preserving approaches to record linkage which do not require the transmission of personally identifying information (Schnell 2013). Facebook also has developed a proceed to link their records to voting behavior; this was done to evaluate an experiment that I’ll tell you about in Chapter 4 (Bond et al. 2012; Jones et al. 2013).&lt;/p&gt;

&lt;p&gt;For more on construct validity, see Shadish, Cook, and Campbell (2001), Chapter 3.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Inaccessible&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For more on the AOL search log debacle, see Ohm (2010). I offer advice about partnering with companies and governments in Chapter 4 when I describe experiments. A number of authors have expressed concerns about research that relies on inaccessible data, see Huberman (2012) and boyd and Crawford (2012).&lt;/p&gt;

&lt;p&gt;One good way for university researchers to acquire data access is to work at a company as an intern or visiting researcher. In addition to enabling data access, this process will also help the researcher learn more about how the data was created, which is important for analysis.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Non-representative&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Non-representativeness is a major problem for researchers and governments who wish to make statements about an entire population. This is less of concern for companies that are typically focused on their users. For more on how Statistics Netherlands considers the issue of non-representativeness of business big data, see Buelens et al. (2014).&lt;/p&gt;

&lt;p&gt;In Chapter 3, I’ll describe sampling and estimation in much greater detail. Even if data are non-representative, under certain conditions, they can be weighted to produce good estimates.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Drifting&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;System drift is very hard to see from the outside. However, the MovieLens project (discussed more in Chapter 4) has been run for more than 15 years by an academic research group. Therefore, they have documented and shared information about the way that the system has evolved over time and how this might impact analysis (Harper and Konstan 2015).&lt;/p&gt;

&lt;p&gt;A number of scholars have focused on drift in Twitter: Liu, Kliman-Silver, and Mislove (2014) and Tufekci (2014).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Algorithmically confounded&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;I first heard the term “algorithmically confounded” used by Jon Kleinberg in a talk. The main idea behind performativity is that some social science theories are “engines not cameras” (Mackenzie 2008). That is, they actually shape the world rather than just capture it.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Dirty&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Governmental statistical agencies call data cleaning, statistical data editing. De Waal, Puts, and Daas (2014) describe statistical data editing techniques developed for survey data and examine to which extent they are applicable to big data sources, and Puts, Daas, and Waal (2015) presents some of the same ideas for a more general audience.&lt;/p&gt;

&lt;p&gt;For some examples of studies focused on spam in Twitter, Clark et al. (2016) and Chu et al. (2012). Finally, Subrahmanian et al. (2016) describes the results of the DARPA Twitter Bot Challenge.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Sensitive&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Ohm (2015) reviews earlier research on the idea of sensitive information and offers a multi-factor test. The four factors he proposes are: the probability of harm; probability of harm; presence of a confidential relationship; and whether the risk reflect majoritarian concerns.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Counting things (Section 2.4.1)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Farber’s study of taxis in New York was based on an earlier study by Camerer et al. (1997) that used three different convenience samples of paper trip sheets—paper forms used by drivers to record trip start time, end time, and fare. This earlier study found that drivers seemed to be target earners: they worked less on days where their wages were higher.&lt;/p&gt;

&lt;p&gt;Kossinets and Watts (2009) was focused on the origins of homophily in social networks. See Wimmer and Lewis (2010) for a different approach to the same problem which uses data from Facebook.&lt;/p&gt;

&lt;p&gt;In subsequent work, King and colleagues have further explored online censorship in China (King, Pan, and Roberts 2014; King, Pan, and Roberts 2016). For a related approach to measuring online censorship in China, see Bamman, O’Connor, and Smith (2012). For more on statistical methods like the one used in King, Pan, and Roberts (2013) to estimate the sentiment of the 11 million posts, see Hopkins and King (2010). For more on supervised learning, see James et al. (2013) (less technical) and Hastie, Tibshirani, and Friedman (2009) (more technical).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Forecasting (Section 2.4.2)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Forecasting is a big part of industrial data science (Mayer-Schönberger and Cukier 2013; Provost and Fawcett 2013). One type of forecasting that are commonly done by social researchers are demographic forecasting, for example Raftery et al. (2012).&lt;/p&gt;

&lt;p&gt;Google Flu Trends was not the first project to use search data to nowcast influenza prevalence. In fact, researchers in the United States (Polgreen et al. 2008; Ginsberg et al. 2009) and Sweden (Hulth, Rydevik, and Linde 2009) have found that certain search terms (e.g., “flu”) predicted national public health surveillance data before it was released. Subsequently many, many other projects have tried to use digital trace data for disease surveillance detection, see Althouse et al. (2015) for a review.&lt;/p&gt;

&lt;p&gt;In addition to using digital trace data to predict health outcomes, there has also been a huge amount of work using Twitter data to predict election outcomes; for reviews see Gayo-Avello (2011), Gayo-Avello (2013), Jungherr (2015) (Ch. 7), and Huberty (2015).&lt;/p&gt;

&lt;p&gt;Using search data to predicting influenza prevalence and using Twitter data to predict elections are both examples of using some kind of digital trace to predict some kind of event in the world. There an enormous number of studies that have this general structure. Table 2.5 includes a few other examples.&lt;/p&gt;

&lt;p&gt;Table 2.5: Partial list of studies use some digital trace to predict some event.&lt;/p&gt;

&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Digital trace&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Outcome&lt;/th&gt;
&lt;th align=&#34;right&#34;&gt;Citation&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;

&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Twitter&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Box office revenue of movies in the US&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Asur and Huberman (2010)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Search logs&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Sales of movies, music, books, and video games in the US&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Goel et al. (2010)&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td&gt;Twitter&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Dow Jones Industrial Average (US stock market)&lt;/td&gt;
&lt;td align=&#34;right&#34;&gt;Bollen, Mao, and Zeng (2011)&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Approximating experiments (Section 2.4.3)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;The journal P.S. Political Science had a symposium on big data, causal inference, and formal theory, and Clark and Golder (2015) summarizes each contribution. The journal Proceedings of the National Academy of Sciences of the United States of America had a symposium on causal inference and big data, and Shiffrin (2016) summarizes each contribution.&lt;/p&gt;

&lt;p&gt;In terms of natural experiments, Dunning (2012) provides an excellent book length treatment. For more on using the Vietnam draft lottery as a natural experiment, see Berinsky and Chatfield (2015). For machine learning approaches that attempt to automatically discover natural experiments inside of big data sources, see Jensen et al. (2008) and Sharma, Hofman, and Watts (2015).&lt;/p&gt;

&lt;p&gt;In terms of matching, for an optimistic review, see Stuart (2010), and for a pessimistic review see Sekhon (2009). For more on matching as a kind of pruning, see Ho et al. (2007). For books that provide excellent treatments of matching, see Rosenbaum (2002), Rosenbaum (2009), Morgan and Winship (2014), and Imbens and Rubin (2015).&lt;/p&gt;

&lt;h2 id=&#34;activities&#34;&gt;Activities&lt;/h2&gt;

&lt;p&gt;Key:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;degree of difficulty: easy easy, medium medium, hard hard, very hard very hard&lt;/li&gt;
&lt;li&gt;requires math (requires math)&lt;/li&gt;
&lt;li&gt;requires coding (requires coding)&lt;/li&gt;
&lt;li&gt;data collection (data collection)&lt;/li&gt;
&lt;li&gt;my favorites (my favorite)&lt;/li&gt;
&lt;/ul&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;[medium, my favorite] Algorithmic confounding was a problem with Google Flu Trends. Read the paper by Lazer et al. (2014), and write a short, clear email to an engineer at Google explaining the problem and offering an idea of how to fix the problem.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;[medium] Bollen, Mao, and Zeng (2011) claims that data from Twitter can be used to predict the stock market. This finding led to the creation of a hedge fund—Derwent Capital Markets—to invest in the stock market based on data collected from Twitter (Jordan 2010). What evidence would you want to see before putting your money in that fund?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;[easy] While some public health advocates hail e-cigarettes as an effective aid for smoking cessation, others warn about the potential risks, such as the high-levels of nicotine. Imagine that a researcher decides to study public opinion toward e-cigarettes by collecting e-cigarettes-related Twitter posts and conducting sentiment analysis.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;What are the three possible biases that you are most worried about in this study?&lt;/li&gt;
&lt;li&gt;Clark et al. (2016) ran just such a study. First, they collected 850,000 tweets that used e-cigarette-related keywords from January 2012 through December 2014. Upon closer inspection, they realized that many of these tweets were automated (i.e., not produced by humans) and many of these automated tweets were essentially commercials. They developed a Human Detection Algorithm to separate automated tweets from organic tweets. Using this Human Detect Algorithm they found that 80% of tweets were automated. Does this finding change your answer to part (a)?&lt;/li&gt;
&lt;li&gt;When they compared the sentiment in organic and automated tweets they found that the automated tweets are more positive than organic tweets (6.17 versus 5.84). Does this finding change your answer to (b)?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;[easy] In November 2009, Twitter changed the question in the tweet box from “What are you doing?” to “What’s happening?” (&lt;a href=&#34;https://blog.twitter.com/2009/whats-happening&#34; target=&#34;_blank&#34;&gt;https://blog.twitter.com/2009/whats-happening&lt;/a&gt;).&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;How do you think the change of prompts will affect who tweet and/or what they tweet?&lt;/li&gt;
&lt;li&gt;Name one research project for which you would prefer the prompt “What are you doing?” Explain why.&lt;/li&gt;
&lt;li&gt;Name one research project for which you would prefer the prompt “What’s happening?” Explain why.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;[medium] Kwak et al. (2010) analyzed 41.7 million user profiles, 1.47 billion social relations, 4262 trending topics, and 106 million tweets between June 6th and June 31st, 2009. Based on this analysis they concluded that Twitter serves more as a new medium of information sharing than a social network.&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Considering Kwak et al’s finding, what type of research would you do with Twitter data? What type of research would you not do with Twitter data? Why?&lt;/li&gt;
&lt;li&gt;In 2010, Twitter added a Who To Follow service making tailored suggestion to users. Three recommendations are shown at a time on the main page. Recommendations are often drawn from one’s “friends-of-friends,” and mutual contacts are also displayed in the recommendation. Users can refresh to see a new set of recommendations or visit a page with a longer list of recommendations. Do you think this new feature would change your answer to part a)? Why or why not?&lt;/li&gt;
&lt;li&gt;Su, Sharma, and Goel (2016) evaluated the effect of Who To Follow service and found that while users across the popularity spectrum benefited from the recommendations, the most popular users profited substantially more than average. Does this finding change your answer to part b)? Why or why not?&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;[easy] “Retweets” are often used to measure influence and spread of influence on Twitter. Initially, users had to copy and paste the tweet they liked, tag the original author with his/her handle, and manually type “RT” before the tweet to indicate that it’s a retweet. Then, in 2009 Twitter added a “retweet” button. In June 2016, Twitter made it possible for users to retweet their own tweets (&lt;a href=&#34;https://twitter.com/twitter/status/742749353689780224&#34; target=&#34;_blank&#34;&gt;https://twitter.com/twitter/status/742749353689780224&lt;/a&gt;). Do you think these changes should affect how you use “retweets” in your research? Why or why not?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;[medium, data collection, requires coding] Michel et al. (2011) constructed a corpus emerging from Google’s effort to digitize books. Using the first version of the corpus, which was published in 2009 and contained over 5 million digitized books, the authors analyzed word usage frequency to investigate linguistic changes and cultural trends. Soon the Google Books Corpus became a popular data source for researchers, and a 2nd version of the database was released in 2012.&lt;/p&gt;

&lt;p&gt;However, Pechenick, Danforth, and Dodds (2015) warned that researchers need to fully characterize the sampling process of the corpus before using it for drawing broad conclusions. The main issue is that the corpus is library-like, containing one of each book. As a result, an individual, prolific author is able to noticeably insert new phrases into the Google Books lexicon. Moreover, scientific texts constitute an increasingly substantive portion of the corpus throughout the 1900s. In addition, by comparing two versions of the English Fiction datasets, Pechenick et al. found evidence that insufficient filtering was used in producing the first version. All of the data needed for activity is available here: &lt;a href=&#34;http://storage.googleapis.com/books/ngrams/books/datasetsv2.html&#34; target=&#34;_blank&#34;&gt;http://storage.googleapis.com/books/ngrams/books/datasetsv2.html&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;In Michel et al.’s original paper (2011), they used the 1st version of the English data set, plotted the frequency of usage of the years “1880”, “1912” and “1973”, and concluded that “we are forgetting our past faster with each passing year” (Fig. 3A, Michel et al.). Replicate the same plot using 1) 1st version of the corpus, English dataset (same as Fig. 3A, Michel et al.)&lt;/p&gt;

&lt;p&gt;Now replicate the same plot with the 1st version, English fiction dataset.&lt;/p&gt;

&lt;p&gt;Now replicate the same plot with the 2nd version of the corpus, English dataset.&lt;/p&gt;

&lt;p&gt;Finally, replicate the same plot with the 2nd version, English fiction dataset.&lt;/p&gt;

&lt;p&gt;Describe the differences and similarities between these four plots. Do you agree with Michel et al.’s original interpretation of the observed trend? (Hint: c) and d) should be the same as Figure 16 in Pechenick et al.)&lt;/p&gt;

&lt;p&gt;Now that you have replicated this one finding using different Google Books corpora, choose another linguistic change or cultural phenomena presented in Michel et al.’s original paper. Do you agree with their interpretation in light of the limitations presented in Pechenick et al.? To make your argument stronger, try replicate the same graph using different versions of data set as above.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;[very hard, data collection, requires coding, my favorite] Penney (2016) explores whether the widespread publicity about NSA/PRISM surveillance (i.e., the Snowden revelations) in June 2013 is associated with a sharp and sudden decrease in traffic to Wikipedia articles on topics that raise privacy concerns. If so, this change in behavior would be consistent with a chilling effect resulting from mass surveillance. The approach of Penney (2016) is sometimes called an interrupted time series design and is related to the approaches in the chapter about approximating experiments from observational data (Section 2.4.3).&lt;/p&gt;

&lt;p&gt;To choose the topic keywords, Penney referred to the list used by U.S. Department of Homeland Security for tracking and monitoring social media. The DHS list categorizes certain search terms into a range of issues, i.e. “Health Concern,” “Infrastructure Security,” and “Terrorism.” For the study group, Penney used the forty-eight keywords related to “Terrorism” (see Table 8 Appendix). He then aggregated Wikipedia article view counts on a monthly basis for the corresponding forty-eight Wikipedia articles over a thirty-two month period, from the beginning of January 2012 to the end of August 2014. To strengthen his argument, he also created several comparison groups by tracking article views on other topics.&lt;/p&gt;

&lt;p&gt;Now, you are going to replicate and extend Penney (2016). All the raw data that you will need for this activity is available from Wikipedia (&lt;a href=&#34;https://dumps.wikimedia.org/other/pagecounts-raw/&#34; target=&#34;_blank&#34;&gt;https://dumps.wikimedia.org/other/pagecounts-raw/&lt;/a&gt;). Or you can get it from the R package wikipediatrend (Meissner and Team 2016). When you write-up your responses, please note which data source you used. (Note: This same activity also appears in Chapter 6)&lt;/p&gt;

&lt;p&gt;Read Penney (2016) and replicate Figure 2 which shows the page views for “Terrorism”-related pages before and after the Snowden revelation. Interpret the findings.&lt;/p&gt;

&lt;p&gt;Next, replicate Fig 4A, which compares the study group (“Terrorism”-related articles) with a comparator group using keywords categorized under “DHS &amp;amp; Other Agencies” from the DHS list (see Appendix Table 10). Interpret the findings.&lt;/p&gt;

&lt;p&gt;In part b) you compared the study group to one comparator group. Penney also compared to two other comparator groups: “Infrastructure Security”-related articles (Appendix Table 11) and popular Wikipedia pages (Appendix Table 12). Come up with an alternative comparator group, and test if the findings from part b) is sensitive to your choice of comparator group. Which choice of comparator group makes most sense? Why?&lt;/p&gt;

&lt;p&gt;The author stated that keywords relating to “Terrorism” were used to select the Wikipedia articles because the U.S. government cited terrorism as a key justification for its online surveillance practices. As a check of these 48 “Terrorism”-related keywords, Penney (2016) also conducted a survey on MTurk asking respondents to rate each of keywords in terms of Government Trouble, Privacy-Sensitive, and Avoidance (Appendix Table 7 and 8). Replicate the survey on MTurk and compare your results.&lt;/p&gt;

&lt;p&gt;Based on the results in part d) and your reading of the article, do you agree with the author’s choice of topic keywords in the study group? Why or why not? If not, what would you suggest instead?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;[easy] Efrati (2016) reports, based on confidential information, that “total sharing” on Facebook had declined by about 5.5% year over year while “original broadcast sharing” was down 21% year over year. This decline was particularly acute with Facebook users under 30 years of age. The report attributed the decline to two factors. One is the growth in the number of “friends” people have on Facebook. The other is that some sharing activity has shifted to messaging and to competitors such as SnapChat. The report also revealed the several tactics Facebook had tried to boost sharing, including News Feed algorithm tweaks that make original posts more prominent, as well as periodical reminders of the original posts users “On This Day” several years ago. What implications, if any, does these findings have for researchers who want to use Facebook as a data source?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;[medium] Tumasjan et al. (2010) reported that proportion of tweets mentioning a political party matched the proportion of votes that party received in the German parliamentary election in 2009 (Figure 2.9). In other words, it appeared that you could use Twitter to predict the election. At the time this study was published it was considered extremely exciting because it seemed to suggest a valuable use for a common source of big data.&lt;/p&gt;

&lt;p&gt;Given the bad features of big data, however, you should immediately be skeptical of this result. Germans on Twitter in 2009 were quite a non-representative group, and supporters of one party might tweet about politics more often. Thus, it seems surprising that all the possible biases that you could imagine would somehow cancel out. In fact, the results in Tumasjan et al. (2010) turned out to be too good to be true. In their paper, Tumasjan et al. (2010) considered six political parties: Christian Democrats (CDU), Christian Social Democrats (CSU), SPD, Liberals (FDP), The Left (Die Linke), and the Green Party (Grüne). However, the most mentioned German political party on Twitter at that time was the Pirate Party (Piraten), a party that fights government regulation of the Internet. When the Pirate Party was included in the analysis, Twitter mentions becomes a terrible predictor of election results (Figure 2.9) (Jungherr, Jürgens, and Schoen 2012).&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/e5b24f95166fc15fa7aad386ff75fd2b.png&#34; alt=&#34;fig2.9&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;Figure 2.9: Twitter mentions appear to predict the results of the 2009 German election (Tumasjan et al. 2010), but this result turns out to depend on some arbitrary and unjustified choices (Jungherr, Jürgens, and Schoen 2012).
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Subsequently, other researchers around the world have used fancier methods—such as using sentiment analysis to distinguish between positive and negative mentions of the parties—in order to improve the ability of Twitter data to predict a variety of different types of elections (Gayo-Avello 2013; Jungherr 2015, Ch. 7.). Here’s how Huberty (2015) summarized the results of these attempts to predict elections:&lt;/p&gt;

&lt;p&gt;“All known forecasting methods based on social media have failed when subjected to the demands of true forward-looking electoral forecasting. These failures appear to be due to fundamental properties of social media, rather than to methodological or algorithmic difficulties. In short, social media do not, and probably never will, offer a stable, unbiased, representative picture of the electorate; and convenience samples of social media lack sufficient data to fix these problems post hoc.”&lt;/p&gt;

&lt;p&gt;Read some of the research that lead Huberty (2015) to that conclusion, and write a one page memo to a political candidate describing if and how Twitter should be used to forecast elections.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;p&gt;[medium] What is the difference between a sociologist and a historian? According to Goldthorpe (1991), the main difference between a sociologist and a historian is control over data collection. Historians are forced to use relics whereas sociologists can tailor their data collection to specific purposes. Read Goldthorpe (1991). How is the difference between sociology and history related to the idea of Custommades and Readymades?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;[hard] Building on the previous question, Goldthorpe (1991) drew a number of critical responses, including one from Nicky Hart (1994) that challenged Goldthorpe’s devotion to tailor made data. To clarify the potential limitations of tailor-made data, Hart described the Affluent Worker Project, a large survey to measure the relationship between social class and voting that was conducted by Goldthorpe and colleagues in the mid-1960s. As one might expect from a scholar who favored designed data over found data, the Affluent Worker Project collected data that was tailored to address a recently proposed theory about the future of social class in an era of increasing living standards. But, Goldthorpe and colleagues somehow “forgot” to collect information about the voting behavior of women. Here’s how Nicky Hart (1994) summaries the whole episode:&lt;/p&gt;

&lt;p&gt;“&amp;hellip; it [is] difficult to avoid the conclusion that women were omitted because this ‘tailor made’ dataset was confined by a paradigmatic logic which excluded female experience. Driven by a theoretical vision of class consciousness and action as male preoccupations &amp;hellip; , Goldthorpe and his colleagues constructed a set of empirical proofs which fed and nurtured their own theoretical assumptions instead of exposing them to a valid test of adequacy.”&lt;/p&gt;

&lt;p&gt;Hart continued:&lt;/p&gt;

&lt;p&gt;“The empirical findings of the Affluent Worker Project tell us more about the masculinist values of mid-century sociology than they inform the processes of stratification, politics and material life.”&lt;/p&gt;

&lt;p&gt;Can you think of other examples where tailor-made data collection has the biases of the data collector built into it? How does this compare to algorithmic confounding? What implications might this have for when researchers should use Readymades and when they should use Custommades?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;[medium] In this chapter, I contrasted data collected by researchers for researchers with administrative records created by companies and governments. Some people call these administrative records “found data,” which they contrast with “designed data.” It is true that administrative records are found by researchers, but they are also highly designed. For example, modern tech companies spend enormous amounts of time and resources to collect and curate their data. Thus, these administrative records are both found and designed, it just depends on your perspective (Figure 2.10).&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/ea3b7813b00b8b90a27151f5f102f13a.png&#34; alt=&#34;fig2.10&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 2.10: The picture is both a duck and a rabbit; what you see depends on your perspective. Government and business administrative records are both found and designed; what you see depends on your perspective. For example, the call data records collected by a cell phone company are found data from the perspective of a researcher. But, these exact same records are designed data perspective of someone working in the billing department of the phone company. Source: Wikimedia Commons&lt;/p&gt;

&lt;p&gt;Provide an example of data source where seeing it both as found and designed is helpful when using that data source for research.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;[easy] In a thoughtful essay, Christian Sandvig and Eszter Hargittai (2015) describe two kinds of digital research, where the digital system is “instrument” or “object of study.” An example of the first kind of study is where Bengtsson and colleagues (2011) used mobile phone data to track migration after the earthquake in Haiti in 2010. An example of the second kind is where Jensen (2007) studies how the introduction of mobile phones throughout Kerala, India impacted the functioning of the market for fish. I find this helpful because it clarifies that studies using digital data sources can have quite different goals even if they are using the same kind of data source. In order to further clarify this distinction, describe four studies that you’ve seen: two that use a digital system as an instrument and two that use a digital system as an object of study. You can use examples from this chapter if you want.&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;3-asking-questions&#34;&gt;3 Asking questions&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;http://www.bitbybitbook.com/en/asking-questions/&#34; target=&#34;_blank&#34;&gt;http://www.bitbybitbook.com/en/asking-questions/&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;3-1-introduction&#34;&gt;3.1 Introduction&lt;/h2&gt;

&lt;p&gt;Researchers who study dolphins can’t ask them questions. So, dolphin researchers are forced to study behavior.Researchers who study humans, on the other hand, should take advantage of the fact that our participants can talk. Asking people questions has been an important part of social research for a long time, and the digital age both enables and requires certain changes in survey research. Despite the pessimism that some survey researchers currently feel, I expect that the digital age is going to be a golden age of survey research.&lt;/p&gt;

&lt;p&gt;The history of survey research can be roughly divided into three overlapping eras, separated by two contested transitions (Groves 2011; Converse 1987). Right now we are in a period of transition between the second and third eras, but the first and the second eras—as well as the transition between them—provide insight into the future of survey research.&lt;/p&gt;

&lt;p&gt;During the first era of survey research, roughly 1930 - 1960, developments in scientific sampling and questionnaire design gradually resulted in a modern understanding of survey research. The first era of survey research was characterized by area probability sampling and face-to-face interviews.&lt;/p&gt;

&lt;p&gt;Then, a technological development—the widespread diffusion of landline phones in wealthy countries—eventually led to the second era of survey research. This second era, roughly from 1960 - 2000, was characterized by random digit dialing (RDD) probability sampling and telephone interviews. The change from the first era to the second era resulted in major increases in efficiency and decreases in cost. Many researchers regard this second era as the golden age of survey research.&lt;/p&gt;

&lt;p&gt;Now, another technological development—the digital age—will eventually bring us to a third era of survey research. This transition is being driven by both push and pull factors. In part, researchers are being forced to change because approaches from the second era are breaking down in the digital age (Meyer, Mok, and Sullivan 2015). For example, more and more homes do not have landline telephones and non-response rates—respondents who are sampled but do not participate in surveys—have been increasing (Council 2013). Simultaneous with this breakdown of second-era approaches to sampling and interviewing, there is increasing availability of big data sources (see Chapter 2) that appears to threaten to replace surveys. In addition to these push factors, there are also pull factors: the third era approaches offer incredible opportunities, as I’ll show in this chapter. Although things are not totally settled yet, I expect that the third era of survey research will be characterized by non-probability sampling and computer-administered interviews. Further, although the earlier eras were characterized by their approaches to sampling and interviewing, I expect that the third era of survey research will also be characterized by the linkage of surveys with big data sources (Table 3.1).&lt;/p&gt;

&lt;p&gt;Table 3.1: Three eras of survey research. This chapter will focus on the third era of survey research: non-probability sampling, computer-administered interviews, and surveys linked to other data.&lt;/p&gt;

&lt;p&gt;|Time   |Sampling   |Interviewing|  Data environment|
|&amp;ndash;|:&amp;ndash;|:&amp;ndash;|:&amp;ndash;|
|First era| 1930 - 1960|    Area probability sampling|  Face-to-face    |Stand-alone surveys|
|Second era |1960 - 2000    |Random digit dialing (RDD) probability sampling|   Telephone   |Stand-alone surveys|
|Third era  |2000 - present|    Non-probability sampling    Computer-administered   |Surveys linked to other data|&lt;/p&gt;

&lt;p&gt;The transition between the second and third eras of survey research has not been completely smooth, and there have been fierce debates about how researchers should proceed. Looking back on the transition between the first and second eras, I think there is one key insight for us now: the beginning is not the end. That is, initially many second-era methods were ad-hoc and did not work very well. But, through hard work, researchers solved these problems, and second-era approaches eventually were better than first-era approaches. For example, researchers had been doing telephone random digit dialing for many years before Mitofsky and Waksberg developed a random digit dialing sampling method that had good practical and theoretical properties (Waksberg 1978; Brick and Tucker 2007). Thus, we should not confuse the current state of third-era approaches with their ultimate outcomes. The history of survey research makes clear that the field evolves, driven by changes in technology and society. There is no way to stop that evolution. Rather, we should embrace it, while continuing to draw wisdom from earlier eras. In fact, I believe that the digital age will be the most exciting age yet for asking people questions.&lt;/p&gt;

&lt;p&gt;The remainder of the chapter begins by arguing that big data sources will not replace surveys and that the abundance of data increases—not decreases—the value of surveys (Section 3.2). Given that motivation, I’ll summarize the total survey error framework (Section 3.3) that was developed during the first two eras of survey research. This framework enables us to understand new approaches to representation—in particular, non-probability samples (Section 3.4)—and new approaches to measurement—in particular, new ways of asking questions to respondents (Section 3.5). Finally, I’ll describe two research templates for linking survey data to big data sources (Section 3.6).&lt;/p&gt;

&lt;h2 id=&#34;3-2-asking-vs-observing&#34;&gt;3.2 Asking vs. observing&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;We are always going to need to ask people questions.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Given that more and more of our behavior is capture in government and business administrative data, some people might think that asking questions is a thing of the past. But, its not that simple. It is certainly true that researchers will ask less about behavior in the future, but, as I discussed in Chapter 2, there are real problems with the accuracy, completeness, and accessibility of many big data sources. Therefore, I expect that the problems with these data sources mean that researchers will continue to ask respondents about their behavior for the foreseeable future.&lt;/p&gt;

&lt;p&gt;In addition to these practical reasons, there is also a more fundamental reason to ask: behavior data—even perfect behavior data—is limited. Some of the most important social outcomes and predictors are internal states, such as emotions, knowledge, expectations, and opinions. Internal states exist only inside people’s heads, and sometimes the best way to learn about internal states is to ask.&lt;/p&gt;

&lt;p&gt;The practical and fundamental limitations of big data sources, and how they can be overcome with surveys, are illustrated by Moira Burke and Robert Kraut’s (2014) research on how the strength of friendships was impacted by interaction on Facebook. At the time, Burke was working at Facebook so she had complete access to one of the most massive and detailed records of human behavior ever created. But, even still, Burke and Kraut had to use surveys in order to answer their research question. Their outcome of interest—how close the respondent feels to specific friends—is an internal state that only exists inside the respondent’s head. Further, in addition to using a survey to collect their outcome of interest, Burke and Kraut also had to use a survey to learn about other potentially confounding factors. In particular, they wanted to separate the impact of communicating on Facebook from communication through other channels (e.g., email, phone, face-to-face). Even though interactions through email and phone are automatically recorded, these traces were not available to Burke and Kraut. Combining their survey data about friendship strength and non-Facebook interaction with the Facebook log data, Burke and Kraut concluded that &lt;strong&gt;communication via Facebook did in fact lead to increased feelings of closeness&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;As the work of Burke and Kraut illustrates, big data sources will not eliminate the need to ask people questions. In fact, I would draw the opposite lesson from this study: big data actually increases the value of asking questions, as I will show throughout this chapter. Therefore, the best way to think about the relationship between asking and observing is that they are complements rather than substitutes; they are like peanut butter and jelly. When there is more peanut butter, people want more jelly; when there is more big data, people want more surveys.&lt;/p&gt;

&lt;h3 id=&#34;3-3-the-total-survey-error-framework&#34;&gt;3.3 The total survey error framework&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;Total survey error = representation errors + measurement errors.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;There are many kinds of errors that can creep into estimates from surveys, and since the 1940s researchers have worked to systematically organize, understand, and reduce these errors. An important result from all of that effort is the total survey error framework (Groves et al. 2009; Weisberg 2005). The main insight from the total survey error framework is that problems can be grouped into two main buckets: problems related to who you talk to (representation) and problems related to what you learn from those conversations (measurement). For example, you might be interested in estimating attitudes about online privacy among adults living in France. Making these estimates requires two quite different types of inference. First, from the answers that respondents give, you have to infer their attitudes about online privacy. Second, from the inferred attitudes among respondents, you must infer the attitudes in the population as a whole. The first type of inference is the domain of psychology and cognitive science; and the second type of inference is the domain of statistics. A perfect sampling scheme with bad survey questions will produce bad estimates, and a bad sampling scheme with perfect survey questions will also produce bad estimates. Good estimates requires sound approaches to measurement and representation. Given that background, next, I’ll review how survey researchers have thought about representation and measurement in the past. I expect that much of this material will be review to social scienitsts, but it may be new to some data scientists. Then, I’ll show you how those ideas guide digital age survey research.&lt;/p&gt;

&lt;h3 id=&#34;3-3-1-representation&#34;&gt;3.3.1 Representation&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;Representation is about making inferences from your respondents to your target population.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In order to understand the kind of errors that can happen when inferring from respondents to the larger population, let’s consider the Literary Digest straw poll that tried to predict the outcome of the 1936 US Presidential election. Although it was more than 75 years ago, this debacle still has an important lesson to teach researchers today.&lt;/p&gt;

&lt;p&gt;Literary Digest was a popular general-interest magazine, and starting in 1920 they began running straw polls to predict the outcomes of Presidential Elections. To make these predictions they would send ballots to lots of people, and then simply tally up the ballots that were returned; Literary Digest proudly reported that the ballots they received were neither “weighted, adjusted, nor interpreted.” This procedure correctly predicted the winner of the elections in 1920, 1924, 1928 and 1932. In 1936, in the midst of the Great Depression, Literary Digest sent out ballots to 10 million people, whose names predominately came from telephone directories and automobile registration records. Here’s how they described their methodology:&lt;/p&gt;

&lt;p&gt;“THE DIGEST’s smooth-running machine moves with the swift precision of thirty years’ experience to reduce guesswork to hard facts &amp;hellip; .This week 500 pens scratched out more than a quarter of a million addresses a day. Every day, in a great room high above motor-ribboned Fourth Avenue, in New York, 400 workers deftly slide a million pieces of printed matter—enough to pave forty city blocks—into the addressed envelops [sic]. Every hour, in THE DIGEST’S own Post Office Substation, three chattering postage metering machines sealed and stamped the white oblongs; skilled postal employees flipped them into bulging mailsacks; fleet DIGEST trucks sped them to express mail-trains &amp;hellip; Next week, the first answers from these ten million will begin the incoming tide of marked ballots, to be triple-checked, verified, five-times cross-classified and totaled. When the last figure has been totted and checked, if past experience is a criterion, the country will know to within a fraction of 1 percent the actual popular vote of forty million [voters].” (August 22, 1936)
The Digest’s fetishization of size is instantly recognizable to any “big data” researcher today. Of the 10 million ballots distributed, an amazing 2.4 million ballots were returned—that’s roughly 1,000 times larger than modern political polls. From these 2.4 million respondents the verdict was clear: Literary Digest predicted that the challenger Alf Landon was going to defeat the incumbent Franklin Roosevelt. But, in fact, the exact opposite happened. Roosevelt defeated Landon in a landslide. How could Literary Digest go wrong with so much data? Our modern understanding of sampling makes Literary Digest’s errors clear and helps us avoid making similar errors in the future.&lt;/p&gt;

&lt;p&gt;Thinking clearly about sampling requires us to consider four different groups of people (Figure 3.1). The first group of people is the target population; this is the group that the research defines as the population of interest. In the case of Literary Digest the target population was voters in the 1936 Presidential Election. After deciding on a target population, a researcher next needs to develop a list of people that can be used for sampling. This list is called a sampling frame and the population on the sampling frame is called the frame population. In the case of Literary Digest the frame population was the 10 million people whose names came predominately from telephone directories and automobile registration records. Ideally the target population and the frame population would be exactly the same, but in practice this is often not the case. Differences between the target population and frame population are called coverage error. Coverage error does not, by itself guarantee problems. But, if the people in the frame population are systematically different from people not in the frame population there will be coverage bias. Coverage error was the first of the major flaws with the Literary Digest poll. They wanted to learn about voters—that was their target population—but they constructed a sampling frame predominately from telephone directories and automobile registries, sources that over-represented wealthier Americans who were more likely to support Alf Landon (recall that both of these technologies, which are common today, were relatively new at the time and that the US was in the midst of the Great Depression).&lt;/p&gt;

&lt;p&gt;Figure 3.1: Representation errors.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/21069033ac1b35779e0a1b3b7cc7000e.png&#34; alt=&#34;fig3.1&#34; /&gt;&lt;/p&gt;

&lt;p&gt;After defining the frame population, the next step is for a researcher to select the sample population; these are the people that the researcher will attempt to interview. If the sample has different characteristics than the frame population, then we can introduce sampling error. This is the kind of error quantified in the margin of error that usually accompanies estimates. In the case of the Literary Digest fiasco, there actually was no sample; they attempted to contact everyone in the frame population. Even though there was no sampling error, there was obviously still error. This clarifies that the margins of errors that are typically reported with estimates from surveys are usually misleadingly small; they don’t include all sources of error.&lt;/p&gt;

&lt;p&gt;Finally, a researcher attempts to interview everyone in the sample population. Those people that are successfully interviewed are called respondents. Ideally, the sample population and the respondents would be exactly the same, but in practice there is non-response. That is, people who are selected in the sample refuse to participate. If the people who respond are different from those who don’t respond, then there can be non-response bias. Non-response bias was the second main problem with the Literary Digest poll. Only 24% of the people who received a ballot responded, and it turned out that people who supported Landon were more likely to respond.&lt;/p&gt;

&lt;p&gt;Beyond just being an example to introduce the ideas of representation, the Literary Digest poll is an oft-repeated parable, cautioning researchers about the dangers of haphazard sampling. Unfortunately, I think that the lesson that many people draw from this story is the wrong one. The most common moral of the story is that researchers can’t learn anything from non-probability samples (i.e., samples without strict probability-based rules for selecting participants). But, as I’ll show later in this chapter, that’s not quite right. Instead, I think there are really two morals to this story; morals that are as true today as they were in 1936. First, a large amount of haphazardly collected data will not guarantee a good estimate. Second, researchers need to account for how their data was collected when they are making estimates from it. In other words, because the data collection process in the Literary Digest poll was systematically skewed toward some respondents, researchers need to use a more complex estimation process that weights some respondents more than others. Later in this chapter, I’ll show you one such weighting procedure—post-stratification—that can enable you to make better estimates with non-probability samples.&lt;/p&gt;

&lt;h3 id=&#34;3-3-2-measurement&#34;&gt;3.3.2 Measurement&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;Measurement is about making inferences from what your respondents say to what your respondents think and do.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The second category of the total survey error framework is measurement; it deals with how we can make inferences from the answers that respondents give to our questions. It turns out that the answers we receive, and therefore the inferences we make, can depend critically—and in sometimes surprising ways—on exactly how we ask. Perhaps nothing illustrates this important point better than a joke in the wonderful book Asking Questions by Norman Bradburn, Seymour Sudman, and Brian Wansink (2004):&lt;/p&gt;

&lt;p&gt;Two priests, a Dominican and a Jesuit, are discussing whether it is a sin to smoke and pray at the same time. After failing to reach a conclusion, each goes off to consult his respective superior. The Dominican says, “What did your superior say?”
  The Jesuit responds, “He said it was alright.”
  “That’s funny” the Dominican replies, “My supervisor said it was a sin.”
  The Jesuit said, “What did you ask him?” The Dominican replies, “I asked him if it was alright to smoke while praying.” “Oh” said the Jesuit, “I asked if it was OK to pray while smoking.”
  There are many examples of anomalies like the one experienced by the two priests. In fact, the very issue at the root of this joke has a name in the survey research community: question form effects (Kalton and Schuman 1982). To see how question form effects might impact real surveys, consider these two very similar looking survey questions:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;“How much do you agree with the following statement: Individuals are more to blame than social conditions for crime and lawlessness in this country.”&lt;/li&gt;
&lt;li&gt;“How much do you agree with the following statement: Social conditions are more to blame than individuals for crime and lawlessness in this country.”&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Although both questions appear to measure the same thing, they produced different results in a real survey experiment (Schuman and Presser 1996). When asked one way, about 60% of respondents reported that individuals were more to blame for crime, but when asked the other way about 60% reported that social conditions were more to blame (Figure 3.2). In other words, the small difference between the two questions could lead researchers to a different conclusion.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/db7d00bba31f46cb720768d8e699ff0e.png&#34; alt=&#34;fig3.2&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 3.2: Results from a survey experiment , Table 8.1(Schuman and Presser 1996, Table 8.1). Researchers can get different answers depending on exactly how they asked the question. This is an example of a question form effect (Kalton and Schuman 1982).&lt;/p&gt;

&lt;p&gt;In addition to the structure of the question, respondents also can give different answers based on the specific words used. For example, in order to measure opinions about governmental priorities, respondents were read the following prompt:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;“We are faced with many problems in this country, none of which can be solved easily or inexpensively. I’m going to name some of these problems, and for each one I’d like you to tell me whether you think we’re spending too much money on it, too little money, or about the right amount.”
Next, half of the respondents were asked about “welfare” and half were asked about “aid for the poor.” While these might seem like two different phrases for the same thing, they elicited very different results (Figure 3.3); Americans report being much more supportive of “aid to the poor” than “welfare” (Smith 1987; Rasinski 1989; Huber and Paris 2013). While survey researchers consider these wording effects to be anomalies, they could also consider them research findings. That is, we have learned something about public opinion from this result.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/b6a97a16e7a5c9f34d19e59601ebef3c.png&#34; alt=&#34;fig3.3&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 3.3: Results from Huber and Paris (2013). Respondents are much more supportive of aid to the poor than welfare. This is an example of a question wording effect whereby the answers that researchers receive depend on exactly which words they use in their questions.&lt;/p&gt;

&lt;p&gt;As these examples about question form effects and wording effects show, the answers that researchers receive can be influenced in subtle ways based on how they ask their questions. This does not mean that surveys should not be used; often there is no choice. Rather, the examples illustrate that we should construct our questions carefully and we should not accept responses uncritically.&lt;/p&gt;

&lt;p&gt;Most concretely, this means that if you are analyzing survey data collected by someone else, make sure that you have read the actual questionnaire. And, if you are creating your own questionnaire, I have three suggestions. First, I suggest you read more about questionnaire design (e.g., Bradburn, Sudman, and Wansink (2004)); there is more to it than I’ve been able to describe here. Second, I suggest that you copy—word for word—questions from high-quality surveys. Although this sounds like plagiarism, copying questions is encouraged in survey research (as long as you cite the original survey). If you copy questions from high-quality surveys, you can be sure that they have been tested and you can compare the responses to your survey to responses from some other survey. Finally, I suggest you pre-test your questions with some people from your frame population (Presser et al. 2004); my experience is that pre-testing always reveals surprising issues.&lt;/p&gt;

&lt;h3 id=&#34;3-3-3-cost&#34;&gt;3.3.3 Cost&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;Surveys are not free, and this is a real constraint.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So far, I’ve briefly reviewed the total survey error framework, which itself is the subject of book length treatments (Weisberg 2005; Groves et al. 2009). Although this framework is comprehensive, it generally causes researchers to leave out an important factor: cost. Although cost—which can be measured by either time or money—is rarely explicitly discussed by academic researchers, it is a real constraint that we ignore at our peril. In fact, the reason researchers interview samples of people rather than entire population is to save money. Thus, cost is fundamental to the process of survey research (Groves 2004). A single-minded devotion to minimizing error while completely ignoring costs is not always in our best interest.&lt;/p&gt;

&lt;p&gt;The limitations of an obsession with reducing error are illustrated by the landmark study of Scott Keeter and colleagues (2000) on the effects of expensive field operations in order to reduce non-response in telephone surveys . Keeter and colleagues ran two simultaneous surveys, one using “Standard” procedures and one using “Rigorous” procedures. Although the “Rigorous” procedures did produce a lower rate of non-response, estimates from both samples were basically the same. However, the “Rigorous” procedures cost roughly twice as much and took 8 times as long. Are we better off with 2 reasonable surveys or 1 pristine survey? What about 10 reasonable surveys or 1 pristine survey? What about 100 reasonable surveys or 1 pristine survey? At some point cost advantages must outweigh vague, non-specific concerns about quality.&lt;/p&gt;

&lt;p&gt;Many of the opportunities created by the digital age are not about creating estimates that obviously have lower error. Rather, these opportunities are about creating estimates cheaper and faster, but perhaps with errors that are currently higher or harder to measure. As many of the examples in this chapter will show, researchers who insist on a single-minded obsession with minimizing error at the expense of other dimensions of quality are going to miss out on exciting opportunities. Given this background about the total survey error framework, we will now turn to three main areas of the third era of survey research: new approaches to representation (Section 3.4), new approaches to measurement (Section 3.5), and new strategies for combining surveys with digital traces (Section 3.6).&lt;/p&gt;

&lt;h2 id=&#34;3-4-who-to-ask&#34;&gt;3.4 Who to ask&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Probability samples and non-probability samples are not that different in practice; in both cases, it’s all about the weights.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Sampling is fundamental to survey research. Researchers almost never ask their questions to everyone in their target population. In this regard, surveys are not unique. Most research, in one way or another, involves sampling. Sometimes this sampling is done explicitly by the researcher; other times it happens implicitly. For example, a researcher that runs a laboratory experiment on undergraduate students in her university has also taken a sample. Thus, sampling is a problem that comes up throughout this book. In fact, one of the most common concerns that I hear about digital age sources of data is “they are not representative.” As we will see in this Section, this concern is both less serious and more subtle than many skeptics realize. In fact, I will argue that the whole concept of “representativeness” is not helpful for thinking about probability and non-probability samples. Instead, the key is to think about how the data was collected and how any biases in that data collection can be undone when making estimates.&lt;/p&gt;

&lt;p&gt;Currently, the dominant theoretical approach to representation is probability sampling. When data are collected with a probability sampling method that has been perfectly executed, researchers are able to weight their data based on the way that they were collected to make unbiased estimates about the target population. However, perfect probability sampling basically never happens in the real world. There are typically two main problems 1) differences between the target population and the frame population and 2) non-response (these are exactly the problems that wrecked the Literary Digest poll). Thus, rather than thinking of probability sampling as a realistic model of what actually happens in the world, it is better to think of probability sampling as a helpful, abstract model, much like the way physicists think about a frictionless ball rolling down an infinitely long ramp.&lt;/p&gt;

&lt;p&gt;The alternative to probability sampling is non-probability sampling. The main difference between probability and non-probability sampling is that with probability sampling everyone in the population has a known probability of inclusion. There are, in fact, many varieties of non-probability sampling, and these methods of data collection are becoming increasingly common in the digital age. But, non-probability sampling has a terrible reputation among social scientists and statisticians. In fact, non-probability sampling is associated with some of the most dramatic failures of survey researchers, such as the Literary Digest fiasco (discussed earlier) and the incorrect prediction about the US presidential elections of 1948 (“Dewey Defeats Truman”) (Mosteller 1949; Bean 1950; Freedman, Pisani, and Purves 2007).&lt;/p&gt;

&lt;p&gt;However, the time is right to reconsider non-probability sampling for two reasons. First, as probability samples have become increasingly difficult to do in practice, the line between probability samples and non-probability samples is blurring. When there are high rates of non-response (as there are in real surveys now), the actual probability of inclusions for respondents are not known, and thus, probability samples and non-probability samples are not as different as many researchers believe. In fact, as we will see below, both approaches basically rely on the same estimation method: post-stratification. Second, there have been many developments in the collection and analysis of non-probability samples. These methods are different enough from the methods that caused problems in the past that I think it makes sense to think of them as “non-probability sampling 2.0.” We should not have an irrational aversion to non-probability methods because of errors that happened a long time ago.&lt;/p&gt;

&lt;p&gt;Next, in order to make this argument more concrete, I’ll review standard probability sampling and weighting (Section 3.4.1). The key idea is that how you collected your data should impact how you make estimates. In particular, if everyone does not have the same probability of inclusion, then everyone should not have the same weight. In other words, if your sampling is not democratic, then your estimations should not be democratic. After reviewing weighting, I’ll describe two approaches to non-probability sampling: one that focuses on weighting to deal with the problem of haphazardly collected data (Section 3.4.2), and one that tries to place more control over how the data is collected (Section 3.4.3). The arguments in the main text will be explained below with words and pictures; readers who would like a more mathematical treatment should also see the technical appendix.&lt;/p&gt;

&lt;h3 id=&#34;3-4-1-probability-sampling-data-collection-and-data-analysis&#34;&gt;3.4.1 Probability sampling: data collection and data analysis&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;Weights can undo distortions intentionally caused by the sampling process.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Probability samples are those where all people have a known, non-zero probability of inclusion, and the simplest probability sampling design is simple random sampling where each person has equal probability of inclusion. When respondents are selected via simple random sampling with perfect execution (e.g., no coverage error and no non-response), then estimation is straightforward because the sample will—on average—be a miniature version of the population.&lt;/p&gt;

&lt;p&gt;Simple random sampling is rarely used in practice, however. Rather, researchers intentionally select people with unequal probabilities of inclusion in order to reduce cost and increase accuracy. When researchers intentionally select people with different probabilities of inclusion, then adjustments are needed to undo the distortions caused by the sampling process. In other words, how we generalize from a sample depends on how the sample was selected.&lt;/p&gt;

&lt;p&gt;For example, the Current Population Survey (CPS) is used by the US government to estimate the unemployment rate. Each month about 100,000 people are interviewed, either face-to-face or over the telephone, and the results are used to produce the estimated unemployment rate. Because the government wishes to estimate the unemployment rate in each state, it cannot do a simple random sample of adults because that would yield too few respondents in states with small populations (e.g., Rhode Island) and too many from states with large populations (e.g., California). Instead, the CPS samples people in different states at different rates, a process called stratified sampling with unequal probability of selection. For example, if the CPS wanted 2,000 respondents per state, then adults in Rhode Island would have about 30 times higher probability of inclusion than adults in California (Rhode Island: 2,000 respondents per 800,000 adults vs California: 2,000 respondents per 30,000,000 adults). As we will see later, this kind of sampling with unequal probability happens with online sources of data too, but unlike the CPS, the sampling mechanism is usually not known or controlled by the researcher.&lt;/p&gt;

&lt;p&gt;Given its sampling design, the CPS is not directly representative of the US; it includes too many people from Rhode Island and too few from California. Therefore, it would be unwise to estimate the unemployment rate in the country with the unemployment rate in the sample. Instead of the sample mean, it is better to take a weighted mean, where the weights account for the fact that people from Rhode Island were more likely to be included than people from California. For example, each person from California would be upweighted— they would count more in the estimate—and each person from Rhode Island would be downweighted—they would count less in the estimate. In essence, you are given more voice to people that you are less likely to learn about.&lt;/p&gt;

&lt;p&gt;This toy example illustrates an important but commonly misunderstood point: a sample does not need to be a miniature version of the population in order to produce good estimates. If enough is known about how the data was collected, then that information can be used when making estimates from the sample. The approach I’ve just described—and that I describe mathematically in the technical appendix—falls squarely within the classical probability sampling framework. Now, I’ll show how that same idea can be applied to non-probability samples.&lt;/p&gt;

&lt;h3 id=&#34;3-4-2-non-probability-samples-weighting&#34;&gt;3.4.2 Non-probability samples: weighting&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;With non-probability samples, weights can undo distortions caused by the assumed sampling process.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In the same way that researchers weight responses from probability samples, they can also weight responses from non-probability samples. For example, as an alternative to the CPS, imagine that you placed banner ads on thousands of websites to recruit participants for a survey to estimate the unemployment rate. Naturally, you would be skeptical that the simple mean of your sample would be a good estimate of the unemployment rate. Your skepticism is probably because you think that some people are more likely to complete your survey than others. For example, people who don’t spend a lot of time on the web are less likely to complete your survey.&lt;/p&gt;

&lt;p&gt;As we saw in the last section, however, if we know how the sample was selected—as we do with probability samples—then we can undo distortions caused by the sampling process. Unfortunately, when working with non-probability samples, we don’t know how the sample was selected. But, we can make assumptions about the sampling process and then apply weighting in the same way. If these assumptions are correct, then the weighting will undo the distortions caused by the sampling process.&lt;/p&gt;

&lt;p&gt;For example, imagine that in response to your banner ads, you recruited 100,000 respondents. However, you don’t believe that these 100,000 respondents are a simple random sample of American adults. In fact, when you compare your respondents to the US population, you find that people from some states (e.g., New York) are over-represented and that people from some states (e.g., Alaska) are under-represented. Thus, the unemployment rate of your sample is likely to be a bad estimate of the unemployment rate in the target population.&lt;/p&gt;

&lt;p&gt;One way to undo the distortion that happened in the sampling process is to assign weights to each person; lower weights to people from states that are over-represented in the sample (e.g., New York) and higher weights to people from states that are under-represented in the sample (e.g., Alaska). More specifically, the weight for each respondent is related to their prevalence in your sample relative to their prevalence in the US population. This weighting procedure is called post-stratification, and the idea of weighing should remind you of the example in Section 3.4.1 where respondents from Rhode Island were given less weight than respondents from California. Post-stratification requires that you know enough to put your respondents into groups and to know the proportion of the target population in each group.&lt;/p&gt;

&lt;p&gt;Although the weighting of the probability sample and of the non-probability sample are the same mathematically (see technical appendix), they work well in different situations. If the researcher has a perfect probability sample (i.e., no coverage error and no non-response), then weighting will produce unbiased estimates for all traits in all cases. This strong theoretical guarantee is why advocates of probability samples find them so attractive. On the other hand, weighting non-probability samples will only produce unbiased estimates for all traits if the response propensities are the same for everyone in each group. In other words, thinking back to our example, using post-stratification will produce unbiased estimates if everyone in New York has the same probability of participating and everyone in Alaska has the same probability of participating and so on. This assumption is called the homogeneous-response-propensities-within-groups assumption, and it plays a key role in knowing if post-stratification will work well with non-probability samples.&lt;/p&gt;

&lt;p&gt;Unfortunately, in our example, the homogeneous-response-propensities-within-groups assumption seems unlikely to be true. That is, it seems unlikely that everyone in Alaska has the same probability of being in your survey. But, there are three important points to keep in mind about post-stratification, all of which make it seem more promising.&lt;/p&gt;

&lt;p&gt;First, homogeneous-response-propensities-within-groups assumption becomes more plausible as the number of groups increases. And, researchers are not limited to groups just based on a single geographic dimension. For example, we could create groups based on state, age, sex, and level of education. It seems more plausible that there is homogeneous response propensities within the group of 18-29, female, college graduates living in Alaska than within the group of all people living in Alaska. Thus, as the number of groups used for post-stratification increases, the assumptions needed to support it become more reasonable. Given this fact, it seems like a researchers would want to create a huge number of groups for post-stratification. But, as the number of groups increases, researchers run into a different problem: data sparsity. If there are only a small number of people in each group, then the estimates will be more uncertain, and in the extreme case where there is a group that has no respondents, then post-stratification completely breaks down. There are two ways out of this inherent tension between the plausibility of homogeneous- response-propensity-within-groups assumption and the demand for reasonable sample sizes in each group. One approach is to move to a more sophisticated statistical model for calculating weights and the other is to collect a larger, more diverse sample, which helps ensure reasonable sample sizes in each group. And, sometimes researchers do both, as I’ll describe in more detail below.&lt;/p&gt;

&lt;p&gt;A second consideration when working with post-stratification from non-probability samples is that the homogeneous-response-propensity-within-groups assumption is already frequently made when analyzing probability samples. The reason that this assumption is needed for probability samples in practice is that probability samples have non-response, and the most common method for adjusting for non-response is post-stratification as described above. Of course, just because many researchers make a certain assumption doesn’t mean that you should do it too. But, it does mean that when comparing non-probability samples to probability samples in practice, we must keep in mind that both depend on assumptions and auxiliary information in order to produce estimates. In most realistic settings, there is simply no assumption-free approach to inference.&lt;/p&gt;

&lt;p&gt;Finally, if you care about one estimate in particular—in our example unemployment rate—then you need a condition weaker than homogeneous-response-propensity-within-groups assumption. Specifically, you don’t need to assume that everyone has the same response propensity, you only need to assume that there is no correlation between response propensity and unemployment rate within each group. Of course, even this weaker condition will not hold in some situations. For example, imagine estimating the proportion of Americans that do volunteer work. If people who do volunteer work are more likely to agree to be in a survey, then researchers will systematically over-estimate the amount of volunteering, even if they do post-stratification adjustments, a result that has been demonstrated empirically by Abraham, Helms, and Presser (2009).&lt;/p&gt;

&lt;p&gt;As I said earlier, non-probability samples are viewed with great skepticism by social scientists, in part because of their role in some of the most embarrassing failures in the early days of survey research. A clear example of how far we have come with non-probability samples is the research of Wei Wang, David Rothschild, Sharad Goel, and Andrew Gelman that correctly recovered the outcome of the 2012 US election using a non-probability sample of American Xbox users—a decidedly non-random sample of Americans (Wang et al. 2015). The researchers recruited respondents from the XBox gaming system, and as you might expect, the Xbox sample skewed male and skewed young: 18 - 29 year olds make up 19% of the electorate but 65% of the Xbox sample and men make up 47% of the electorate and 93% of the Xbox sample (Figure 3.4). Because of these strong demographic biases, the raw Xbox data was a poor indicator of election returns. It predicted a strong victory for Mitt Romney over Barack Obama. Again, this is another example of the dangers of raw, unadjusted non-probability samples and is reminiscent of the Literary Digest fiasco.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/e5c8d32a576e139c045eb6604302f8e1.png&#34; alt=&#34;fig3.4&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 3.4: Demographics of respondents in Wang et al. (2015). Because respondents were recruited from XBox, they were more likely to be young and more likely to be male, relative to voters in the 2012 election.&lt;/p&gt;

&lt;p&gt;However, Wang and colleagues were aware of these problems and attempted to weight the respondents to correct for the sampling process. In particular, they used a more sophisticated form of the post-stratification I told you about. It is worth learning a bit more about their approach because it builds intuition about post-stratification, and the particular version Wang and colleagues used is one of the most exciting approaches to weighting non-probability samples.&lt;/p&gt;

&lt;p&gt;In our simple example about estimating unemployment in Section 3.4.1, we divided the population into groups based on state of residence. In contrast, Wang and colleagues divided the population into into 176,256 groups defined by: gender (2 categories), race (4 categories), age (4 categories), education (4 categories), state (51 categories), party ID (3 categories), ideology (3 categories) and 2008 vote (3 categories). With more groups, the researchers hoped that it would be increasingly likely that within each group, response propensity was uncorrelated with support for Obama. Next, rather than constructing individual-level weights, as we did in our example, Wang and colleagues used a complex model to estimate the proportion of people in each group that would vote for Obama. Finally, they combined these group estimates of support with the known size of each group to produce an estimated overall level of support. In other words, they chopped up the population into different groups, estimated the support for Obama in each group, and then took a weighted average of the group estimates to produce an overall estimate.&lt;/p&gt;

&lt;p&gt;Thus, the big challenge in their approach is to estimate the support for Obama in each of these 176,256 groups. Although their panel included 345,858 unique participants, a huge number by the standards of election polling, there were many, many groups for which Wang and colleagues had almost no respondents. Therefore, to estimate the support in each group they used a technique called multilevel regression with post-stratification, which researchers affectionately call Mr. P. Essentially, to estimate the support for Obama within a specific group, Mr. P. pools information from many closely related groups. For example, consider the challenge of estimating the support for Obama among female, Hispanics, between 18-29 years old, who are college graduates, who are registered Democrats, who self-identify as moderates, and who voted for Obama in 2008. This is a very, very specific group, and it is possible that there is nobody in the sample with these characteristics. Therefore, to make estimates about this group, Mr. P. pools together estimates from people in very similar groups.&lt;/p&gt;

&lt;p&gt;Using this analysis strategy, Wang and colleagues were able to use the XBox non-probability sample to very closely estimate the overall support that Obama received in the 2012 election (Figure 3.5). In fact their estimates were more accurate than an aggregate of public opinion polls. Thus, in this case, weighting—specifically Mr. P.—seems to do a good job correcting the biases in non-probability data; biases that are visible when you look at the estimates from the unadjusted Xbox data.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/1511f22180cef2f8ac3f7e68de4e1ec1.png&#34; alt=&#34;fig3.5&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 3.5: Estimates from Wang et al. (2015). Unadjusted XBox sample produced inaccurate estimates. But, the weighted XBox sample produced estimates that were more accurate than an average of probability-based telephone surveys.&lt;/p&gt;

&lt;p&gt;There are two main lessons from the study of Wang and colleagues. First, unadjusted non-probability samples can lead to bad estimates; this is a lesson that many researchers have heard before. However, the second lesson is that non-probability samples, when weighted properly, can actually produce quite good estimates. In fact, their estimates were more accurate than the estimates from pollster.com, an aggregation of more traditional election polls.&lt;/p&gt;

&lt;p&gt;Finally, there are important limitations to what we can learn from this one specific study. Just because post-stratification worked well in this particular case, there is no guarantee that it will work well in other cases. In fact, elections are perhaps one of the easiest settings because pollsters have been studying elections for almost 100 years, there is regular feedback (we can see who wins the elections), and party identification and demographic characteristics are relatively predictive of voting. At this point, we lack solid theory and empirical experience to know when weighting adjustments to non-probability samples will produce sufficiently accurate estimates. One thing that is clear, however, is if you are forced to work with non-probability samples, then there is strong reason to believe that adjusted estimates will be better than non-adjusted estimates.&lt;/p&gt;

&lt;h3 id=&#34;3-4-3-non-probability-samples-sample-matching&#34;&gt;3.4.3 Non-probability samples: sample matching&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;Not all non-probability samples are the same. We can add more control on the front end.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The approach Wang and colleagues used to estimate the outcome of the 2012 US presidential election depended entirely on improvements in data analysis. That is, they collected as many responses as they could and then attempted to re-weight them. A complementary strategy for working with non-probability sampling is to have more control over the data collection process.&lt;/p&gt;

&lt;p&gt;The simplest example of a partially controlled non-probability sampling process is quota sampling, a technique that goes back to the early days of survey research. In quota sampling, researchers divide the population into different groups (e.g., young men, young women, etc) and then set quotas for the number of people to be selected in each group. Respondents are selected in a haphazard manner until the researcher has met their quota in each group. Because of the quotas, the resulting sample looks more like the target population than would be true otherwise, but because the probabilities of inclusion are unknown many researchers are skeptical of quota sampling. In fact, quota sampling was a cause of the “Dewey Defeats Truman” error in the 1948 US Presidential polls. Because it provides some control over the sampling process, however, one can see how quota sampling might have some advantages over a completely uncontrolled data collection.&lt;/p&gt;

&lt;p&gt;Moving beyond quota sampling, more modern approaches to controlling the non-probability sampling process are now possible. One such approach is called sample matching, and it is used by some commercial online panel providers. In its simplest form, sample matching requires two data sources: 1) a complete register of the population and 2) a large panel of volunteers. It is important that the volunteers do not need to be a probability sample from any population; to emphasize that there are no requirements for selection into the panel, I’ll call it a dirty panel. Also, both the population register and the dirty panel must include some auxiliary information about each person, in this example, I’ll consider age and sex, but in realistic situations this auxiliary information could be much more detailed. The trick of sample matching is to select samples from a dirty panel in a way that produces samples that look like probability samples.&lt;/p&gt;

&lt;p&gt;Sample matching begins when a simulated probability sample is taken from the population register; this simulated sample becomes a target sample. Then, based on the auxiliary information, cases in the target sample are matched to people in the dirty panel to form a matched sample. For example, if there is a 25 year old female in the target sample, then the researcher finds a 25 year old female from the dirty panel to be in the matched sample. Finally, members of the matched sample are interviewed to produce the final set of respondents.&lt;/p&gt;

&lt;p&gt;Even though the matched sample looks like the target sample, it is important to remember that the matched sample is not a probability sample. Matched samples can only match the target sample on the known auxiliary information (e.g., age and sex), but not on unmeasured characteristics. For example, if people on the dirty panel tend to be poorer—after all, one reason to join a survey panel is to earn money—then even if the matched sample looks like the target sample in terms of age and sex it will still have a bias toward poor people. The magic of true probability sampling is to rule out problems on both measured and unmeasured characteristics (a point that is consistent with our discussion of matching for causal inference from observational studies in Chapter 2).&lt;/p&gt;

&lt;p&gt;In practice, sample matching depends on having a large and diverse panel eager to complete surveys, and thus it is mainly done by companies that can afford to develop and maintain such a panel. Also, in practice, there can be problems with matching (sometimes a good match for someone in the target sample does not exist on the panel) and non-response (sometimes people in the matched sample refuse to participate in the survey). Therefore, in practice, researchers doing sample matching also perform some kind of post-stratification adjustment to make estimates.&lt;/p&gt;

&lt;p&gt;It is hard to provide useful theoretical guarantees about sample matching, but in practice it can perform well. For example, Stephen Ansolabehere and Brian Schaffner (2014) compared three parallel surveys of about 1,000 people conducted in 2010 using three different sampling and interviewing methods: mail, telephone, and an Internet panel using sample matching and post-stratification adjustment. The estimates from the three approaches were quite similar to estimates from high-quality benchmarks such as the Current Population Survey (CPS) and the National Health Interview Survey (NHIS). More specifically, both the Internet and mail surveys were off by an average of 3 percentage points and the phone survey was off by 4 percentage points. Errors this large are approximately what one would expect from samples of about 1,000 people. Although, none of these modes produced substantially better data, both the Internet and phone survey (which took days or weeks) were substantially faster to field than the mail survey (which took eight months), and the Internet survey, which used sample matching, was cheaper than the other two modes.&lt;/p&gt;

&lt;p&gt;In conclusion, social scientists and statisticians are incredibly skeptical of inferences from these non-probability samples, in part because they are associated with some embarrassing failures of survey research such as the Literary Digest poll. In part, I agree with this skepticism: unadjusted non-probability samples are likely to produce bad estimates. However, if researchers can adjust for the biases in the sampling process (e.g., post-stratification) or control the sampling process somewhat (e.g., sample matching), they can produce better estimates, and even estimates of sufficient quality for most purposes. Of course, it would be better to do perfectly executed probability sampling, but that no longer appears to be a realistic option.&lt;/p&gt;

&lt;p&gt;Both non-probability samples and probability samples vary in their quality, and currently it is likely the case that most estimates from probability samples are more trustworthy than estimates from non-probability samples. But, even now, estimates from well-conducted non-probability samples are probably better than estimates from poorly-conducted probability samples. Further, non-probability samples are substantially cheaper. Thus, it appears that probability vs non-probability sampling offers a cost-quality trade-off (Figure 3.6). Looking forward, I expect that estimates from well-done non-probability samples will become cheaper and better. Further, because of the breakdown in landline telephone surveys and increasing rates of non-response, I expect that probability samples will become more expensive and of lower quality. Because of these long-term trends, I think that non-probability sampling will become increasingly important in the third era of survey research.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/a7a8e0fba488e6d71493de7a492d1bfe.png&#34; alt=&#34;fig3.6&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 3.6: Probability sampling in practice and non-probability sampling are both large, heterogeneous categories. In general, there is a cost-error trade-off with non-probability sampling being lower cost but higher error. However, well-done non-probability sampling can produce better estimates than poorly-done probability sampling. In the future, I expect that non-probability sampling will get better and cheaper while probability sampling will get worse and more expensive.&lt;/p&gt;

&lt;h2 id=&#34;3-5-new-ways-of-asking-questions&#34;&gt;3.5 New ways of asking questions&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Traditional surveys are closed, boring, and removed from life. Now we can ask questions more embedded in life, more open, and more fun.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The total survey error framework encourages researchers to think about survey research as a two part process: recruiting respondents and asking them questions. In the previous section I discussed how the digital age changes how we recruit respondents, and now I’ll discuss how the digital age enables new ways to ask questions. These new approaches can be used with either probability samples or non-probability samples.&lt;/p&gt;

&lt;p&gt;A survey mode is the environment in which the questions are asked, and it can have important impacts on measurement (Couper 2011). In the first era of survey research the most common mode was face-to-face, and in the second era the most common mode was telephone. Many researchers view the third era of survey research as just an expansion of survey modes to include computers and mobile phones. However, the digital age is more than just a change in the pipes through which questions and answers flow. Instead, the transition from analogue to digital enables—and will likely require—researchers to change how we ask.&lt;/p&gt;

&lt;p&gt;A study by Michael Schober and colleagues illustrates the advantages of adjusting our asking to the capabilities and social norms around new technologies (Schober et al. 2015). In the study, Schober and colleagues compared different approaches for asking people questions via a mobile phone. They compared voice conversations, which would have been a natural translation of second era approaches, to collecting data via many text messages, an approach with no obvious precedent. Schober and colleagues found that texting led to higher quality data than voice interviews. In other words, simply transferring old approaches onto new technologies was not the best approach. Rather, researchers will have to customize our ways of asking to these new platforms.&lt;/p&gt;

&lt;p&gt;There are many dimensions along which researchers can categorize survey modes, but the most critical feature of digital age survey modes is that they are computer-administered, rather than interviewer-administered (as in telephone and face-to-face surveys). Taking human interviewers out of the data collection process offers enormous benefits and introduces some drawbacks. In terms of benefits, removing interviewers dramatically reduces costs—interviews are one of the biggest expenses in survey research—and increases flexibility; respondents can participate when they want, not only when an interviewer is available. However, removing the interviewer also limits surveys in some ways. In particular, interviewers are critical to encouraging respondents to participate and keeping them engaged while slogging through long and occasionally tedious surveys.&lt;/p&gt;

&lt;p&gt;Next, I’ll describe two approaches showing how researchers can take advantage of the tools of the digital age to ask questions differently: measuring internal states at a more appropriate time and place through ecological momentary assessment (Section 3.5.1) and combining the strengths of open-ended and close-ended survey questions through wiki surveys (Section 3.5.2). However, the move toward computer-administered, ubiquitous asking will also mean that we need to design ways of asking that are more enjoyable for participants, a process called gamification (Section 3.5.3).&lt;/p&gt;

&lt;h3 id=&#34;3-5-1-ecological-momentary-assessments&#34;&gt;3.5.1 Ecological momentary assessments&lt;/h3&gt;

&lt;p&gt;Researchers can chop up big surveys and sprinkle them into peoples’ lives.
Ecological momentary assessments (EMA) involves taking traditional surveys, chopping them up into pieces, and sprinkling them into the lives of participants. Thus, survey questions can be asked at an appropriate time and place, rather than in a long interview weeks after the events have occurred.&lt;/p&gt;

&lt;p&gt;EMA is characterized by four features: (1) collection of data in real-world environments; (2) assessments that focus on individuals’ current or very recent states or behaviors; (3) assessments that may be event-based, time-based, or randomly prompted (depending on the research question); and (4) completion of multiple assessments over time (Stone and Shiffman 1994). EMA is an approach to asking that is greatly facilitated by smart phones that people interact with frequently throughout the day. Further, because smart phones are packed with sensors—such as GPS and accelerometers—it is increasingly possible to trigger measurements based on activity. For example, a smart phone could be programmed to trigger a survey question if a respondent goes into a particular neighborhood.&lt;/p&gt;

&lt;p&gt;The promise of EMA is nicely illustrated by the dissertation research of Naomi Sugie. Since the 1970s the United States has dramatically increased the number of people that it imprisons. As of 2005, about 500 in every 100,000 Americans were in prison, a rate of incarceration higher than anywhere else in the world (Wakefield and Uggen 2010). The surge in the number of people entering prison has also produced a surge in the number of people leaving prison; about 700,000 people leave prison each year (Wakefield and Uggen 2010). These ex-offenders face severe challenges upon leaving prison, and unfortunately many end up back in prison. In order to understand and reduce recidivism, social scientists and policy makers need to understand the experience of ex-offenders as they re-enter society. However, these data are hard to collect with standard survey methods because ex-offenders tend to be difficult to study and their lives are extremely unstable. Measurement approaches that deploy surveys every few months miss enormous amounts of the dynamics in their lives (Sugie 2016).&lt;/p&gt;

&lt;p&gt;In order to study the re-entry process of ex-offenders with much greater precision, Sugie took a standard probability sample of 131 people from the complete list of individuals leaving prison in Newark, New Jersey. She provided each participant with a smart phone that became a rich data collection platform. Sugie used the phones to administer two kinds of surveys. First, she sent an “experience sampling survey” at a randomly selected time between 9am and 6pm asking participants about their current activities and feelings. Second, at 7pm, she sent a “daily survey” asking about all the activities of that day. Together these two surveys provide detailed, longitudinal data about the lives of these ex-offenders.&lt;/p&gt;

&lt;p&gt;In addition to these surveys, the phones recorded their geographic location at regular intervals and kept encrypted records of call and text meta-data. All of this data collection, particularly the passive data collection, raises some ethical questions, but Sugie’s design handled them well. Sugie received meaningful informed consent from each participant for this data collection, used appropriate security protections, and enabled participants to turn off the geographic tracking. Further, in order to minimize the risk of forced disclosure of data (e.g., a subpoena from the police), Sugie obtained a Certificate of Confidentiality from the federal government before any data was collected (Beskow, Dame, and Costello 2008; Wolf et al. 2012). Sugie’s procedures were reviewed by a third-party (her university’s Institutional Review Board), and they went far beyond what is required by existing regulations. As such, I think her work provides a valuable model for other researchers facing these same challenges; see Sugie (2014) and Sugie (2016) for a more detailed discussion.&lt;/p&gt;

&lt;p&gt;The ability to secure and hold a stable job is important for a successful reentry process. However, Sugie found that her participants’ work experiences were informal, temporary, and sporadic. Further, within her participant pool, there were four distinct patterns: “early exit” (those who start searching for work but then drop out of the labor market), “persistent search” (those who spend much of the period searching for work), “recurring work” (those who spend much of the period working), and “low response” (those who do not respond to the surveys regularly). Further, Sugie wanted to understand more about the people who stop searching for jobs. One possibility is that these searchers become discouraged and depressed and eventually drop out of the labor market. Aware of this possibility, Sugie used her surveys to collect data about the emotional state of participants, and she found that the “early exit” group did not report higher levels of stress or unhappiness. Rather, the opposite was the case: those who continued to search for work reported more feelings of emotional distress. All of this fine-grained, longitudinal detail about the behavior and emotional state of the ex-offenders is important for understanding the barriers they face and easing their transition back into society. Further, all of this fine-grained detail would have been missed in a standard survey.&lt;/p&gt;

&lt;p&gt;There are three general lessons from Sugie’s work. First, new approaches to asking are completely compatible with traditional methods of sampling; recall, that Sugie took a standard probability sample from a well-defined frame population. Second, high-frequency, longitudinal measurements can be particularly valuable for studying social experiences that are irregular and dynamic. Third, when survey data collection is combined with digital traces, additional ethical issues can arise. I’ll treat research ethics in more detail in Chapter 6, but Sugie’s work shows that these issues are addressable by conscientious and thoughtful researchers.&lt;/p&gt;

&lt;h3 id=&#34;3-5-2-wiki-surveys&#34;&gt;3.5.2 Wiki surveys&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;Wiki surveys enable new hybrids of closed and open questions.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In addition to asking questions at more natural times and in more natural contexts, new technology also allows us to change the form of the questions. Most survey questions are closed, where respondents choose from a set choices written by researchers. It’s a process that one prominent survey researcher calls “putting words in people’s mouths.” For example, here’s a closed survey question:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;“This next question is on the subject of work. Would you please look at this card and tell me which thing on this list you would most prefer in a job?”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ol&gt;
&lt;li&gt;High income;&lt;/li&gt;
&lt;li&gt;No danger of being fired;&lt;/li&gt;
&lt;li&gt;Working hours are short, lots of free time;&lt;/li&gt;
&lt;li&gt;Chances for advancement;&lt;/li&gt;
&lt;li&gt;The work is important, and gives a feeling of accomplishment.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Now here’s the same question asked in an open form:&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;“This next question is on the subject of work. People look for different things in a job. What would you most prefer in a job?”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Although these two questions appear quite similar, a survey experiment by Howard Schuman and Stanley Presser (1979) revealed that they can produce very different results: nearly 60% of the open results fall outside of the categories in the closed responses (Figure 3.7).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/3aa8645aa30003c130d9506ffa374790.png&#34; alt=&#34;fig3.7&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 3.7: Results from Schuman and Presser (1979). Responses are quite different depending on whether the question is asked in closed or open form.&lt;/p&gt;

&lt;p&gt;Although open and closed questions can yield quite different information and both were popular in the early days of survey research, closed questions have come to dominate the field. This domination is not because closed questions have been proven to provide better measurement, rather it is because they are much easier to use; the process of coding open-ended questions is complicated and expensive. The move away from open questions is unfortunate because it is precisely the information that the researcher did not know ahead of time that can be the most valuable.&lt;/p&gt;

&lt;p&gt;In some research I’ve done with Karen Levy, we tried to create a new kind of survey question that combines the best features of both open and closed questions (Salganik and Levy 2015). That is, it enables researchers to learn new information as in an open question, and it yields easy to analyze data as in a closed question. Inspired by online systems driven by user-generated content, of which Wikipedia is an exemplar, we called our system a wiki survey. By combing the characteristics of Wikipedia and a traditional survey, we hope to create a new way of asking questions.&lt;/p&gt;

&lt;p&gt;The data collection process in a wiki survey is illustrated by a project we did with the New York City Mayor’s Office in order to integrate residents’ ideas into PlaNYC 2030, New York’s citywide sustainability plan. To begin the process, the Mayor’s Office generated a list of 25 ideas based on their previous outreach (e.g., “Require all big buildings to make certain energy efficiency upgrades,” “Teach kids about green issues as part of school curriculum”). Using these 25 ideas as “seeds,” the Mayor’s Office asked the question “Which do you think is a better idea for creating a greener, greater New York City?” Respondents were presented with a pair of ideas (e.g., “Open schoolyards across the city as public playgrounds” and “Increase targeted tree plantings in neighborhoods with high asthma rates”), and asked to choose between them (Figure 3.8). After choosing, respondents were immediately presented with another randomly selected pair of ideas. Respondents were able to continue contributing information about their preferences for as long as they wished by either voting or choosing “I can’t decide.” Crucially, at any point, respondents were able to contribute their own ideas, which—pending approval by the Mayor’s Office—became part of the pool of ideas to be presented to others. Thus, the questions that participants receive is both open and closed simultaneously.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/e106dd49688ff745fa1d114a8b5bca0c.png&#34; alt=&#34;fig3.8&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 3.8: Interface for a wiki survey (Salganik and Levy 2015).&lt;/p&gt;

&lt;p&gt;The Mayor’s Office launched its wiki survey in October 2010 in conjunction with a series of community meetings to obtain resident feedback. Over about four months, 1,436 respondents contributed 31,893 responses and 464 new ideas. Critically, 8 of the top 10 scoring ideas were uploaded by participants rather than part of the set of seed ideas from the Mayor’s Office. And, as we describe in our paper, this is a pattern in many wiki surveys. In other words, by being open to new information, researchers are able to learn things that would have been missed using more closed approaches to asking.&lt;/p&gt;

&lt;h3 id=&#34;3-5-3-gamification&#34;&gt;3.5.3 Gamification&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;Standard surveys are boring for participants, but that can and must change.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So far, I’ve told you about new approaches to asking that are facilitated by computer-administered interviews. However, one downside of computer-administered interviews is that there is no human interviewer to help induce participation. This is a problem because surveys are both time-consuming and boring. Therefore, in the future, survey designers are going to have to design around their participants and make the process of answering questions more enjoyable and game-like. This process is sometimes called gamification.&lt;/p&gt;

&lt;p&gt;To illustrate what a fun survey might look like, let’s consider Friendsense, a survey that was packaged as a game on Facebook. Sharad Goel, Winter Mason, and Duncan Watts (2010) wanted to estimate how much people think they are like their friends and how much they are actually like their friends. This question about real and perceived attitude similarity gets directly at people’s ability to accurately perceive their social environment and has implications for political polarization and the dynamics of social change. Conceptually, real and perceived attitude similarity is an easy thing to measure. The researchers could have just asked lots of people about their opinions and then asked their friends about their opinions (this allows for measurement of real attitude agreement) and they could have asked lots of people to guess their friends’ attitudes (this allows for measurement of perceived attitude agreement). Unfortunately, it is logistically very difficult to interview both a respondent and her friend. Therefore, Goel and colleagues turned their survey into a Facebook app that was fun to play.&lt;/p&gt;

&lt;p&gt;After a participant consented to be in a research study, the app selected a friend from the respondent’s Facebook account and asked a question about the attitude of that friend (Figure 3.9). Intermixed with questions about randomly selected friends, the respondent also answered questions about herself. After answering a question about a friend, the respondent was told whether her answer was correct or, if her friend had not answered, the respondent was able to encourage her friend to participate. Thus, the survey spread in part through viral recruitment.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/d927d36b179209e6790a59c04934fe41.png&#34; alt=&#34;fig3.9&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 3.9: Interface from the Friendsense study (Goel, Mason, and Watts 2010). The researchers turned a standard attitude survey into a fun, gamelike experience. The app asked participants both serious questions and more lighthearted questions, such as the one shown in this image. Image used with permission from Sharad Goel.&lt;/p&gt;

&lt;p&gt;The attitude questions were adapted from the General Social Survey. For example, “Does [your friend] sympathize with the Israelis more than the Palestinians in the Middle East situation?” and “Would [your friend] pay higher taxes for the government to provide universal health care?” On top of these serious questions, the researchers mixed in more lighthearted questions: “Would [your friend] rather drink wine over beer?” and “Would [your friend] rather have the power to read minds, instead of the power to fly?” These lighthearted questions made the process more enjoyable to participants and also enabled an interesting comparison: would attitude agreement be similar for serious political questions as lighthearted questions about drinking and superpowers?&lt;/p&gt;

&lt;p&gt;There were three main results from the study. First, friends were more likely to give the same answer than strangers, but even close friends still disagree on about 30% of the questions. Second, respondents over-estimate their agreement with their friends. In other words, most of the diversity of opinions that exists between friends is not noticed. Finally, participants were as likely to be aware of disagreements with their friends on serious matters of politics than lighthearted issues about drinking and superpowers.&lt;/p&gt;

&lt;p&gt;Although the app is no longer available to play (unfortunately), it was a nice example of how researchers can a standard attitude survey into something enjoyable. More generally, with some creativity and design work, it is possible to improve the user experience for survey participants. So, next time you are designing a survey, take a moment to think about what you could do to make the experience better for your participants. Some may fear that these steps toward gamification could hurt data quality, but I think that bored participants are a far greater risk to data quality.&lt;/p&gt;

&lt;p&gt;The work of Goel and colleagues also illustrates the theme of the next section: linking surveys to other data sources. In this case by linking the survey with Facebook the researchers automatically had access to a list of the participants’ friends. In the next section, we will consider the linkages between surveys and other data sources in greater detail.&lt;/p&gt;

&lt;h2 id=&#34;3-6-surveys-linked-to-other-data&#34;&gt;3.6 Surveys linked to other data&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Linking surveys to other data sources enables you to produce estimates that would impossible otherwise.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Most surveys are stand-alone, self-contained efforts. They don’t build on each other and they don’t take advantage of all of the other data that exists in the world. This will change. There is just too much to be gained by linking survey data to other data sources, such as the digital trace data discussed in Chapter 2. This ability to link surveys to other data highlights the fact that asking and observing are complements and not substitutes.&lt;/p&gt;

&lt;p&gt;In particular, I’ll distinguish between two different ways of linking asking and observing: amplified asking and enriched asking (Figure 3.10). In amplified asking, the digital traces are not of any direct interest other than their ability to help extract more value from the survey data. In enriched asking, on the other hand, the digital trace actually has a core measure of interest and the survey data builds the necessary context around it.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/80df17cb34b263b59e725a42bbf92cad.png&#34; alt=&#34;fig3.10&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 3.10: Two main ways to combine digital traces and survey data. In amplified asking (Section 3.6.1) the digital traces are used to amplify the survey data. In enriched asking (Section 3.6.2) the digital traces actually have a core measure of interest and the survey data builds the necessary context around it.&lt;/p&gt;

&lt;h3 id=&#34;3-6-1-amplified-asking&#34;&gt;3.6.1 Amplified asking&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;Linking your survey to digital traces can be like asking everyone your questions at all times.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Asking generally comes in two main categories: sample surveys and censuses. Sample surveys, where you access a small number of people, can be flexible, timely, and relatively cheap. However, sample surveys, because they are based on a sample, are often limited in their resolution; with a sample survey, it is often hard to make estimates about specific geographic regions or for specific demographic groups. Censuses, on the other, attempt to interview everyone in the population. They have great resolution, but they are generally expensive, narrow in focus (they only include a small number of questions), and not timely (they happen on a fixed schedule, such as every 10 years) (Kish 1979). Now imagine if researchers could combine the best characteristics of sample surveys and censuses; imagine if researchers could ask every question to everyone every day.&lt;/p&gt;

&lt;p&gt;Obviously, this continual, ubiquitous, always-on survey is a kind of social science fantasy. But, it appears that we can begin to approximate this by combining survey questions from a small number of people with digital traces from many people. I call this type of combination amplified asking. If done well, it could help us provides estimate that are more local (for smaller geographic areas), more granular (for specific demographic groups), and more timely.&lt;/p&gt;

&lt;p&gt;One example of amplified asking comes from the work of Joshua Blumenstock, who wanted to collect data that would help guide development in poor countries. More specifically, Blumenstock wanted to create a system to measure wealth and well-being that combined the completeness of a census with the flexibility and frequency of a survey (Blumenstock 2014; Blumenstock, Cadamuro, and On 2015). In fact, I’ve already described Blumenstock’s work briefly in Chapter 1.&lt;/p&gt;

&lt;p&gt;To start, Blumenstock partnered with the largest mobile phone provider in Rwanda. The company provided him anonymized transaction records from about 1.5 million customers covering behavior from 2005 and 2009. The logs contain information about each call and text message such as the start time, duration, and approximate geographic location of the caller and receiver. Before we start talking about the statistical issues, it is worth pointing out that this first step may be one of the hardest. As described in Chapter 2, most digital trace data is inaccessible to researchers. And, many companies are justifiably hesitant to share their data because it is private; that is their customers probably didn’t expect that their records will be shared—in bulk—with researchers. In this case, the researchers took careful steps to anonymize the data and their work was overseen by a third-party (i.e., their IRB). But, despite these efforts, these data are probably still identifiable and they likely contain sensitive information (Mayer, Mutchler, and Mitchell 2016; Landau 2016). I’ll return to these ethical question in Chapter 6.&lt;/p&gt;

&lt;p&gt;Recall that Blumenstock was interested in measuring wealth and well-being. But, these traits are not directly in the call records. In other words, these call records are incomplete for this research, a common feature of digital traces that was discussed in detail in Chapter 2. But, it seems likely that the call records probably have some information about wealth and well-being. So, one way of asking Blumenstock’s question could be: is it possible to predict how someone will respond to a survey based on their digital trace data? If so, then by asking a few people we can guess the answers of everyone else.&lt;/p&gt;

&lt;p&gt;To assess this empirically, Blumenstock and research assistants from Kigali Institute of Science and Technology called a sample of about a thousand mobile phone customers. The researchers explained the goals of the project to the participants, asked for their consent to link the survey responses to the call records, and then asked them a series of questions to measure their wealth and well-being, such as “Do you own a radio?” and “Do you own a bicycle?” (see Figure 3.11 for a partial list). All participants in the survey were compensated financially.&lt;/p&gt;

&lt;p&gt;Next, Blumenstock used a two-step procedure common in data science: feature engineering followed by supervised learning. First, in the feature engineering step, for everyone that was interviewed, Blumenstock converted the call records into a set of characteristics about each person; data scientists might call these characteristics “features” and social scientists would call them “variables.” For example, for each person, Blumenstock calculated total number of days with activity, the number of distinct people a person has been in contact with, the amount of money spent on airtime, and so on. Critically, good feature engineering requires knowledge of the research setting. For example, if it is important to distinguish between domestic and international calls (we might expect people who call internationally to be wealthier), then this must be done at the feature engineering step. A researcher with little understanding of Rwanda might not include this feature, and then the predictive performance of the model will suffer.&lt;/p&gt;

&lt;p&gt;Next, in the supervised learning step, Blumenstock built a statistical model to predict the survey response for each person based on their features. In this case, Blumenstock used logistic regression with 10-fold cross-validation, but he could have used a variety of other statistical or machine learning approaches.&lt;/p&gt;

&lt;p&gt;So how well did it work? Was Blumenstock able to predict answers to survey questions like “Do you own a radio?” and “Do you own a bicycle?” using features derived from call records? Sort of. The accuracy of the predictions were high for some traits (Figure 3.11). But, it is always important to compare a complex prediction method against a simple alternative. In this case, a simple alternative is to predict that everyone will give the most common answer. For example, 97.3% reported owning a radio so if Blumenstock had predicted that everyone would report owning a radio he would have had an accuracy of 97.3%, which is surprisingly similar to the performance of his more complex procedure (97.6% accuracy). In other words, all the fancy data and modeling increased the accuracy of the prediction from 97.3% to 97.6%. However, for other questions, such as “Do you own a bicycle?”, the predictions improved from 54.4% to 67.6%. More generally, Figure 3.12 shows for some traits Blumenstock did not improve much beyond just making the simple baseline prediction, but that for other traits there was some improvement.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/179dfd9b6498529cef6071910525b5d9.png&#34; alt=&#34;fig3.11&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 3.11: Predictive accuracy for statistical model trained with call records. Results from Table 2 of Blumenstock (2014).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/e1b8b367faf3a73279a91b17423396e9.png&#34; alt=&#34;fig3.12&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 3.12: Comparison of predictive accuracy for statistical model trained with call records to simple baseline prediction. Points are slightly jittered to avoid overlap; see Table 2 of Blumenstock (2014) for exact values.&lt;/p&gt;

&lt;p&gt;At this point you might be thinking that these results are a bit disappointing, but just one year later, Blumenstock and two colleagues—Gabriel Cadamuro and Robert On—published a paper in Science with substantially better results (Blumenstock, Cadamuro, and On 2015). There were two main technical reasons for the improvement: 1) they used more sophisticated methods (i.e., a new approach to feature engineering and a more sophisticated machine learning model) and 2) rather than attempting to infer responses to individual survey questions (e.g., “Do you own a radio?”), they attempted to infer a composite wealth index.&lt;/p&gt;

&lt;p&gt;Blumenstock and colleagues demonstrated the performance of their approach in two ways. First, they found that for the people in their sample, they could do a pretty good job of predicting their wealth from call records (Figure 3.14). Second, and ever more importantly, Blumenstock and colleagues showed that their procedure could produce high-quality estimates of the geographic distribution of wealth in Rwanda. More specifically, they used their machine learning model, which was trained on their sample of about 1,000 people, to predict the wealth of all 1.5 million people in the call records. Further, with the geospatial data embedded in the call data (recall that the call data includes the location of the nearest cell tower for each call), the researchers were able to estimate the approximate place of residence of each person. Putting these two estimates together, the research produced an estimate of the geographic distribution of subscriber wealth at extremely fine spatial granularity. For example, they could estimate the average wealth in each of Rwanda’s 2148 cells (the smallest administrative unit in the country). These predicted wealth values were so granular they were difficult to check. So, the researchers aggregated their results to produce estimates of the average wealth of Rwanda’s 30 districts. These district-level estimates were strongly related to the estimates from a gold standard traditional survey, the Rwandan Demographic and Health Survey (Figure 3.14). Although the estimates from the two sources were similar, the estimates from Blumenstock and colleagues were about 50 times cheaper and 10 times faster (when cost in measured in terms of variable costs). This dramatic decrease in cost means that rather than being run every few years—as is standard for Demographic and Health Surveys—the hybrid of small survey combined with big digital trace data could be run every month.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/8e86aaf3bca7fccbfb579f7d15c17148.png&#34; alt=&#34;fig3.13&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 3.13: Schematic of Blumenstock, Cadamuro, and On (2015). Call data from the phone company was converted to a matrix with one row for each person and one column for each feature (i.e., variable). Next, the researchers built a supervised learning model to predict the survey responses from the person by feature matrix. Then, the supervised learning model was used to impute the survey responses for everyone. In essence, the researchers used the responses of about one thousand people to impute the wealth of about one million people. Also, the researchers estimated the approximate place of residence for all 1.5 million people based on the locations of their calls. When these two estimates were combined—the estimated wealth and the estimated place of residence—the results were similar to estimates from the Demographic and Health Survey, a gold-standard traditional survey (Figure 3.14).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/bda2690df7d0803562a8d7c05cc2c44c.png&#34; alt=&#34;fig3.14&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 3.14: Results from Blumenstock, Cadamuro, and On (2015). At the individual-level, the researchers were able to do a reasonable job at predicting someone’s wealth from their call records. The estimates of district-level wealth—which were based on individual-level estimates of wealth and place of residence—the results were similar to results from the Demographic and Health Survey, a gold-standard traditional survey.&lt;/p&gt;

&lt;p&gt;In conclusion, Blumenstock’s amplified asking approach combined survey data with digital trace data to produce estimates comparable with gold-standard survey estimates. This particular example also clarifies some of the trade-offs between amplified asking and traditional survey methods. First, the amplified asking estimates were more timely, substantially cheaper, and more granular. But, on the other hand, at this time, there is not a strong theoretical basis for this kind of amplified asking. That is, this one example does not show when it will work and when it won’t. Further, the amplified asking approach does not yet have good ways to quantify uncertainty around its estimates. However, amplified asking has deep connections to three large areas in statistics—model-based post-stratification (Little 1993), imputation (Rubin 2004), and small-area estimation (Rao and Molina 2015)—and so I expect that progress will be rapid.&lt;/p&gt;

&lt;p&gt;Amplified asking follows a basic recipe that can be tailored to your particular situation. There are two ingredients and two steps. The two ingredients are 1) a digital trace dataset that is wide but thin (that is, it has many people but not the information that you need about each persons) and 2) a survey that is narrow but thick (that is, it has only a few people, but it has the information that you need about those people). Then, there are two steps. First, for the people in both data sources, build a machine learning model that uses digital trace data to predict survey answers. Next, use that machine learning model to impute the survey answers of everyone in the digital trace data. Thus, if there is some question that you want to ask to lots of people, look for digital trace data from those people that might be used to predict their answer.&lt;/p&gt;

&lt;p&gt;Comparing Blumenstock’s first and second attempt at the problem also illustrates an important lesson about the transition from second era to third era approaches to survey research: the beginning is not the end. That is, many times, the first approach will not be the best, but if researchers continuing working, things can get better. More generally, when evaluating new approaches to social research in the digital age, it is important to make two distinct evaluations: 1) how well does this work now and 2) how well do you think this might work in the future as the data landscape changes and as researchers devote more attention to the problem. Although, researchers are trained to make the first kind of evaluation (how good is this particular piece of research), the second is often more important.&lt;/p&gt;

&lt;h3 id=&#34;3-6-2-enriched-asking&#34;&gt;3.6.2 Enriched asking&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;Even though it can be messy, enriched asking can be powerful.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;A different approach to dealing with the incompleteness of digital trace data is to enrich it directly with survey data, a process that I’ll call enriched asking. One example of enriched asking is the study of Burke and Kraut (2014), which I described earlier in the chapter (Section 3.2), about whether interacting on Facebook increases friendship strength. In that case, Burke and Kraut combined survey data with Facebook log data.&lt;/p&gt;

&lt;p&gt;The setting that Burke and Kraut were working in, however, meant that they didn’t have to deal with two big problems that researchers doing enriched asking face. First, actually linking together the data sets—a process called record linkage, the matching of a record in one dataset with the appropriate record in the other dataset—can be difficult and error-prone (we’ll see an example of this problem below). The second main problem of enriched asking is that the quality of the digital traces will frequently be difficult for researchers to assess. For example, sometimes the process through which it is collected is proprietary and could be susceptible to many of the problems described in Chapter 2. In other words, enriched asking will frequently involve error-prone linking of surveys to black-box data sources of unknown quality. Despite the concerns that these two problems introduce, it is possible to conduct important research with this strategy as was demonstrated by Stephen Ansolabehere and Eitan Hersh (2012) in their research on voting patterns in the US . It is worthwhile to go over this study in some detail because many of the strategies that Ansolabehere and Hersh developed will be useful in other applications of enriched asking.&lt;/p&gt;

&lt;p&gt;Voter turnout has been the subject of extensive research in political science, and in the past, researchers’ understanding of who votes and why has generally been based on the analysis of survey data. Voting in the US, however, is an unusual behavior in that the government records whether each citizen has voted (of course, the government does not record who each citizen votes for). For many years, these governmental voting records were available on paper forms, scattered in various local government offices around the country. This made it difficult, but not impossible, for political scientists to have a complete picture of the electorate and to compare what people say in surveys about voting to their actual voting behavior (Ansolabehere and Hersh 2012).&lt;/p&gt;

&lt;p&gt;But, now these voting records have been digitized, and a number of private companies have systematically collected and merged these voting records to produce comprehensive master voting files that record the voting behavior of all Americans. Ansolabehere and Hersh partnered with one of these companies—Catalist LCC—in order to use their master voting file to help develop a better picture of the electorate. Further, because it relied on digital records collected and curated by a company, it offered a number of advantages over previous efforts by researchers that had been done without the aid of companies and using analog records.&lt;/p&gt;

&lt;p&gt;Like many of the digital trace sources in Chapter 2, the Catalist master file did not include much of the demographic, attitudinal, and behavioral information that Ansolabehere and Hersh needed. In addition to this information, Ansolabehere and Hersh were particularly interested in comparing reported voting behavior to validated voting behavior (i.e., the information in the Catalist database). So, the researchers collected the data that they wanted as part of the Cooperative Congressional Election Study (CCES), a large social survey. Next, the researchers gave this data to Catalist, and Catalist gave the researchers back a merged data file that included validated voting behavior (from Catalist), the self-reported voting behavior (from CCES) and the demographics and attitudes of respondents (from CCES). In other words, Ansolabehere and Hersh enriched the voting data with survey data, and the resulting merged file enables them to do something that neither file enabled individually.&lt;/p&gt;

&lt;p&gt;By enriching the Catalist master data file with survey data, Ansolabehere and Hersh came to three important conclusions. First, over-reporting of voting is rampant: almost half of the non-voters reported voting. Or, another way of looking at it is if someone reported voting, there is only an 80% chance that they actually voted. Second, over-reporting is not random; over-reporting is more common among high-income, well-educated, partisans who are engaged in public affairs. In other words, the people who are most likely to vote are also most likely to lie about voting. Third, and most critically, because of the systematic nature of over-reporting, the actual differences between voters and non-voters are smaller than they appear just from surveys. For example, those with a bachelors degree are about 22 percentage points more likely to report voting, but are only 10 percentage points more likely to actual vote. Further, existing resource-based theories of voting are much better at predicting who will report voting than who actually votes, an empirical finding that calls for new theories to understand and predict voting.&lt;/p&gt;

&lt;p&gt;But, how much should we trust these results? Remember these results depend on error-prone linking to black-box data with unknown amounts of error. More specifically, the results hinge on two key steps: 1) the ability of Catalist to combine many disparate data sources to produce an accurate master datafile and 2) the ability of Catalist to link the survey data to its master datafile. Each of these steps is quite difficult and errors at either step could lead researchers to the wrong conclusions. However, both data processing and matching are critical to the continued existence of Catalist as a company so it can invest resources in solving these problems, often at a scale that no individual academic researcher or group of researchers can match. In the further reading at the end of the chapter, I describe these problems in more detail and how Ansolabehere and Hersh build confidence in their results. Although these details are specific to this study, issues similar to these will arise for other researchers wishing to link to black-box digital trace data sources.&lt;/p&gt;

&lt;p&gt;What are the general lessons researchers can draw from this study? First, there is tremendous value from enriching digital traces with survey data. Second, even though these aggregated, commercial data sources should not be considered “ground truth”, in some cases they can be useful. In fact, it is best to compare these data sources not to absolute Truth (from which they will always fall short). Rather, it is better to compare them to other available data sources, which invariably have errors as well.&lt;/p&gt;

&lt;h2 id=&#34;3-7-conclusion&#34;&gt;3.7 Conclusion&lt;/h2&gt;

&lt;p&gt;Big data sources do not mean the end of survey research. In fact, it is the opposite. Big data sources and surveys are complements not substitutes so as the amount of big data increases, I expect that the value of surveys will increases as well.&lt;/p&gt;

&lt;p&gt;Researchers are now in the process of creating the third era of survey research that will most likely be characterized by 1) non-probability sampling, 2) computer-administrated interviews, and 3) linking survey data to other data. Each of these three areas are still in active development, but as the data environment changes and researchers invest more time to addresses these problems, I expect major improvements. Just as researchers took advantage of the widespread diffusion of landline phones in the 1960s and 1970s in order to modernize survey research, I expect that researchers will modernize survey research again in response to the changes created by the digital age.&lt;/p&gt;

&lt;h2 id=&#34;technical-appendix-1&#34;&gt;Technical appendix&lt;/h2&gt;

&lt;p&gt;This section will take a mathematical approach to sampling and estimation in probability and non-probability samples. It will draw on Särndal, Swensson, and Wretman (2003) and Särndal and Lundström (2005).&lt;/p&gt;

&lt;h3 id=&#34;further-commentary-1&#34;&gt;Further commentary&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;This section is designed to be used as a reference, rather than to be read as a narrative.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;strong&gt;Introduction (Section 3.1)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Many of the themes in this chapter have also been echoed in recent Presidential Addresses at the American Association of Public Opinion Research (AAPOR), such as Dillman (2002), Newport (2011), Santos (2014), and Link (2015).&lt;/p&gt;

&lt;p&gt;For more historical background about the development of survey research, see Smith (1976) and Converse (1987). For more on the idea of three eras of survey research, see Groves (2011) and Dillman, Smyth, and Christian (2008) (which breaks up the three eras slightly differently).&lt;/p&gt;

&lt;p&gt;A peak inside the transition from the first to the second era in survey research is Groves and Kahn (1979), which does a detailed head-to-head comparison between a face-to-face and telephone survey. Brick and Tucker (2007) looks back at the historical development of random digit dialing sampling methods.&lt;/p&gt;

&lt;p&gt;For more how survey research has changed in the past in response to changes in society, see Tourangeau (2004), Mitofsky (1989), and Couper (2011).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Asking vs. observing (Section 3.2)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Learning about internal states by asking questions can be problematic because sometimes the respondents themselves are not aware of their internal states. For example, Nisbett and Wilson (1977) have a wonderful paper with the evocative title: “Telling more than we can know: Verbal reports on mental processes.” In the paper the authors conclude: “subjects are sometimes (a) unaware of the existence of a stimulus that importantly influenced a response, (b) unaware of the existence of the response, and &amp;copy; unaware that the stimulus has affected the response.”&lt;/p&gt;

&lt;p&gt;For arguments that researchers should prefer observed behavior to reported behavior or attitudes, see Baumeister, Vohs, and Funder (2007) (psychology) and Jerolmack and Khan (2014) and responses (Maynard 2014; Cerulo 2014; Vaisey 2014; Jerolmack and Khan 2014) (sociology). The difference between asking and observing also arises in economics, where researchers talk about stated and revealed preferences. For example, a researcher could ask respondents whether they prefer eating ice cream or going to gym (stated preferences) or the research could observe how often people eat ice cream and go to the gym (revealed preferences). There is deep skepticism of certain types of stated preferences data in economics (Hausman 2012).&lt;/p&gt;

&lt;p&gt;A main theme from these debates is that reported behavior is not always accurate. But, automatically recorded behavior may not be accurate, may not be collected on a sample of interest, and may not be accessible to researchers. Thus, in some situations, I think that reported behavior can be useful. Further, a second main theme from these debates is that reports about emotions, knowledge, expectations, and opinions are not always accurate. But, if information about these internal states are needed by researchers—either to help explain some behavior or as the thing to be explained—then asking may be appropriate.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Total survey error (Section 3.3)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;For book length treatments on total survey error, see Groves et al. (2009) or Weisberg (2005). For a history of the development of total survey error, see Groves and Lyberg (2010).&lt;/p&gt;

&lt;p&gt;In terms of representation, a great introduction to the issues of non-response and non-response bias is the National Research Council report on Nonresponse in Social Science Surveys: A Research Agenda (2013). Another useful overview is provided by (Groves 2006). Also, entire special issues of the Journal of Official Statistics, Public Opinion Quarterly, and The Annals of the American Academy of Political and Social Science have been published on the topic of non-response. Finally, there are actually many different ways of calculating the response rate; these approaches are described in detail in a report by the American Association of Public Opinion Researchers (AAPOR) (Public Opinion Researchers} 2015).&lt;/p&gt;

&lt;p&gt;The 1936 Literary Digest poll has been studied in detail (Bryson 1976; Squire 1988; Cahalan 1989; Lusinchi 2012). It has also been used as a parable to warn against haphazard data collection (Gayo-Avello 2011). In 1936, George Gallup used a more sophisticated form of sampling, and was able to produce more accurate estimates with a much smaller sample. Gallup’s success over the Literary Digest was a milestone the development of survey research (Converse 1987, Ch 3; Ohmer 2006, Ch 4; Igo 2008, Ch 3).&lt;/p&gt;

&lt;p&gt;In terms of measurement, a great first resource for designing questionnaires is Bradburn, Sudman, and Wansink (2004). For a more advanced treatment focused specifically on attitude questions, see Schuman and Presser (1996). More on pre-testing questions is available in Presser and Blair (1994), Presser et al. (2004), and Chapter 8 of Groves et al. (2009).&lt;/p&gt;

&lt;p&gt;The classic, book-length treatment of the trade-off between survey costs and survey errors is Groves (2004).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Who to ask (Section 3.4)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Classic book-length treatment of standard probability sampling and estimation are Lohr (2009) (more introductory) and Särndal, Swensson, and Wretman (2003) (more advanced). A classic book-length treatment of post-stratification and related methods is Särndal and Lundström (2005). In some digital age settings, researchers know quite a bit about non-respondents, which was not often true in the past. Different forms of non-response adjustment are possible when researchers have information about non-respondents (Kalton and Flores-Cervantes 2003; Smith 2011).&lt;/p&gt;

&lt;p&gt;The Xbox study of Wang et al. (2015) uses a technique called multilevel regression and post-stratification (MRP, sometimes called “Mister P”) that allows researchers to estimate cell means even when there are many, many cells. Although there is some debate about the quality of the estimates from this technique, it seems like a promising area to explore. The technique was first used in Park, Gelman, and Bafumi (2004), and there has been subsequent use and debate (Gelman 2007; Lax and Phillips 2009; Pacheco 2011; Buttice and Highton 2013; Toshkov 2015). For more on the connection between individual weights and cell-based weights see Gelman (2007).&lt;/p&gt;

&lt;p&gt;For other approaches to weighting web surveys, see Schonlau et al. (2009), Valliant and Dever (2011), and Bethlehem (2010).&lt;/p&gt;

&lt;p&gt;Sample matching was proposed by Rivers (2007). Bethlehem (2015) argues that the performance of sample matching will actually be similar to other sampling approaches (e.g., stratified sampling) and other adjustment approaches (e.g., post-stratification). For more on online panels, see Callegaro et al. (2014).&lt;/p&gt;

&lt;p&gt;Sometimes researchers have found that probability samples and non-probability samples yield estimates of similar quality (Ansolabehere and Schaffner 2014), but other comparisons have found that non-probability samples do worse (Malhotra and Krosnick 2007; Yeager et al. 2011). One possible reason for these differences is that non-probability samples have improved over time. For a more pessimistic view of non-probability sampling methods see the the AAPOR Task Force on Non-probability Sampling (Baker et al. 2013), and I also recommend reading the commentary that follows the summary report.&lt;/p&gt;

&lt;p&gt;For a meta-analysis on the effect of weighting to reduce bias in non-probability samples, see Table 2.4 in Tourangeau, Conrad, and Couper (2013), which leads the authors to conclude “adjustments seem to be useful but fallible corrections &amp;hellip;”&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;How to ask (Section 3.5)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Conrad and Schober (2008) provides an edited volume titled Envisioning the Survey Interview of the Future, and it addresses many of the themes in this section. Couper (2011) addresses similar themes, and Schober et al. (2015) offers a nice example of how data collection methods that are tailored to a new setting can result in higher quality data.&lt;/p&gt;

&lt;p&gt;For another interesting example of using Facebook apps for social science surveys, see Bail (2015).&lt;/p&gt;

&lt;p&gt;For more advice on making surveys an enjoyable and valuable experience for participants, see work on the Tailored Design Method (Dillman, Smyth, and Christian 2014).&lt;/p&gt;

&lt;p&gt;Stone et al. (2007) offers a book length treatment of ecological momentary assessment and related methods.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Surveys linked to other data (Section 3.6)&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Judson (2007) described the process of combining surveys and administrative data as “information integration,” discusses some advantages of this approach, and offers some examples.&lt;/p&gt;

&lt;p&gt;Another way that researchers can use digital traces and administrative data is a sampling frame for people with specific characteristics. However, access these records to be used a sampling frame can also create questions related to privacy (Beskow, Sandler, and Weinberger 2006).&lt;/p&gt;

&lt;p&gt;Regarding amplified asking, this approach is not as new as it might appear from how I’ve described it. This approach has deep connections to three large areas in statistics—model-based post-stratification (Little 1993), imputation (Rubin 2004), and small area estimation (Rao and Molina 2015). It is also related to the use of surrogate variables in medical research (Pepe 1992).&lt;/p&gt;

&lt;p&gt;In addition to the ethical issues regarding accessing the digital trace data, amplified asking could also be used to infer sensitive traits that people might not choose to reveal in a survey (Kosinski, Stillwell, and Graepel 2013).&lt;/p&gt;

&lt;p&gt;The cost and time estimates in Blumenstock, Cadamuro, and On (2015) refer more to variable cost—the cost of one additional survey—and do not include fixed costs such as the cost to clean and process the call data. In general, amplified asking will probably have high fixed costs and low variable costs similar to digital experiments (see Chapter 4). More details on the data used in Blumenstock, Cadamuro, and On (2015) paper are in Blumenstock and Eagle (2010) and Blumenstock and Eagle (2012). Approaches from multiple imputuation (Rubin 2004) might help capture uncertainty in estimates from amplified asking. If researchers doing amplified asking only care about aggregate counts, rather than individual-level traits, then the approaches in King and Lu (2008) and Hopkins and King (2010) may be useful. For more about the machine learning approaches in Blumenstock, Cadamuro, and On (2015), see James et al. (2013) (more introductory) or Hastie, Tibshirani, and Friedman (2009) (more advanced). Another popular machine learning textbook is Murphy (2012).&lt;/p&gt;

&lt;p&gt;Regarding enriched asking, the results in Ansolabehere and Hersh (2012) hinge on two key steps: 1) the ability of Catalist to combine many disparate data sources to produce an accurate master datafile and 2) the ability of Catalist to link the survey data to its master datafile. Therefore, Ansolabehere and Hersh check each of these steps carefully.&lt;/p&gt;

&lt;p&gt;To create the master datafile, Catalist combines and harmonizes information from many different sources including: multiple voting records snapshots from each state, data from the Post Office’s National Change of Address Registry, and data from other unspecified commercial providers. The gory details about how all this cleaning and merging happens are beyond the scope of this book, but this process, no matter how careful, will propagate errors in the original data sources and will introduce errors. Although Catalist was willing to discuss its data processing and provide some of its raw data, it was simply impossible for researchers to review the entire Catalist data pipeline. Rather, the researchers were in a situation where the Catalist data file had some unknown, and perhaps unknowable, amount of error. This is a serious concern because a critic might speculate that the large differences between the survey reports on the CCES and the behavior in the Catalist master data file were caused by errors in the master data file, not by misreporting by respondents.&lt;/p&gt;

&lt;p&gt;Ansolabehere and Hersh took two different approaches to addressing the data quality concern. First, in addition to comparing self-reported voting to voting in the Catalist master file, the researchers also compared self-reported party, race, voter registration status (e.g., registered or not registered) and voting method (e.g., in person, absentee ballot, etc.) to those values found in the Catalist databases. For these four demographic variables, the researchers found much higher levels of agreement between survey report and data in the Catalist master file than for voting. Thus, the Catalist master data file appears to have high quality information for traits other than voting, suggesting that it is not of poor overall quality. Second, in part using data from Catalist, Ansolabehere and Hersh developed three different measures of quality of county voting records, and they found that the estimated rate of over-reporting of voting was essentially unrelated to any of these data quality measures, a finding that suggest that the high rates of over-reporting are not being driven by counties with unusually low data quality.&lt;/p&gt;

&lt;p&gt;Given the creation of this master voting file, the second source of potential errors is linking the survey records to it. For example, if this linkage is done incorrectly it could lead to an over-estimate of the difference between reported and validated voting behavior (Neter, Maynes, and Ramanathan 1965). If every person had a stable, unique identifier that was in both data sources, then linkage would be trivial. In the US and most other countries, however, there is no universal identifier. Further, even if there were such an identifier people would probably be hesitant to provide it to survey researchers! Thus, Catalist had to do the linkage using imperfect identifiers, in this case four pieces of information about each respondent: name, gender, birth year, and home address. For example, Catalist had to decide if the Homie J Simpson in the CCES was the same person as the Homer Jay Simpson in their master data file. In practice, matching is a difficult and messy process, and, to make matters worse for the researchers, Catalist considered its matching technique to be proprietary.&lt;/p&gt;

&lt;p&gt;In order to validate the matching algorithms, they relied on two challenges. First, Catalist participated in a matching competition that was run by an independent, third-party: the MITRE Corporation. MITRE provided all participants two noisy data files to be matched, and different teams competed to return to MITRE the best matching. Because MITRE itself knew the correct matching they were able to score the teams. Of the 40 companies that competed, Catalist came in second place. This kind of independent, third-party evaluation of proprietary technology is quite rare and incredibly valuable; it should give us confidence that Catalist’s matching procedures are essentially at the state-of-the-art. But is the state-of-the-art good enough? In addition to this matching competition, Ansolabehere and Hersh created their own matching challenge for Catalist. From an earlier project, Ansolabehere and Hersh had collected voter records from Florida. They provided some of these records with some of their fields redacted to Catalist and then compared Catalist’s reports of these fields to their actual values. Fortunately, Catalist’s reports were close to the withheld values, indicating that Catalist could match partial voter records onto their master data file. These two challenges, one by a third-party and one by Ansolabehere and Hersh, give us more confidence in the Catalist matching algorithms, even though we cannot review their exact implementation ourselves.&lt;/p&gt;

&lt;p&gt;There have been many previous attempts to validate voting. For an overview of that literature, see Belli et al. (1999), Berent, Krosnick, and Lupia (2011), Ansolabehere and Hersh (2012), and Hanmer, Banks, and White (2014).&lt;/p&gt;

&lt;p&gt;It is important to note that although in this case researchers were encouraged by the quality of data from Catalist, other evaluations of commercial vendors have been less enthusiastic. Researchers have found poor quality when data from a survey to a consumer-file from Marketing Systems Group (which itself merged together data from three providers: Acxiom, Experian, and InfoUSA) (Pasek et al. 2014). That is, the data file did not match survey responses that researchers expected to be correct, the datafile had missing data for a large number of questions, and the missing data pattern was correlated to reported survey value (in other words the missing data was systematic, not random).&lt;/p&gt;

&lt;p&gt;For more on record linkage between surveys and administrative data, see Sakshaug and Kreuter (2012) and Schnell (2013). For more on record linkage in general, see Dunn (1946) and Fellegi and Sunter (1969) (historical) and Larsen and Winkler (2014) (modern). Similar approaches have also been developed in computer science under the names such as data deduplication, instance identification, name matching, duplicate detection, and duplicate record detection (Elmagarmid, Ipeirotis, and Verykios 2007). There are also privacy preserving approaches to record linkage which do not require the transmission of personally identifying information (Schnell 2013). Researchers at Facebook developed a procedure to probabilisticsly link their records to voting behavior (Jones et al. 2013); this linkage was done to evaluate an experiment that I’ll tell you about in Chapter 4 (Bond et al. 2012).&lt;/p&gt;

&lt;p&gt;Another example of linking a large-scale social survey to government administrative records comes from the Health and Retirement Survey and the Social Security Administration. For more on that study, including information about the consent procedure, see Olson (1996) and Olson (1999).&lt;/p&gt;

&lt;p&gt;The process of combining many sources of administrative records into a master datafile—the process that Catalist employees—is common in the statistical offices of some national governments. Two researchers from Statistics Sweden have written a detailed book on the topic (Wallgren and Wallgren 2007). For an example of this approach in a single county in the United States (Olmstead County, Minnesota; home of the Mayo Clinic), see Sauver et al. (2011). For more on errors that can appear in administrative records, see Groen (2012).&lt;/p&gt;

&lt;h2 id=&#34;activities-1&#34;&gt;Activities&lt;/h2&gt;

&lt;p&gt;Key:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;degree of difficulty: easy easy, medium medium, hard hard, very hard very hard&lt;/li&gt;
&lt;li&gt;requires math (requires math)&lt;/li&gt;
&lt;li&gt;requires coding (requires coding)&lt;/li&gt;
&lt;li&gt;data collection (data collection)&lt;/li&gt;
&lt;li&gt;my favorites (my favorite)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;[hard, requires math] In the chapter, I was very positive about post-stratification. However, it does not always improve the quality of estimates. Construct a situation where can post-stratification can decrease the quality of estimates. (For a hint, see Thomsen (1973)).&lt;/p&gt;

&lt;p&gt;[hard, data collection, requires coding] Design and conduct a non-probability survey on Amazon MTurk to ask about gun ownership (“Do you, or does anyone in your household, own a gun, rifle or pistol? Is that you or someone else in your household?”) and attitudes towards gun control (“What do you think is more important–to protect the right of Americans to own guns, or to control gun ownership?”).&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;How long does your survey take? How much does it cost? How do the demographics of your sample compare to the demographics of the U.S. population?&lt;/li&gt;
&lt;li&gt;What is the raw estimate of gun ownership using your sample?&lt;/li&gt;
&lt;li&gt;Correct for the non-representativeness of your sample using post-stratification or some other technique. Now what is the estimate of gun ownership?&lt;/li&gt;
&lt;li&gt;How do your estimates compare to the latest estimate from Pew Research Center? What do you think explain the discrepancies, if there is any?&lt;/li&gt;
&lt;li&gt;Repeat the exercise 2-5 for attitudes toward gun control. How do your findings differ?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;[very hard, data collection, requires coding] Goel and colleagues (2016) administered a non-probability-based survey consisting of 49 multiple-choice attitudinal questions drawn from the General Social Survey (GSS) and select surveys by the Pew Research Center on Amazon MTurk. They then adjust for the non-representativeness of data using model-based post-stratification (Mr. P), and compare the adjusted estimates with those estimated using probability-based GSS/Pew surveys. Conduct the same survey on MTurk and try to replicate Figure 2a and Figure 2b by comparing your adjusted estimates with the estimates from the most recent rounds of GSS/Pew (See Appendix Table A2 for the list of 49 questions).&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Compare and contrast your results to the results from Pew and GSS.&lt;/li&gt;
&lt;li&gt;Compare and contrast your results to the results from the MTurk survey in Goel, Obeng, and Rothschild (2016).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;[medium, data collection, requires coding] Many studies use self-report measures of mobile phone activity data. This is an interesting setting where researchers can compare self-reported behavior with logged behavior (see e.g., Boase and Ling (2013)). Two common behaviors to ask about are calling and texting, and two common time frames are “yesterday” and “in the past week.”&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Before collecting any data, which of the self-report measures do you think is more accurate? Why?&lt;/li&gt;
&lt;li&gt;Recruit 5 of your friends to be in your survey. 3. Please briefly summarize how these 5 friends were sampled. Might this sampling procedure induce specific biases in your estimates?&lt;/li&gt;
&lt;li&gt;Please ask them the following micro-survey:

&lt;ul&gt;
&lt;li&gt;“How many times did you use mobile phone to call others yesterday?”&lt;/li&gt;
&lt;li&gt;“How many text messages did you send yesterday?”&lt;/li&gt;
&lt;li&gt;“How many times did you use your mobile phone to call others in the last seven days?”&lt;/li&gt;
&lt;li&gt;“How many times did you use your mobile phone to send or receive text messages/SMS in the last seven days?” Once the survey is complete, ask to check their usage data as logged by their phone or service provider.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;How does self-report usage compare to log data? Which is most accurate, which is least accurate?&lt;/li&gt;
&lt;li&gt;Now combine the data that you have collected with the data from other people in your class (if you are doing this activity for a class). With this larger dataset, repeat part (d).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;[medium, data collection] Schuman and Presser (1996) argue that question orders would matter for two types of relations between questions: part-part questions where two questions are at the same level of specificity (e.g. ratings of two presidential candidates); and part-whole questions where a general question follows a more specific question (e.g. asking “How satisfied are you with your work?” followed by “How satisfied are you with your life?”).&lt;/p&gt;

&lt;p&gt;They further characterize two types of question order effect: consistency effects occur when responses to a later question are brought closer (than they would otherwise be) to those given to an earlier question; contrast effects occur when there are greater differences between responses to two questions.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Create a pair of part-part questions that you think will have a large question order effect, a pair of part-whole questions that you think will have a large order effect, and another pair of questions whose order you think would not matter. Run a survey experiment on MTurk to test your questions.&lt;/li&gt;
&lt;li&gt;How large was the part-part effect were you able to create? Was it a consistency or contrast effect?&lt;/li&gt;
&lt;li&gt;How large was the part-whole effect were you able to create? Was it a consistency or contrast effect?&lt;/li&gt;
&lt;li&gt;Was there a question order effect in your pair where you did not think the order would matter?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;[medium, data collection] Building on the work of Schuman and Presser, Moore (2002) describes a separate dimension of question order effect: additive and subtractive. While contrast and consistency effects are produced as a consequence of respondents’ evaluations of the two items in relation to each other, additive and subtractive effects are produced when respondents are made more sensitive to the larger framework within which the questions are posed. Read Moore (2002), then design and run a survey experiment on MTurk to demonstrate additive or subtractive effects.&lt;/p&gt;

&lt;p&gt;[hard, data collection] Christopher Antoun and colleagues (2015) conducted a study comparing the convenience samples obtained from four different online recruiting sources: MTurk, Craigslist, Google AdWords and Facebook. Design a simple survey and recruit participants through at least two different online recruiting sources (they can be different sources from the four sources used in Antoun et al. (2015)).&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Compare the cost per recruit, in terms of money and time, between different sources.&lt;/li&gt;
&lt;li&gt;Compare the composition of the samples obtained from different sources.&lt;/li&gt;
&lt;li&gt;Compare the quality of data between the samples. For ideas about how to measure data quality from respondents, see Schober et al. (2015).&lt;/li&gt;
&lt;li&gt;What is your preferred source? Why?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;[medium] YouGov, an internet-based market research firm, conducted online polls of a panel of about 800,000 respondents in the UK and used Mr. P. to predict the result of EU Referendum (i.e., Brexit) where the UK voters vote either to remain in or leave the European Union.&lt;/p&gt;

&lt;p&gt;A detailed description of YouGov’s statistical model is here (&lt;a href=&#34;https://yougov.co.uk/news/2016/06/21/yougov-referendum-model/&#34; target=&#34;_blank&#34;&gt;https://yougov.co.uk/news/2016/06/21/yougov-referendum-model/&lt;/a&gt;). Roughly speaking, YouGov partitions voters into types based on 2015 general election vote choice, age, qualifications, gender, date of interview, as well as the constituency they live in. First, they used data collected from the YouGov panelists to estimate, among those who vote, the proportion of people of each voter type who intend to vote Leave. They estimate turnout of each voter type by using the 2015 British Election Study (BES) post-election face-to-face survey, which validated turnout from the electoral rolls. Finally, they estimate how many people there are of each voter type in the electorate based on latest Census and Annual Population Survey (with some addition information from the BES, YouGov survey data from around the general election, and information on how many people voted for each party in each constituency).&lt;/p&gt;

&lt;p&gt;Three days before the vote, YouGov showed a two point lead for Leave. On the eve of voting, the poll showed too close to call (49-51 Remain). The final on-the-day study predicted &lt;sup&gt;48&lt;/sup&gt;&amp;frasl;&lt;sub&gt;52&lt;/sub&gt; in favor of Remain (&lt;a href=&#34;https://yougov.co.uk/news/2016/06/23/yougov-day-poll/&#34; target=&#34;_blank&#34;&gt;https://yougov.co.uk/news/2016/06/23/yougov-day-poll/&lt;/a&gt;). In fact, this estimate missed the final result (52-48 Leave) by four percentage points.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Use the total survey error framework discussed in this chapter to assess what could have gone wrong.&lt;/li&gt;
&lt;li&gt;YouGov’s response after the election (&lt;a href=&#34;https://yougov.co.uk/news/2016/06/24/brexit-follows-close-run-campaign/&#34; target=&#34;_blank&#34;&gt;https://yougov.co.uk/news/2016/06/24/brexit-follows-close-run-campaign/&lt;/a&gt;) explained: “This seems in a large part due to turnout – something that we have said all along would be crucial to the outcome of such a finely balanced race. Our turnout model was based, in part, on whether respondents had voted at the last general election and a turnout level above that of general elections upset the model, particularly in the North.” Does this change your answer to part (a)?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;[medium, requires coding] Write a simulation to illustrate each of the representation errors in Figure 3.1.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Create a situation where these errors actually cancel out.&lt;/li&gt;
&lt;li&gt;Create a situation where the errors compound each other.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;[very hard, requires coding] The research of Blumenstock and colleagues (2015) involved building a machine learning model that could use digital trace data to predict survey responses. Now, you are going to try the same thing with a different dataset. Kosinski, Stillwell, and Graepel (2013) found that Facebook likes can predict individual traits and attributes. Surprisingly, these predictions can be even more accurate than those of friends and colleagues (Youyou, Kosinski, and Stillwell 2015).&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Read Kosinski, Stillwell, and Graepel (2013), and replicate Figure 2. Their data are available here: &lt;a href=&#34;http://mypersonality.org/&#34; target=&#34;_blank&#34;&gt;http://mypersonality.org/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Now, replicate Figure 3.&lt;/li&gt;
&lt;li&gt;Finally, try their model on your own Facebook data: &lt;a href=&#34;http://applymagicsauce.com/&#34; target=&#34;_blank&#34;&gt;http://applymagicsauce.com/&lt;/a&gt;. How well does it work for you?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;[medium] Toole et al. (2015) use call detail records (CDRs) from mobile phones to predict aggregate unemployment trends.&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Compare and contrast the design of Toole et al. (2015) with Blumenstock, Cadamuro, and On (2015).&lt;/li&gt;
&lt;li&gt;Do you think CDRs should replace traditional surveys, complement them or not be used at all for government policymakers to track unemployment? Why?&lt;/li&gt;
&lt;li&gt;What evidence would convince you that CDRs can completely replace traditional measures of the unemployment rate?&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&#34;4-running-experiments&#34;&gt;4 Running experiments&lt;/h1&gt;

&lt;p&gt;4.1 Introduction
4.2 What are experiments?
4.3 Two dimensions of experiments: lab-field and analog-digital
4.4 Moving beyond simple experiments
4.4.1 Validity
4.4.2 Heterogeneity of treatment effects
4.4.3 Mechanisms
4.5 Making it happen
4.5.1 Just do it yourself
4.5.1.1 Use existing environments
4.5.1.2 Build your own experiment
4.5.1.3 Build your own product
4.5.2 Partner with the powerful
4.6 Advice
4.6.1 Create zero variable cost data
4.6.2 Replace, Refine, and Reduce
4.7 Conclusion
- Technical appendix
- Further commentary
- Activities&lt;/p&gt;

&lt;h2 id=&#34;4-1-introduction&#34;&gt;4.1 Introduction&lt;/h2&gt;

&lt;p&gt;In the approaches covered so far in this book—observing behavior (Chapter 2) and asking questions (Chapter 3)—researchers collect data about what is naturally occurring in the world. The approach covered in this chapter—running experiments—is fundamentally different. When researchers run experiments, they systematically intervene in the world to create data that is ideally suited to answering questions about cause-and-effect relationships.&lt;/p&gt;

&lt;p&gt;Cause-and-effect questions are very common in social research, and examples includes questions such as Does increasing teachers’ salaries increase student learning? What is the effect of minimum wage on employment rates? How does a job applicant’s race effect her chance of getting a job? In addition to these explicitly causal questions, sometimes cause-and-effect questions are implicit in more general questions about maximization of some performance metric. For example, the question “What color button will maximize donations on an NGO website site?” is really lots of questions about the effect of different button colors on donations.&lt;/p&gt;

&lt;p&gt;One way to answer cause-and-effect questions is to look for patterns in existing data. For example, using data from thousands of schools, you might calculate that students learn more in schools that offer high teacher salaries. But, does this correlation show that higher salaries cause students to learn more? Of course not. Schools where teachers earn more might be different in many ways. For example, students in schools with high teacher salaries might come from wealthier families. Thus, what looks like an effect of teachers could just come from comparing different types of students. These unmeasured differences between students are called confounders, and in general, the possibility of confounders wreaks havoc on researchers ability to answer cause-and-effect questions by looking for patterns in existing data.&lt;/p&gt;

&lt;p&gt;One solution to the problem of confounders is to try to make fair comparisons by adjusting for observable differences between the groups. For example, you might be able to download property tax data from a number of government websites. Then, you could compare student performance in schools where home prices are similar but teacher salaries are different, and you still might find that students learn more in schools with higher teacher pay. But, there are still many possible confounders. Maybe the parents of these student differ in their level of education or maybe the schools differ in their closeness to public libraries or maybe the schools with higher teacher pay also have higher pay for principals and principal pay, not teacher pay, is really what is increasing student learning. You could try to measure these other factors as well, but the list of possible confounders is essentially endless. In many situations, you just cannot measure and adjust for all the possible confounders. This approach can only take you so far.&lt;/p&gt;

&lt;p&gt;A better solution to the problem of confounders is running experiments. Experiments enable researchers to move beyond the correlations in naturally occurring data in order to reliably answer cause-and-effect question. In the analog age, experiments were often logistically difficult and expensive. Now, in the digital age, logistical constraints are gradually fading away. Not only is it easier to do experiments like those researchers have done in the past, it is now possible to run new kinds of experiments.&lt;/p&gt;

&lt;p&gt;In what I’ve written so far I’ve been a bit loose in my language, but it is important to distinguish between two things: experiments and randomized controlled experiments. In an experiment a researcher intervenes in the world and then measures an outcome. I’ve heard this approach described as “perturb and observe.” This strategy is very effective in the natural sciences, but in medical and social sciences, there is another approach that works better. In a randomized controlled experiment a researcher intervenes for some people and not for others, and, critically, the researcher decides which people receive the intervention by randomization (e.g., flipping a coin). This procedure ensures that randomized controlled experiments create fair comparisons between two groups: one that has received the intervention and one that has not. In other words, randomized controlled experiments are a solution to the problems of confounders. Despite the important differences between experiments and randomized controlled experiments, social researchers often use these terms interchangeably. I’ll follow this convention, but, at certain points, I’ll break the convention to emphasize the value of randomized controlled experiments over experiments without randomization and a control group.&lt;/p&gt;

&lt;p&gt;Randomized controlled experiments have proven to be a powerful way to learn about the social world, and in this chapter, I’ll teach you more about how to use them in your research. In Section 4.2, I’ll illustrate the basic logic of experimentation with an example of an experiment on Wikipedia. Then, in Section 4.3, I’ll describe the difference between lab experiments and field experiments and the differences between analog experiments and digital experiments. Further, I’ll argue that digital field experiments can offer the best features of analog lab experiments (tight control) and analog field experiments (realism), all at a scale that was not possible previously. Next, in Section 4.4, I’ll describe three concepts—validity, heterogeneity of treatment effects, and mechanisms—that are critical for designing rich experiments. With that background, I’ll describe the trade-offs involved in the two main strategies for conducting digital experiments: doing it yourself (Section 4.5.1) or partnering with the powerful (Section 4.5.2). Finally, I’ll conclude with some design advice about how you can take advantage of the real power of digital experiments (Section 4.6.1) and describe some of responsibility that comes with that power (Section 4.6.2). The chapter will be presented with a minimum of mathematical notation and formal language; readers interested in a more formal, mathematical approach to experiments should also read the Technical Appendix at the end of the chapter.&lt;/p&gt;

&lt;h2 id=&#34;4-2-what-are-experiments&#34;&gt;4.2 What are experiments?&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Randomized controlled experiments have four main ingredients: recruitment of participants, randomization of treatment, delivery of treatment, and measurement of outcomes.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Randomized controlled experiments can take many forms and can be used to study many types of behavior. But, at their core, randomized controlled experiments have four main ingredients: recruitment of participants, randomization of treatment, delivery of treatment, and measurement of outcomes. The digital age does not change the fundamental nature of experimentation, but it does make them easier logistically. For example, in the past it might have been difficult to measure the behavior of millions of people, but that is now routinely happening in many digital systems. Researchers who can figure out how to harness these new opportunities will be able to run experiments that were impossible previously.&lt;/p&gt;

&lt;p&gt;To make this all a bit more concrete—both what has stayed the same and what has changed—let’s consider Michael Restivo and Arnout van de Rijt’s (2012). The researchers wanted to understand the effect of informal peer rewards on editorial contributions to Wikipedia. In particular, they studied the effects of barnstars, an award that any Wikipedian can give to any other Wikipedian to acknowledge hard work and due diligence. Restivo and van de Rijt gave barnstars to 100 deserving Wikipedians. Then, Restivo and van de Rijt tracked the recipients’ subsequent contributions to Wikipedia over the next 90 days. Much to their surprise, the people to whom they awarded barnstars tended to make fewer edits after receiving one. In other words, the barnstars seemed to be discouraging rather than encouraging contribution.&lt;/p&gt;

&lt;p&gt;Fortunately, Restivo and van de Rijt were not running a “perturb and observe” experiment; they were running a randomized controlled experiment. So, in addition to choosing 100 top contributors to receive a barnstar, they also picked 100 top contributors to whom they did not give a barnstar. These hundred served as a control group, and who got a barnstar and who didn’t was determined randomly. When Restivo and van de Rijt looked at the control group they found that it had a steep drop in contributions too. Finally, when the researchers compared people in the treatment group (i.e., received barnstars) and people in the control group, they found that the barnstar caused editors to contribute about 60% more. But, this increase in contribution was taking place as part of an overall decline in both groups.&lt;/p&gt;

&lt;p&gt;As this study illustrates, the control group in experiments is critical in a way that is somewhat paradoxical. In order to precisely measure the effect of barnstars, Restivo and van der Rijt needed to observe people that did not receive barnstars. Many times researchers who are not familiar with experiments fail to appreciate the incredible value of the control group. If Restivo and van de Rijt didn’t have a control group, they would have drawn exactly the wrong conclusion. Control groups are so important that the CEO of a major casino company has said that there are only three ways that employees can be fired from his company: theft, sexual harassment, and running an experiment without a control group (Schrage 2011).&lt;/p&gt;

&lt;p&gt;Restivo and van de Rijt’s study illustrates the four main ingredients of an experiment: recruitment, randomization, intervention, and outcomes. Together, these four ingredients allow scientists to move beyond correlations and measure the causal effect of treatments. Specifically, randomization means that when you compare outcomes for the treatment and control groups you get an estimate of the causal effect of that intervention for that set of participants. In other words, with a randomized controlled experiment you can be sure that any differences in outcomes are caused by the intervention and not a confounder, a claim that I make precise in the Technical Appendix using the potential outcomes framework.&lt;/p&gt;

&lt;p&gt;In addition to being a nice illustration of the mechanics of experiments, Restivo and van de Rijt’s study also shows that the logistics of digital experiments can be completely different from analog experiments. In Restivo and van de Rijt’s experiment, it was easy to give the barnstar to anyone in the world and it was easy to track the outcome—number of edits—over an extended period of time (because edit history is automatically recorded by Wikipedia). This ability to deliver treatments and measure outcomes at no cost is qualitatively unlike experiments in the past. Although this experiment involved 200 people, it could have been run with 2,000 or 20,000 people. The main thing preventing the researchers from scaling up their experiment by a factor of 100 was not cost, it was ethics. That is, Restivo and van de Rijt didn’t want to give barnstars to undeserving editors and they didn’t want their experiment to disrupt the Wikipedia community (Restivo and Rijt 2012; Restivo and Rijt 2014). So, although the experiment of Restivo and van de Rijt is relatively simple, it clearly shows that some things about experiments have stayed the same and some have changed. In particular, the basic logic of experimentation is the same, but the logistics have changed. Next, in order to more clearly isolate the opportunities created by this change, I’ll compare the experiments that researchers can do now to the kinds of experiments that have been done in the past.&lt;/p&gt;

&lt;h2 id=&#34;4-3-two-dimensions-of-experiments-lab-field-and-analog-digital&#34;&gt;4.3 Two dimensions of experiments: lab-field and analog-digital&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Lab experiments offer control, field experiments offer realism, and digital field experiments combine control and realism at scale.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Experiments come in many different shapes and sizes. But, despite these differences, researchers have found it helpful to organize experiments along a continuum between lab experiments and field experiments. Now, however, researchers should also organize experiments along a continuum between analog experiments and digital experiments. This two-dimensional design space will help you understand the strengths and weaknesses of different approaches and suggest areas of greatest opportunity (Figure 4.1).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/258535053919dd099aea28e121d1c280.png&#34; alt=&#34;fig4.1&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 4.1: Schematic of design space for experiments. In the past, experiments varied along the lab-field dimension. Now, they also vary on the analog-digital dimension. In my opinion, the area of greatest opportunity is digital field experiments.&lt;/p&gt;

&lt;p&gt;In the past, the main way that researchers organized experiments was along the lab-field dimension. The majority of experiments in the social sciences are lab experiments where undergraduate students perform strange tasks in a lab for course credit. This type of experiment dominates research in psychology because it enables researchers to create very specific treatments designed to test very specific theories about social behavior. For certain problems, however, something feels a bit strange about drawing strong conclusions about human behavior from such unusual people performing such unusual tasks in such an unusual setting. These concerns have led to a movement toward field experiments. Field experiments combine the strong design of randomized control experiments with more representative groups of participants, performing more common tasks, in more natural settings.&lt;/p&gt;

&lt;p&gt;Although some people think of lab and field experiments as competing methods, it is best to think of them as complementary methods with different strengths and weaknesses. For example, Correll, Benard, and Paik (2007) used both a lab experiment and a field experiment in an attempt to find the sources of the “motherhood penalty.” In the United States, mothers earn less money than childless women, even when comparing women with similar skills working in similar jobs. There are many possible explanations for this pattern, and one is that employers are biased against mothers. (Interestingly, the opposite seems to be true for fathers: they tend to earn more than comparable childless men). In order to assess possible bias against mothers, Correll and colleagues ran two experiments: one in the lab and one in the field.&lt;/p&gt;

&lt;p&gt;First, in a lab experiment Correll and colleagues told participants, who were college undergraduates, that a California-based start-up communications company was conducting an employment search for a person to lead its new East Coast marketing department. Students were told that the company wanted their help in the hiring process and they were asked to review resumes of several potential candidates and to rate the candidates on a number of dimensions such as their intelligence, warmth, and commitment to work. Further, the students were asked if they would recommend hiring the applicant and what they would recommend as a starting salary. Unbeknownst to the students, however, the resumes were specifically constructed to be similar except for one thing: some of the resumes signaled motherhood (by listing involvement in a parent-teacher association) and some did not. Correll found that students were less likely to recommend hiring the mothers and offered them lower starting salary. Further, through a statistical analysis of both the ratings and the hiring-related decisions, Correll found that mothers’ disadvantages were largely explained by the fact that mothers were rated lower in terms of competence and commitment. In other words, Correll argues that these traits are the mechanism through which mothers are disadvantaged. Thus, this lab experiment allowed Correll and colleagues to measure a causal effect and provide a possible explanation for that effect.&lt;/p&gt;

&lt;p&gt;Of course, one might be skeptical about drawing conclusions about the entire US labor market based on the decisions of a few hundred undergraduates who have probably never had a full time job, let alone hired people. Therefore, Correll and colleagues also conducted a complementary field experiment. The researchers responded to hundreds of advertised job openings by sending in fake cover letters and resumes. Similar to the materials shown to the undergraduates, some resumes signaled motherhood and some did not. Correll and colleagues found that mothers were less likely to get called back for interviews than equally qualified childless women. In other words, real employers making consequential decisions in a natural setting behaved much like the undergraduates. Did they make similar decisions for the same reason? Unfortunately, we don’t know. The researchers were not able to ask the employers to rate the candidates or explain their decisions.&lt;/p&gt;

&lt;p&gt;This pair of experiments reveals a lot about lab and field experiments in general. Lab experiments offer researchers near total control of the environment in which participants are making decisions. So, for example, in the lab experiment, Correll was able to ensure that all the resumes were read in a quiet setting; in the field experiment, some of the resumes might not have even been read. Further, because participants in the lab setting know that they are being studied, researchers are often able to collect additional data that can help them understand why participants are making their decisions. For example, Correll asked participants in the lab experiment to rate the candidates on different dimensions. This kind of process data could help researchers understand the mechanisms behind differences in how participants treat the resumes.&lt;/p&gt;

&lt;p&gt;On the other hand, these exact same characteristics that I just described as advantages are also sometimes considered disadvantages. Researchers who prefer field experiments argue that participants in lab experiments could act very differently when they are being closely observed. For example, in the lab experiment participants might have guessed the goal of the research and altered their behavior so as not to appear biased. Further, researchers who prefer field experiments might argue that small differences on resumes can only stand out in a very clean, sterile lab environment, and thus the lab experiment will over-estimate the effect of motherhood on real hiring decisions. Finally, many proponents of field experiments criticize lab experiments reliance on WEIRD participants: mainly students from Western, Educated, Industrialized, Rich, and Democratic countries (Henrich, Heine, and Norenzayan 2010). The experiments by Correll and colleagues (2007) illustrate the two extremes on the lab-field continuum. In between these two extremes there are a variety of hybrid designs including approaches such as bringing non-students into a lab or going into the field but still having participants perform an unusual task.&lt;/p&gt;

&lt;p&gt;In addition to the lab-field dimension that has existed in the past, the digital age means that researchers now have a second major dimension along which experiments can vary: analog-digital. Just as there are pure lab experiments, pure field experiments, and a variety of hybrids in between, there are pure analog experiments, pure digital experiments, and a variety of hybrids. It is tricky to offer a formal definition of this dimension, but a useful working definition is that fully digital experiments are experiments that make use of digital infrastructure to recruit participants, randomize, deliver treatments, and measure outcomes. For example, Restivo and van de Rijt’s (2012) study of barnstars and Wikipedia was a fully digital experiment because it used digital systems for all four of these steps. Likewise fully analog experiments are experiments that do not make use of digital infrastructure for any of these four steps. Many of the classic experiments in psychology are analog experiments. In between these two extremes there are partially digital experiments that use a combination of analog and digital systems for the four steps.&lt;/p&gt;

&lt;p&gt;Critically, the opportunities to run digital experiments are not just online. Researchers can run partially digital experiments by using digital devices in the physical world in order to deliver treatments or measure outcomes. For example, researchers could use smart phones to deliver treatments or sensors in the built environment to measure outcomes. In fact, as we will see later in this chapter, researchers have already used home power meters to measure outcomes in experiments about social norms and energy consumption involving 8.5 million of households (Allcott 2015). As digital devices become increasingly integrated into people’s lives and sensors become integrated into the built environment, these opportunities to run partially digital experiments in the physical world will increase dramatically. In other words, digital experiments are not just online experiments.&lt;/p&gt;

&lt;p&gt;Digital systems create new possibilities for experiments everywhere along the lab-field continuum. In pure lab experiments, for example, researchers can use digital systems for finer measurement of participants’ behavior; one example of this type of improved measurement is eye-tracking equipment which provides precise and continuous measures of gaze location. The digital age also creates the possibility to run lab-like experiments online. For example, researchers have rapidly adopted Amazon Mechanical Turk (MTurk) to recruit participants for online experiments (Figure 4.2). MTurk matches “employers” who have tasks that need to be completed with “workers” who wish to complete those tasks for money. Unlike traditional labor markets, however, the tasks involved usually only require a few minutes to complete and the entire interaction between employer and worker is virtual. Because MTurk mimics aspects of traditional lab experiments—paying people to complete tasks that they would not do for free—it is naturally suited for certain types of experiments. Essentially, MTurk has created the infrastructure for managing a pool of participants—recruiting and paying people—and researchers have taken advantage of that infrastructure to tap into an always available pool of participants.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/d765780c3a5f82e192770ad25edacf25.png&#34; alt=&#34;fig4.2&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 4.2: Papers published using data from Amazon Mechanical Turk (MTurk) (Bohannon 2016). MTurk and other online labor markets offer researchers a convenient way to recruit participants for experiments.&lt;/p&gt;

&lt;p&gt;Digital experiments create even more possibilities for field-like experiments. Digital field experiments can offer tight control and process data to understand possible mechanisms (like lab experiments) and more diverse participants making real decisions in a natural environment (like field experiments). In addition to this combination of good characteristics of earlier experiments, digital field experiments also offer three opportunities that were difficult in analog lab and field experiments.&lt;/p&gt;

&lt;p&gt;First, whereas most analog lab and field experiments have hundreds of participants, digital field experiments can have millions of participants. This change in scale is because some digital experiments can produce data at zero variable cost. That is, once researchers have created an experimental infrastructure, increasing the number of participants typically does not increase the cost. Increasing the number of participants by a factor of 100 or more is not just a quantitative change, it is a qualitative change, because it enables researchers to learn different things from experiments (e.g., heterogeneity of treatment effects) and run entirely different experimental designs (e.g., large group experiments). This point is so important, I’ll return to it towards the end of the chapter when I offer advice about creating digital experiments.&lt;/p&gt;

&lt;p&gt;Second, whereas most analog lab and field experiments treat participants as indistinguishable widgets, digital field experiments often use background information about participants in the design and analysis stages of the research. This background information, which is called pre-treatment information, is often available in digital experiments because they take place in fully measured environments. For example, a researcher at Facebook has much more pre-treatment information than a researcher designing a standard lab experiment with undergraduates. This pre-treatment information enables researchers to move beyond treating participants as indistinguishable widgets. More specifically, pre-treatment information enables more efficient experimental designs—such as blocking (Higgins, Sävje, and Sekhon 2016) and targeted recruitment of participants (Eckles, Kizilcec, and Bakshy 2016)—and more insightful analysis—such as estimation of heterogeneity of treatment effects (Athey and Imbens 2016a) and covariate adjustment for improved precision (Bloniarz et al. 2016).&lt;/p&gt;

&lt;p&gt;Third, whereas many analog lab and field experiments deliver treatments and measure outcomes in a relatively compressed amount of time, some digital field experiments involve treatments that can be delivered over time and the effects can also be measured over time. For example, Restivo and van de Rijt’s experiment has the outcome measured daily for 90 days, and one of the experiments I’ll tell you about later in the chapter (Ferraro, Miranda, and Price 2011) tracks outcomes over 3 years at basically no cost. These three opportunities—size, pre-treatment information, and longitudinal treatment and outcome data—are most common when experiments are run on top of always-on measurements systems (see Chapter 2 for more on always-on measurement systems).&lt;/p&gt;

&lt;p&gt;While digital field experiments offer many possibilities, they also share some weaknesses with both analog lab and field experiments. For example, experiments cannot be used to study the past, and they can only estimate the effects of treatments that can be manipulated. Also, although experiments are undoubtedly useful to guide policy, the exact guidance they can offer is somewhat limited because of complications such as environmental dependence, compliance problems, and equilibrium effects (Banerjee and Duflo 2009; Deaton 2010). Finally, digital field experiments magnify the ethical concerns created by field experiments. Proponents of field experiments trumpet their ability to unobtrusively and randomly intervene into consequential decisions made by millions of people. These features offer certain scientific advantages, but they can also make field experiments ethically complex (think about it as researchers treating people like “lab rats” on a massive scale). Further, in addition to possible harms to participants, digital field experiments, because of their scale, can also raise concerns about the disruption of working social systems (e.g., concerns about disrupting Wikipedia’s reward system if Restivo and van der Rijt gave too many barnstars).&lt;/p&gt;

&lt;h2 id=&#34;4-4-moving-beyond-simple-experiments&#34;&gt;4.4 Moving beyond simple experiments&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Let’s move beyond simple experiments. Three concepts are useful for rich experiments: validity, heterogeneity of treatment effects, and mechanism.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Researchers who are new to experiments often focus on a very specific, narrow question: does this treatment “work”? For example, does a phone call from a volunteer encourage someone to vote? Does changing a website button from blue to green increase click-through rate? Unfortunately, loose phrasing about what “works” obscures the fact that narrowly focused experiments don’t really tell you whether a treatment “works” in a general sense. Rather, narrowly focused experiments answer a much more specific question: what is the average effect of this specific treatment with this specific implementation for this population of participants at this time? I’ll call experiments that focus on this narrow question simple experiments.&lt;/p&gt;

&lt;p&gt;Simple experiments can provide valuable information, but they fail to answer many questions that are both important and interesting such as: are there some people for whom the treatment had a larger or smaller effect?; is there another treatment that would be more effective?; and how does this experiment relate to broader social theories?&lt;/p&gt;

&lt;p&gt;In order to show the value of moving beyond simple experiments, let’s consider one of my favorite analog field experiments, a study by P. Wesley Schultz and colleagues on the relationship between social norms and energy consumption (Schultz et al. 2007). Schultz and colleagues hung doorhangers on 300 households in San Marcos, California, and these doorhangers delivered different messages designed to encourage energy conservation. Then, Schultz and colleagues measured the effect of these messages on electricity consumption, both after one week and three weeks; see Figure 4.3 for a more detailed description of the experimental design.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/92b8eda01a164d1181f85f1a65ab0da1.png&#34; alt=&#34;fig4.3&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 4.3: Schematic of design from Schultz et al. (2007). The field experiment involved visiting about 300 households in San Marcos, California five times over an eight week period. On each visit the researchers manually took a reading from the house’s power meter. On two of the visits the researchers placed doorhangers on the house providing some information about their energy usage. The research question was how the content of these messages would impact energy use.&lt;/p&gt;

&lt;p&gt;The experiment had two conditions. In the first condition, households received general energy saving tips (e.g., use fans instead of air conditioners) and information about their household’s energy usage compared to the average of the energy usage in their neighborhood. Schultz and colleagues called this the descriptive normative condition because the information about the energy use in their neighborhood provided information about typical behavior (i.e., a descriptive norm). When Schultz and colleagues looked at the resulting energy usage in this group, the treatment appeared to have no effect, either in the short-term or the long-term; in other words, the treatment didn’t seem to “work” (Figure 4.4).&lt;/p&gt;

&lt;p&gt;But, fortunately, Schultz et al. (2007) did not settle for this simplistic analysis. Before the experiment began they reasoned that heavy users of electricity—people above the mean—might reduce their consumption, and that light users of electricity—people below the mean—might actually increase their consumption. When they looked at the data, that’s exactly what they found (Figure 4.4). Thus, what looked like a treatment that was having no effect was actually a treatment that had two offsetting effects. The researchers called this counter-productive increase among the light users a boomerang effect.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/f69d15b861dd45c4ec08f4e6819a2c3f.png&#34; alt=&#34;fig4.4&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 4.4: Results from Schultz et al. (2007). The first panel shows that the descriptive norm treatment has an estimated zero average treatment effect. However, the second panel shows that this average treatment effect is actually composed of two offsetting effects. For heavy users, the treatment decreased usage but for light users, the treatment increased usage. Finally, the third panel shows that the second treatment, which used descriptive and injunctive norms, had roughly the same effect on heavy users but mitigated the boomerang effect on light users.&lt;/p&gt;

&lt;p&gt;Further, Schultz and colleagues anticipated this possibility, and in the second condition they deployed a slightly different treatment, one explicitly designed to eliminate the boomerang effect. The households in the second condition received the exact same treatment—general energy saving tips and information about their household’s energy usage compared to the their neighborhood—with one tiny addition: for people with below-average consumption, the researchers added a :) and for people with above-average consumption they added a :(. These emoticons were designed to trigger what the researchers called injunctive norms. Injunctive norms refer to perceptions of what is commonly approved (and disapproved) whereas descriptive norms refer to perceptions of what is commonly done (Reno, Cialdini, and Kallgren 1993).&lt;/p&gt;

&lt;p&gt;By adding this one tiny emoticon, the researchers dramatically reduced the boomerang effect (Figure 4.4). Thus, by making this one simple change—a change that was motivated by an abstract social psychological theory (Cialdini, Kallgren, and Reno 1991)—the researchers were able to turn a program from one that didn’t seem to work into one that worked, and, simultaneously, they were able to contribute to the general understanding of how social norms affect human behavior.&lt;/p&gt;

&lt;p&gt;At this point, however, you might notice that something is a bit different about this experiment. In particular, the experiment of Schultz and colleagues doesn’t really have a control group in the same way that randomized controlled experiments do. The comparison between this design and the design of Restivo and van de Rijt illustrates the differences between two major designs used by researchers. In between-subjects designs, such as Restivo and van de Rijt, there is a treatment group and a control group, and in within-subjects designs the behavior of participants is compared before and after the treatment (Greenwald 1976; Charness, Gneezy, and Kuhn 2012). In a within-subject experiment it is as if each participant acts as her own control group. The strength of between-subjects designs is that it provides protection against confounders (as I described earlier), and the strength of within-subjects experiments is increased precision in estimates. When each participant acts as their own control, between-participant variation is eliminated (see Technical Appendix). To foreshadow an that will come later when I offer advice about designing digital experiments, there is a final design, called a mixed design, that combines the improved precision of within-subjects designs and the protection against confounding of between-subjects designs.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/4c860a773a714864fc96c6fcfc63bdcd.png&#34; alt=&#34;fig4.5&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 4.5: Three experimental designs. Standard randomized controlled experiments use between-subjects designs. An example of a between-subjects design is Restivo and van de Rijt’s (2012) experiment on barnstars and contributions to Wikipedia: researchers randomly divided participants into treatment and control groups, gave participants in the treatment group a barnstar, and compared outcomes for the two groups. A second type of design is a within-subjects design. The two experiments in Schultz and colleague’s (2007) study on social norms and energy use illustrate a within-subjects design: researchers compared the electricity use of participants before and after receiving the treatment. Within-subjects designs offer improved statistical precision by eliminating between subject variance (see Technical Appendix), but they are open to possible confounders (e.g., changes in weather between the pre-treatment and treatment period) (Greenwald 1976; Charness, Gneezy, and Kuhn 2012). Within-subjects designs are also sometimes called repeated measures designs. Finally, mixed designs combine the improved precision of within-subjects designs and the protection against confounding of between-subjects designs. In a mixed design, a researcher compares the change in outcomes for people in the treatment and control groups. When researchers already have pre-treatment information, as is the case in many digital experiments, mixed designs are preferable to between-subjects designs because of gains in precision (see Technical Appendix).&lt;/p&gt;

&lt;p&gt;Overall, the design and results of Schultz et al. (2007) show the value of moving beyond simple experiments. Fortunately, you don’t need to be a genius to create experiments like this. Social scientists have developed three concepts that will guide you toward richer and more creative experiments: 1) validity, 2) heterogeneity of treatment effects, and 3) mechanisms. That is, if you keep these three ideas in mind while you are designing your experiment, you will naturally create more interesting and useful experiments. In order to illustrate these three concepts in action, I’ll describe a number of follow-up partially digital field experiments that built on the elegant design and exciting results in Schultz et al. (2007). As you will see, through more careful design, implementation, analysis, and interpretation, you too can move beyond simple experiments.&lt;/p&gt;

&lt;h3 id=&#34;4-4-1-validity&#34;&gt;4.4.1 Validity&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;Validity refers to how much the results of an experiment support a more general conclusion.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;No experiment is perfect, and researchers have developed an extensive vocabulary to describe possible problems. Validity refers to the extent to which the results of a particular experiment support some more general conclusion. Social scientists have found it helpful to split validity into four main types: statistical conclusion validity, internal validity, construct validity, and external validity (Shadish, Cook, and Campbell 2001, Ch 2). Mastering these concepts will provide you a mental checklist for critiquing and improving the design and analysis of an experiment, and it will help you communicate with other researchers.&lt;/p&gt;

&lt;p&gt;Statistical conclusion validity centers around whether the statistical analysis of the experiment was done correctly. In the context of Schultz et al. (2007) such question might center on whether they computed their p-values correctly. Statistical analysis is beyond the scope of this book, but I can say that the statistical principles needed to design and analyze experiments have not changed in the digital age. However, the different data environment in digital experiments does create new statistical opportunities (e.g., using machine learning methods to estimate heterogeneity of treatment effects (Imai and Ratkovic 2013)) and new computational challenges (e.g., blocking in massive experiments (Higgins, Sävje, and Sekhon 2016)).&lt;/p&gt;

&lt;p&gt;Internal validity centers around whether the experimental procedures were performed correctly. Returning to the experiment of Schultz et al. (2007), questions about internal validity could center around the randomization, delivery of the treatment, and measurement of outcomes. For example, you might be concerned that the research assistants did not read the electric meters reliably. In fact, Schultz and colleagues were worried about this problem and they had a sample of meters read twice; fortunately, the results were essentially identical. In general, Schultz and colleagues’ experiment appears to have high internal validity, but this is not always the case; complex field and online experiments often run into problems actually delivering the right treatment to the right people and measuring the outcomes for everyone. Fortunately, the digital age can help reduce concerns about internal validity because it makes it easier to ensure that the treatment is delivered as designed to those who are supposed to receive it and to measure outcomes for all participants.&lt;/p&gt;

&lt;p&gt;Construct validity centers around the match between the data and the theoretical constructs. As discussed in Chapter 2, constructs are abstract concepts that social scientists reason about. Unfortunately, these abstract concepts don’t always have clear definitions and measurements. Returning to Schultz et al. (2007), the claim that injunctive social norms can lower electricity use requires researchers to design a treatment that would manipulate “injunctive social norms” (e.g., an emoticon) and to measure “electricity use”. In analog experiments, many researchers designed their own treatments and measured their own outcomes. This approach ensures that, as much as possible, the experiments match the abstract constructs being studied. In digital experiments where researchers partner with companies or governments to deliver treatments and use always-on data systems to measure outcomes, the match between the experiment and the theoretical constructs may be less tight. Thus, I expect that construct validity will tend to be a bigger concern in digital experiments than analog experiments.&lt;/p&gt;

&lt;p&gt;Finally, external validity centers around whether the results of this experiment would generalize to other situations. Returning to Schultz et al. (2007), one could ask, will this same idea—providing people information about their energy usage in relationship to their peers and a signal of injunctive norms (e.g., an emoticon)—reduce energy usage if it was done in a different way in a different setting? For most well-designed and well-run experiments, concerns about external validity are the hardest to address. In the past, these debates about external validity were frequently just a bunch of people sitting in a room trying to imagine what would have happened if the procedures were done in a different way, or in a different place, or with different people. Fortunately, the digital age enables researchers to move beyond these data-free speculations and assess external validity empirically.&lt;/p&gt;

&lt;p&gt;Because the results from Schultz et al. (2007) were so exciting, a company named Opower partnered with utilities in the United States to deploy the treatment more widely. Based on the design of Schultz et al. (2007), Opower created customized Home Energy Reports that had two main modules, one showing a household’s electricity usage relative to its neighbors with an emoticon and one providing tips for lowering energy usage (Figure 4.6). Then, in partnership with researchers, Opower ran randomized controlled experiments to assess the impact of the Home Energy Reports. Even though the treatments in these experiments were typically delivered physically—usually through old fashioned snail mail—the outcome was measured using digital devices in the physical world (e.g., power meters). Rather than manually collecting this information with research assistants visiting each house, the Opower experiments were all done in partnership with power companies enabling the researchers to access the power readings. Thus, these partially digital field experiments were run at a massive scale at low variable cost.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/5915ed8848b985c8933707d3c5820467.png&#34; alt=&#34;fig4.6&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 4.6: The Home Energy Reports in Allcott (2011) had a Social Comparison Module and an Action Steps Module.&lt;/p&gt;

&lt;p&gt;In a first set of experiments involving 600,000 households served by 10 utility companies around the United States, Allcott (2011) found the Home Energy Report lowered electricity consumption by 1.7%. In other words, the results from the much larger, more geographically diverse study were qualitatively similar to the results from Schultz et al. (2007). But, the effect size was smaller: in Schultz et al. (2007) the households in the descriptive and injective norms condition (the one with the emoticon) reduced their electricity usage by 5%. The precise reason for this difference is unknown, but Allcott (2011) speculated that receiving a handwritten emoticon as part of a study sponsored by a university might have a larger effect on behavior than receiving a printed emoticon as part of a mass produced report from a power company.&lt;/p&gt;

&lt;p&gt;Further, in subsequent research, Allcott (2015) reported on an additional 101 experiments involving an additional 8 million households. In these next 101 experiments the Home Energy Report continued to cause people to lower their electricity consumption, but the effects were even smaller. The precise reason for this decline is not known, but Allcott (2015) speculated that the effectiveness of the report appeared to be declining over time because it was actually being applied to different types of participants. More specifically, utilities in more environmentalist areas were more likely adopt the program earlier and their customers were more responsive to the treatment. As utilities with less environmental customers adopted the program, its effectiveness appeared to decline. Thus, just as randomization in experiments ensures that the treatment and control group are similar, randomization in research sites ensures that the estimates can be generalized from a one group of participants to a more general population (think back to Chapter 3 about sampling). If research sites are not sampled randomly, then generalization—even from a perfectly designed and conducted experiment—can be problematic.&lt;/p&gt;

&lt;p&gt;Together, these 111 experiments—10 in Allcott (2011) and 101 in Allcott (2015)—involved about 8.5 million households from all over the United States. They consistently show that Home Energy Reports reduce average electricity consumption, a result that supports the original findings of Schultz and colleagues from 300 homes in California. Beyond just replicating these original results, the follow-up experiments also show that the size of the effect varies by location. This set of experiments also illustrates two more general points about partially digital field experiments. First, researchers will be able to empirically address concerns about external validity when the cost of running experiments is low, and this can occur if the outcome is already being measured by an always-on data system. Therefore, it suggests that research should be on the look-out for other interesting and important behaviors that are already being recorded, and then design experiments on top of this existing measuring infrastructure. Second, this set of experiments reminds us that digital field experiments are not just online; increasingly I expect that they will be everywhere with many outcomes measured by sensors in the built environment.&lt;/p&gt;

&lt;p&gt;The four types of validity—statistical conclusion validity, internal validity, construct validity, external validity—provide a mental checklist to help researchers assess whether the results from a particular experiment support a more general conclusion. Compared to analog age experiments, in digital age experiments it should be easier to address external validity empirically and it should be easier to ensure internal validity. On the other hand, issues of construct validity will probably be more challenging in digital age experiments (although that was not the case with the Opower experiments).&lt;/p&gt;

&lt;h3 id=&#34;4-4-2-heterogeneity-of-treatment-effects&#34;&gt;4.4.2 Heterogeneity of treatment effects&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;Experiments normally measure the average effect, but the effect can be different for different people.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The second key idea for moving beyond simple experiments is heterogeneity of treatment effects. The experiment of Schultz et al. (2007) powerfully illustrates how the same treatment can have different effects on different kinds of people (Figure 4.4), but this analysis of heterogeneity is actually quite unusual for an analog age experiment. Most analog age experiments involve a small number of participants that are treated as interchangeable “widgets” because little about them is known pre-treatment. In digital experiments, however, these data constraints are less common because researchers tend to have more participants and know more about them. In this different data environment, we can estimate heterogeneity of treatment effects in order to provide clues about how the treatment works, how it can be improved, and how it can be targeted to those mostly likely to benefit.&lt;/p&gt;

&lt;p&gt;Two examples of heterogeneity of treatment effects in the context of social norms and energy use come from additional research on the Home Energy Reports. First, Allcott (2011) used the large sample size (600,000 households) to further split the sample and estimate the effect of the Home Energy Report by decile of pre-treatment energy usage. While Schultz et al. (2007) found differences between heavy and light users, Allcott (2011) found that there were also differences within the heavy and light user group. For example, the heaviest users (those in the top decile) reduced their energy usage twice as much as someone in the middle of the heavy user group (Figure 4.7). Further, estimating the effect by pre-treatment behavior also revealed that there was not a boomerang effect even for the lightest users (Figure 4.7).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/c424b8e630762465e1641b2d1d9bc0c7.png&#34; alt=&#34;fig4.7&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 4.7: Heterogeneity of treatment effects in Allcott (2011). The decrease in energy use was different for people in different deciles of baseline usage.&lt;/p&gt;

&lt;p&gt;In a related study, Costa and Kahn (2013) speculated that the effectiveness of the Home Energy Report could vary based on a participant’s political ideology and that the treatment might actually cause people with certain ideologies to increase their electricity use. In other words, they speculated that the Home Energy Reports might be creating a boomerang effect for some types of people. To assess this possibility, Costa and Kahn merged the Opower data with data purchased from a third-party aggregator that included information such as political party registration, donations to environment organizations, and household participation in renewable energy programs. With this merged dataset, Costa and Kahn found that the Home Energy Reports produced broadly similar effects for participants with different ideologies; there was no evidence that any group exhibited boomerang effects (Figure 4.8).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/a7fa530346919085bdb5760654cdb917.png&#34; alt=&#34;fig4.8&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 4.8: Heterogeneity of treatment effects in Costa and Kahn (2013). The estimated average treatment effect for the entire sample is -2.1% [-1.5%, -2.7%]. By combining information from the experiment with information about the households, Costa and Kahn (2013) used a series of statistical models to estimate the treatment effect for very specific groups of people. Two estimates are presented for each group because the estimates depend on the covariates they included in their statistical models (see model 4 and model 6 in Table 3 and Table 4 in Costa and Kahn (2013)). As this example illustrates, treatment effects can be different for different people and estimates of treatment effects that come from statistical models can depend on the details of those models (Grimmer, Messing, and Westwood 2014).&lt;/p&gt;

&lt;p&gt;As these two examples illustrate, in the digital age, we can move from estimating average treatment effects to estimating the heterogeneity of treatment effects because we can have many more participants and we know more about those participants. Learning about heterogeneity of treatment effects can enable targeting of a treatment where it is most effective, provide facts that stimulate new theory development, and provide hints about a possible mechanism, the topic to which I now turn.&lt;/p&gt;

&lt;h3 id=&#34;4-4-3-mechanisms&#34;&gt;4.4.3 Mechanisms&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;Experiments measure what happened. Mechanisms explain why and how it happened.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The third key idea for moving beyond simple experiments is mechanisms. Mechanisms tell us why or how a treatment caused an effect. The process of searching for mechanisms is also sometimes called looking for intervening variables or mediating variables. Although experiments are good for estimating causal effects, they are often not designed to reveal mechanisms. Digital age experiments can help us identify mechanisms in two ways: 1) they enable us to collect more process data and 2) they enable us to test many related treatments.&lt;/p&gt;

&lt;p&gt;Because mechanisms are tricky to define formally (Hedström and Ylikoski 2010), I’m going to start with a simple example: limes and scurvy (Gerber and Green 2012). In the 18th century doctors had a pretty good sense that when sailors ate limes they did not get scurvy. Scurvy is a terrible disease so this was powerful information. But, these doctors did not know why limes prevented scurvy. It was not until 1932, almost 200 years later, that scientists could reliably show that vitamin C was the reason that lime prevented scurvy (Carpenter 1988, p 191). In this case, vitamin C is the mechanism through which limes prevent scurvy (Figure 4.9). Of course, identifying the mechanism is very important scientifically—lots of science is about understanding why things happen. Identifying mechanisms is very important practically. Once we understand why a treatment works, we can potentially develop new treatments that work even better.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/4a6b19bb8d02275a273a866cc102bd5c.png&#34; alt=&#34;fig4.9&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 4.9: Limes prevent scurvy and the mechanism is Vitamin C.&lt;/p&gt;

&lt;p&gt;Unfortunately, isolating mechanisms is very difficult. Unlike limes and scurvy, in many social settings, treatments probably operate through many interrelated pathways, which makes isolation of mechanisms extremely difficult. However, in the case of social norms and energy use, researchers have tried to isolate mechanisms by collecting process data and testing related treatments.&lt;/p&gt;

&lt;p&gt;One way to test possible mechanisms is by collecting process data about how the treatment impacted possible mechanisms. For example, recall that Allcott (2011) showed that Home Energy Reports caused people to lower their electricity usage. But, how do these reports lower electricity usage? What were the mechanisms? In a follow-up study, Allcott and Rogers (2014) partnered with a power company that, through a rebate program, had acquired information on which consumers upgraded their appliances to more energy efficient models. Allcott and Rogers (2014) found that slightly more people receiving the Home Energy Reports upgraded their appliances. But, this difference was so small that it could only account for 2% of the decrease in energy use in the treated households. In other words, appliance upgrades were not the dominant mechanism through which the Home Energy Report decreased electricity consumption.&lt;/p&gt;

&lt;p&gt;A second way to study mechanisms is to run experiments with slightly different versions of the treatment. For example, in the experiment of Schultz et al. (2007) and all the subsequent Home Energy Report experiments, participants were provided with a treatment that has two main parts 1) tips about energy savings and 2) information about their energy use relative to their peers (Figure 4.6). Thus, it is possible that the energy saving tips are what caused the change, not the peer information. To assess the possibility that the tips alone might have been sufficient, Ferraro, Miranda, and Price (2011) partnered with a water company near Atlanta, GA, and ran a related experiment on water conservation involving about 100,000 households. There were four conditions:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;a group that received tips on saving water.&lt;/li&gt;
&lt;li&gt;a group that received tips on saving water + a moral appeal to save water.&lt;/li&gt;
&lt;li&gt;a group that received tips on saving water + a moral appeal to save water + information about their water use relative to their peers.&lt;/li&gt;
&lt;li&gt;a control group.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The researchers found that the tips only treatment had no effect on water usage in the short (one year), medium (two year), and long (three year) term. The tips + appeal treatment caused participants to decrease water usage, but only in the short-term. Finally, the tips + appeal + peer information treatment caused decreased usage in the short, medium, and long term (Figure 4.10). These kinds of experiments with unbundled treatments are a good way to figure out which part of the treatment—or which parts together—are the ones that are causing the effect (Gerber and Green 2012, Sec. 10.6). For example, the experiment of Ferraro and colleagues shows us that water saving tips alone are not enough to decrease water usage.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/67e268bade13a3707d4ad62584b7bb7e.png&#34; alt=&#34;fig4.10&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 4.10: Results from Ferraro, Miranda, and Price (2011). Treatments were sent May 21, 2007, and effects were measured during the summers of 2007, 2008, and 2009. By unbundling the treatment the researchers hoped to develop a better sense of the mechanisms. The tips only treatment had essentially no effect in the short (one year), medium (two years), and long (three years) term. The tips + appeal treatment caused participants to decrease water usage, but only in the short-term. The advice + appeal + peer information treatment caused participants to decrease water usage in the short, medium, and long term. Vertical bars are estimated confidence intervals. See Bernedo, Ferraro, and Price (2014) for actual study materials.&lt;/p&gt;

&lt;p&gt;Ideally, one would move beyond the layering of components (tips; tips + appeal; tips + appeal + peer information) to a full factorial design—also sometimes called a $2^k$ factorial design—where each possible combination of the three elements is tested (Table 4.1). By testing every possible combination of components, researchers can fully assess the effect of each component in isolation and in combination. For example, the experiment of Ferraro and colleagues does not reveal whether peer comparison alone would have been sufficient to lead to long term changes in behavior. In the past, these full factorial designs have been difficult to run because they require a large number of participants and they require researchers to be able to precisely control and deliver a large number of treatments. But, the digital age removes these logistical constraints in some situations.&lt;/p&gt;

&lt;p&gt;Table 4.1: Example of treatments in a full factorial design with 3 elements: tips, appeal, and peer information. The actual design of Ferraro, Miranda, and Price (2011) was a fractional factorial design that included three treatments: tips; tips + appeal; and tips + appeal + peer information (Figure 4.10).&lt;/p&gt;

&lt;p&gt;|Treatment  |Characteristics|
|&amp;ndash;|:&amp;ndash;|
|1  |control|
|2| tips|
|3  |appeal|
|4  |peer information|
|5  |tips + appeal|
|6  |tips + peer information|
|7  |appeal + peer information|
|8  |tips + appeal + peer information|&lt;/p&gt;

&lt;p&gt;In summary, mechanisms—the pathways through which a treatment has an effect—are incredibly important. Digital age experiments can help researchers learn about mechanisms by 1) collecting process data and 2) enabling full factorial designs. The mechanisms suggested by these approaches can then by tested directly by experiments specifically designed to test mechanisms (Ludwig, Kling, and Mullainathan 2011; Imai, Tingley, and Yamamoto 2013; Pirlott and MacKinnon 2016).&lt;/p&gt;

&lt;p&gt;In total, these three concepts—validity; heterogeneity of treatment effects; and mechanisms—provide a powerful set of ideas for designing and interpreting experiments. These concepts help researchers move beyond simple experiments about what “works” to richer experiments that have tighter links to theory, that reveal where and why treatments work, and might even help researchers design more effective treatments. Given this conceptual background about experiments, I’ll now turn to how you can actually make your experiments happen.&lt;/p&gt;

&lt;h2 id=&#34;4-5-making-it-happen&#34;&gt;4.5 Making it happen&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;Even if you don’t work at a big tech company you can run digital experiments. You can either do it yourself or partner with someone who can help you (and who you can help).&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;By this point, I hope that you are excited about the possibilities of doing your own digital experiments. If you work at a big tech company you might already be doing these experiments all the time. But, if you don’t work at a tech company you might think that you can’t run digital experiments. Fortunately, that’s wrong; with a little creativity and hard work, everyone can run a digital experiment.&lt;/p&gt;

&lt;p&gt;As a first step, it is helpful to distinguish between two main approaches: doing it yourself or partnering with the powerful. And, there are even a couple of different ways that you can do it yourself; you can experiment in existing environments, build your own experiment, or build your own product for repeated experimentation. I’ll illustrate these approaches with lots of examples below, and while you are learning about them you should notice how each approach offers trade-offs along four main dimensions: cost, control, realism, and ethics (Figure 4.11). No approach is the best in all situations.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/962240be37a2a0748a7950178e3fde23.png&#34; alt=&#34;fig4.11&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 4.11: Summary of trade-offs for different ways that you can make your experiment happen. By cost I mean cost to the researcher in terms of time and money. By control I mean the ability to do what you want in terms of recruiting participants, randomization, delivering treatments, and measuring outcomes. By realism I mean the extent to which the decision environment matches those encountered in everyday life; note that high realism is not always important for testing theories (Falk and Heckman 2009). By ethics I mean the ability of well-intentioned researchers to manage ethical challenges that might arise.&lt;/p&gt;

&lt;h3 id=&#34;4-5-1-just-do-it-yourself&#34;&gt;4.5.1 Just do it yourself&lt;/h3&gt;

&lt;p&gt;4.5.1.1 Use existing environments
4.5.1.2 Build your own experiment
4.5.1.3 Build your own product&lt;/p&gt;

&lt;h4 id=&#34;4-5-1-1-use-existing-environments&#34;&gt;4.5.1.1 Use existing environments&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;You can run experiments inside existing environments, often without any coding or partnership.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Logistically, the easiest way to do digital experiments is to overlay your experiment on top of an existing environment, enabling you to run a digital field experiment. These experiments can be run at a reasonably large scale and don’t require partnership with a company or extensive software development.&lt;/p&gt;

&lt;p&gt;For example, Jennifer Doleac and Luke Stein (2013) took advantage of an online marketplace (e.g., craigslist) to run an experiment that measured racial discrimination. Doleac and Stein advertised thousands of iPods, and by systematically varying the characteristics of the seller, they were able to study the effect of race on economic transactions. Further, Doleac and Stein used the scale of their experiment to estimate when the effect is bigger (heterogeneity of treatment effects) and offer some ideas about why the effect might occur (mechanisms).&lt;/p&gt;

&lt;p&gt;Prior to the study of Doleac and Stein, there had been two main approaches to experimentally measuring discrimination. In correspondence studies researchers create resumes of fictional people of different races and use these resumes to, for example, apply for different jobs. Bertrand and Mullainathan’s (2004) paper with the memorable title “Are Emily and Greg More Employable Than Lakisha and Jamal? A Field Experiment on Labor Market Discrimination” is a wonderful illustration of a correspondence study. Correspondence studies have relatively low cost per observation, which enables a single researcher to collect thousands of observations in a typical study. But, correspondence studies of racial discrimination have been questioned because names potentially signal many things in addition to the race of the applicant. That is, names such as Greg, Emily, Lakisha, and Jamal may signal social class in addition to race. Thus, any difference in treatment for resumes of Greg’s and Jamal’s might be due to more than presumed race differences of the applicants. Audit studies, on the other hand, involve hiring actors of different races to apply in person for jobs. Even though audit studies provide a clear signal of applicant race, they are extremely expensive per observation, which means that they typically only have hundreds of observations.&lt;/p&gt;

&lt;p&gt;In their digital field experiment, Doleac and Stein were able to create an attractive hybrid. They were able to collect data at relatively low cost per observation—resulting in thousands of observations (as in a correspondence study)—and they were able to signal race using photographs—resulting in a clear uncounfounded signal of race (as in an audit study). Thus, the online environment sometimes enables researchers to create new treatments that have properties that are hard to construct otherwise.&lt;/p&gt;

&lt;p&gt;The iPod advertisements of Doleac and Stein varied along three main dimensions. First, they varied the characteristics of the seller, which was signaled by the hand photographed holding the iPod &lt;a href=&#34;Figure 4.12&#34; target=&#34;_blank&#34;&gt;white, black, white with tattoo&lt;/a&gt;. Second, they varied the asking price [$90, $110, $130]. Third, they varied the quality of the ad text [high-quality and low-quality (e.g., cApitalization errors and spelin errors)]. Thus, the authors had a 3 X 3 X 2 design which was deployed across more than 300 local markets ranging from towns (e.g., Kokomo, IN and North Platte, NE) to mega-cities (e.g., New York and Los Angeles).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/4206cc43876039218a5002242d34b20e.png&#34; alt=&#34;4.12&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 4.12: Hands used in the experiment of Doleac and Stein (2013). iPods were sold by sellers with different characteristics to measure discrimination in an online marketplace.&lt;/p&gt;

&lt;p&gt;Averaged across all conditions, the outcomes were better for the white seller than the black seller, with the tattooed seller having intermediate results. For example, white sellers received more offers and had higher final sale prices. Beyond these average effects, Doleac and Stein estimated the heterogeneity of effects. For example, one prediction from earlier theory is that discrimination would be less in markets that are more competitive. Using the number of offers received as a proxy for market competition, the authors found that black sellers do indeed receive worse offers in markets with a low degree of competition. Further, by comparing outcomes for the ads with high-quality and low-quality text, Doleac and Stein found that ad quality does not impact the disadvantage faced by black and tattooed sellers. Finally, taking advantage of the fact that advertisements were placed in more than 300 markets, the authors find that black sellers are more disadvantaged in cities with high crime rates and high residential segregation. None of these results give us a precise understanding of exactly why black sellers had worse outcomes, but, when combined with the results of other studies, they can begin to inform theories about the causes of racial discrimination in different types of economic transactions.&lt;/p&gt;

&lt;p&gt;Another example that shows the ability of researchers to conduct digital field experiments in existing systems is the research by Arnout van de Rijt and colleagues (2014) on the keys to success. In many aspects of life, seemingly similar people end up with very different outcomes. One possible explanation for this pattern is that small—and essentially random—advantages can lock-in and grow over time, a process that researchers call cumulative advantage. In order to determine whether small initial successes lock-in or fade away, van de Rijt and colleagues (2014) intervened into four different systems bestowing success on randomly selected participants, and then measured the long-term impacts of this arbitrary success.&lt;/p&gt;

&lt;p&gt;More specifically, van de Rijt and colleagues 1) pledged money to randomly selected projects on kickstarter.com, a crowdfunding website; 2) positively rated randomly selected reviews on the website epinions; 3) gave awards to randomly chosen contributors to Wikipedia; and 4) signed randomly selected petitions on change.org. The researchers found very similar results across all four systems: in each case, participants that were randomly given some early success went on to have more subsequent success than their otherwise completely indistinguishable peers (Figure 4.13). The fact that the same pattern appeared in many systems increases the external validity of these results because it reduces the chance that this pattern is an artifact of any particular system.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/3fd0949079f480fd26586955866926bb.png&#34; alt=&#34;4.13&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 4.13: Long-term effects of randomly bestowed success in four different social systems. Arnout van de Rijt and colleagues (2014) 1) pledged money to randomly selected projects on kickstarter.com, a crowdfunding website; 2) positively rated randomly selected reviews on the website epinions; 3) gave awards to randomly chosen contributors to Wikipedia; and 4) signed randomly selected petitions on change.org.&lt;/p&gt;

&lt;p&gt;Together, these two examples show that researchers can conduct digital field experiments without the need to partner with companies or the need to build complex digital systems. Further, Table 4.2 provides even more examples that show the range of what is possible when researchers use the infrastructure of existing systems to deliver treatment and/or measure outcomes. These experiments are relatively cheap for researchers and they offer a high degree of realism. But, these experiments offer researchers limited control over the participants, treatments, and outcomes to be measured. Further, for experiments taking place in only one system, researchers need to be concerned that the effects could be driven by system-specific dynamics (e.g., the way that Kickstarter ranks projects or the way that change.org ranks petitions; for more information, see the discussion about algorithmic confounding in Chapter 2). Finally, when researchers intervene in working systems, tricky ethical questions emerge about possible harm to participants, non-participants, and systems. We will consider these ethical question in more detail in Chapter 6, and there is an excellent discussion of them in the appendix of van de Rijt (2014). The trade-offs that come with working in an existing system are not ideal for every project, and for that reason some researchers build their own experimental system, the topic of the next section.&lt;/p&gt;

&lt;p&gt;Table 4.2: Examples of experiments in existing systems. These experiments seem to fall into three main categories, and this categorization may help you notice additional opportunities for your own research. First, there are experiments that involve selling or buying something (e.g., Doleac and Stein (2013)). Second, there are experiments that involve delivering a treatment to specific participants (e.g., Restivo and Rijt (2012)). Finally, there are experiments that involve delivering treatments to specific objects such as petitions (e.g., Vaillant et al. (2015)).&lt;/p&gt;

&lt;p&gt;|Topic| Citation|
|&amp;ndash;|:&amp;ndash;|
|Effect of barnstars on contributions to Wikipedia| Restivo and Rijt (2012); Restivo and Rijt (2014); Rijt et al. (2014)|
|Effect of anti-harassment message on racist tweets|    Munger (2016)|
|Effect of auction method on sale price|    Lucking-Reiley (1999)|
|Effect of reputation on price in online auctions|  Resnick et al. (2006)|
|Effect of race of seller on sale of baseball cards on eBay |Ayres, Banaji, and Jolls (2015)|
|Effect of race of seller on sale of iPods  |Doleac and Stein (2013)|
|Effect of race of guest on Airbnb rentals  |Edelman, Luca, and Svirsky (2016)|
|Effect of donations on the success of projects on Kickstarter| Rijt et al. (2014)|
|Effect of race and ethnicity on housing rentals|   Hogan and Berry (2011)|
|Effect of positive rating on future ratings on epinions|   Rijt et al. (2014)|
|Effect of signatures on the success of petitions|  Vaillant et al. (2015); Rijt et al. (2014)|&lt;/p&gt;

&lt;h3 id=&#34;4-5-1-2-build-your-own-experiment&#34;&gt;4.5.1.2 Build your own experiment&lt;/h3&gt;

&lt;blockquote&gt;
&lt;p&gt;Building your own experiment might be costly, but it will enable you to create the experiment that you want.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;In addition to overlaying experiments on top of existing environments, you can also build your own experiment. The main advantage of this approach is control; if you are building the experiment, you can create the environment and treatments that you want. These bespoke experimental environments can create opportunities to test theories that are impossible to test in naturally occurring environments. The main drawbacks of building your own experiment are that it can be expensive and that the environment that you are able to create might not have the realism of a naturally occurring system. Researchers building their own experiment also must have a strategy for recruiting participants. When working in existing systems, researchers are essentially bringing the experiments to their participants. But, when researchers build their own experiment, they need to bring participants to it. Fortunately, services such as Amazon Mechanical Turk (MTurk) can provide researchers a convenient way to bring participants to their experiments.&lt;/p&gt;

&lt;p&gt;One example that illustrates the virtues of bespoke environments for testing abstract theories is the digital lab experiment by Gregory Huber, Seth Hill, and Gabriel Lenz (2012). The experiment explores a possible practical limitation to the functioning of democratic governance. Earlier non-experimental studies of actual elections suggest that voters are not able to accurately assess the performance of incumbent politicians. In particular, voters appear to suffer from three biases: 1) focused on recent rather than cumulative performance; 2) manipulatable by rhetoric, framing, and marketing; and 3) influenced by events unrelated to incumbent performance, such as the success of local sports team and the weather. In these earlier studies, however, it was hard to isolate any of these factors from all the other stuff that happens in real, messy elections. Therefore, Huber and colleagues created a highly simplified voting environment in order to isolate, and then experimentally study, each of these three possible biases.&lt;/p&gt;

&lt;p&gt;As I describe the experimental set-up below it is going to sound very artificial, but remember that realism is not a goal in lab-style experiments. Rather, the goal is to clearly isolate the process that you are trying to study, and this tight isolation is sometimes not possible in studies with more realism (Falk and Heckman 2009). Further, in this particular case, the researchers argued that if voters cannot effectively evaluate performance in this highly simplified setting, then they are not going to be able to do it in a more realistic, more complex setting.&lt;/p&gt;

&lt;p&gt;Huber and colleagues used Amazon Mechanical Turk (MTurk) to recruit participants. Once a participant provided informed consent and passed a short test, she was told that she was participating in a 32 round game to earn tokens that could be converted into real money. At the beginning of the game, each participant was told that she had been assigned an “allocator” that would give her free tokens each round and that some allocators were more generous than others. Further, each participant was also told that she would have a chance to either keep her allocator or be assigned a new one after 16 rounds of the game. Given what you know about Huber and colleagues’ research goals, you can see that the allocator represents a government and this choice represents an election, but participants were not aware of the general goals of the research. In total, Huber and colleagues recruited about 4,000 participants who were paid about $1.25 for a task that took about 8 minutes.&lt;/p&gt;

&lt;p&gt;Recall that one of the findings from earlier research was that voters reward and punish incumbents for outcomes that are clearly beyond their control, such as the success of local sports teams and the weather. To assess whether participants voting decisions could be influenced by purely random events in their setting, Huber and colleagues added a lottery to their experimental system. At either the 8th round or the 16th round (i.e., right before the chance to replace the allocator) participants were randomly placed in a lottery where some won 5000 points, some won 0 points, and some lost 5000 points. This lottery was intended to mimic good or bad news that is independent of the performance of the politician. Even though participants were explicitly told that the lottery was unrelated to the performance of their allocator, the outcome of the lottery still impacted participants’ decisions. Participants that benefited from the lottery were more likely to keep their allocator, and this effect was stronger when the lottery happened in round 16—right before the replacement decision—than when it happened in round 8 (Figure 4.14). These results, along with the results of several other experiments in the paper, led Huber and colleagues to conclude that even in a simplified setting, voters have difficulty making wise decisions, a result that impacted future research about voter decision making (Healy and Malhotra 2013). The experiment of Huber and colleagues shows that MTurk can be used to recruit participants for lab-style experiments to precisely test very specific theories. It also shows the value of building your own experimental environment: it is hard to imagine how these same processes could have been isolated so cleanly in any other setting.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/60b16912591752506ed1748f279ba448.png&#34; alt=&#34;fig4.14&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 4.14: Results from Huber, Hill, and Lenz (2012). Participants that benefited from the lottery were more likely to retain their allocator, and this effect was stronger when the lottery happened in round 16—right before the replacement decision—than when it happened in round 8.&lt;/p&gt;

&lt;p&gt;In addition to building lab-like experiments, researchers can also build experiments that are more field-like. For example, Centola (2010) built a digital field experiment to study the effect of social network structure on the spread of behavior. His research question required him to observe the same behavior spreading in populations that had different social network structures but were otherwise indistinguishable. The only way to do this was with a bespoke, custom-built experiment. In this case, Centola built a web-based health community.&lt;/p&gt;

&lt;p&gt;Centola recruited about 1,500 participants with advertising on health websites. When participants arrived at the online community—which was called the Healthy Lifestyle Network—they provided informed consent and then were assigned “health buddies.” Because of the way Centola assigned these health buddies he was able to knit together different social network structures in different groups. Some groups were built to have random networks (where everyone was equally likely to be connected) and other groups were built to have clustered networks (where connections are more locally dense). Then, Centola introduced a new behavior into each network, the chance to register for a new website with additional health information. Whenever anyone signed up for this new website, all of her health buddies received an email announcing this behavior. Centola found that this behavior—signing-up for the new website—spread further and faster in the clustered network than the random network, a finding that was contrary to some existing theories.&lt;/p&gt;

&lt;p&gt;Overall, building your own experiment gives you much more control; it enables you to construct the best possible environment to isolate what you want to study. It is hard to imagine how either of these experiments could have been performed in an already existing environment. Further, building your own system decreases ethical concerns around experimenting in existing systems. When you build your own experiment, however, you run into many of the problems that are encountered in lab experiments: recruiting participants and concerns about realism. A final downside is that building your own experiment can be costly and time-consuming, although as these examples show, the experiments can range from relatively simple environments (such as the study of voting by Huber, Hill, and Lenz (2012)) to relatively complex environments (such as the study of networks and contagion by Centola (2010)).&lt;/p&gt;

&lt;h4 id=&#34;4-5-1-3-build-your-own-product&#34;&gt;4.5.1.3 Build your own product&lt;/h4&gt;

&lt;blockquote&gt;
&lt;p&gt;Building your own product is high-risk, high-reward. But, if it works, you can benefit from a positive feedback loop that enables distinctive research.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Taking the approach of building your own experiment one step further, some researchers actually build their own products. These products attract participants, and then serve as platforms for experiments and other kinds of research. For example, a group of researchers at the University of Minnesota created MovieLens, which provides free, non-commercial personalized movie recommendations. MovieLens has operated continuously since 1997, and during this time 250,000 registered users have provided more than 20 million ratings about more than 30,000 movies (Harper and Konstan 2015). MovieLens has used the active community of users to conduct wonderful research ranging from testing social science theories about contributions to public goods (Beenen et al. 2004; Cosley et al. 2005; Chen et al. 2010; Ren et al. 2012) to addressing algorithmic challenges in recommendation systems (Rashid et al. 2002; Drenner et al. 2006; Harper, Sen, and Frankowski 2007; Ekstrand et al. 2015); for a full review see Harper and Konstan (2015). Many of these experiments would not have been possible without researchers having complete control over a real working product.&lt;/p&gt;

&lt;p&gt;Unfortunately, building your own product is incredibly difficult, and you should think of it like creating a start-up company: high-risk, high-reward. If it is successful, this approach offers much of the control that comes from building your own experiment with the realism and participants that come from working in existing systems. Further, this approach is potentially able to create a positive feedback loop where more research leads to a better product which leads to more users which leads to more researchers and so on (Figure 4.15). In other words, once a positive feedback loop kicks in, research should get easier and easier. Despite the potential upside to this approach, I can’t find any other examples of success, which shows just how difficult it is to execute successfully. But, my hope is that this strategy will become more practical as technology improves. These difficulties with creating your own product mean that researchers who want to control a product are much more likely to partner with a company, the topic I’ll address next.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/f23e89e2f607e76e4168659e5aca1246.png&#34; alt=&#34;fig4.15&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 4.15: If you can successfully build your own product, you can benefit from a positive feedback loop: research leads to a better product which leads to more users which leads to even more research. These kinds of positive feedback loops are incredibly difficult to create, but they can enable research that would not be possible otherwise. MovieLens is an example of a research project that has succeed in creating a positive feedback loop (Harper and Konstan 2015).&lt;/p&gt;

&lt;h3 id=&#34;4-5-2-partner-with-the-powerful&#34;&gt;4.5.2 Partner with the powerful&lt;/h3&gt;

&lt;p&gt;Partnering can reduce costs and increase scale, but it can alter the kinds of participants, treatments, and outcomes that you can use.
The alternative to doing it yourself is partnering with a powerful organization such as a company, government, or NGO. The advantage of working with a partner is that they can enable you to run experiments that you just can’t do by yourself. For example, one of the experiments that I’ll tell you about below involved 61 million participants; no individual researcher could achieve that scale. At the same time that partnering increases what you can do, it also, simultaneously, constrains you. For example, most companies will not allow you to run an experiment that could harm their business or their reputation. Working with partners also means that when it comes time to publish, you may come under pressure to “re-frame” your results, and some partners might even try to block the publication of your work if it makes them look bad. Finally, partnering also comes with costs related to developing and maintaining these collaborations.&lt;/p&gt;

&lt;p&gt;The core challenge that has to be solved to make these partnerships successful is finding a way to balance the interests of both parties, and a helpful way to think about that balance is Pasteur’s Quadrant (Stokes 1997). Many researchers think that if they are working on something practical—something that might be of interest to a partner—then they cannot be doing real science. This mindset will make it very difficult to create successful partnerships, and it also happens to be completely wrong. The problem with this way of thinking is wonderfully illustrated by the path-breaking research of biologist Louis Pasteur. While working on a commercial fermentation project to convert beet juice into alcohol, Pasteur discovered a new class of microorganism that eventually led to the germ theory of disease. This discovery solved a very practical problem—it helped improve the process of fermentation—and it lead to a major scientific advance. Thus, rather than thinking about research with practical applications as being in conflict with true scientific research, it is better to think of these as two separate dimensions. Research can be motivated by use (or not) and research can seek fundamental understanding (or not). Critically, some research—like Pasteur’s—can be motivated by use and seeking fundamental understanding (Figure 4.16). Research in Pasteur’s Quadrant—research that inherently advances two goals—is ideal for collaborations between researchers and partners. Given that background, I’ll describe two experimental studies with partnerships: one with a company and one with an NGO.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/a49ee82f2be8d2d24ab93dbcfe34562e.png&#34; alt=&#34;fig4.16&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 4.16: Pasteur’s Quadrant (based on Fig 3.5 from Stokes (1997)). Rather than thinking of research as either “basic” or “applied” it is better to think of research as motivated by use (or not) and seeking fundamental understanding (or not). An example of research that both is motivated by use and seeks fundamental understanding is Pasteur’s work on converting beet juice into alcohol which lead to the germ theory of disease. This is the kind of work that is best suited for partnerships with the powerful. Examples of work motivated by use but that does not seek fundamental understanding come from Thomas Edison, and examples of work that is not motivated by use but which seeks understanding come from Niels Bohr. See Stokes (1997) for a more thorough discussion of this framework and each of these cases.&lt;/p&gt;

&lt;p&gt;Large companies, particularly tech companies, have developed incredibly sophisticated infrastructure for running complex experiments. In the tech industry, these experiments are often called A/B tests (because they test the effectiveness of two treatments: A and B). These experiments are frequently run for things like increasing click-through rates on ads, but the same experimental infrastructure can also be used for research that advances scientific understanding. An example that illustrates the potential of this kind of research is a study conducted by a partnership between researchers at Facebook and the University of California, San Diego, on the effects of different messages on voter turnout (Bond et al. 2012).&lt;/p&gt;

&lt;p&gt;On November 2, 2010—the day of the US congressional elections—all 61 million Facebook users who live in the US and are over 18 took part in the experiment about voting. Upon visiting Facebook, users were randomly assigned into one of three groups, which determined what banner (if any) was placed at the top of their News Feed (Figure 4.17):&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;a control group.&lt;/li&gt;
&lt;li&gt;an informational message about voting with a clickable “I Voted” button and a counter (info).&lt;/li&gt;
&lt;li&gt;an informational message about voting with a clickable “I Voted” button and a counter + names and pictures of their friends who had already clicked the “I Voted” (info + social).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Bond and colleagues studied two main outcomes: reported voting behavior and actual voting behavior. First, they found that people in the info + social group were about 2 percentage points more likely than people in the info group to click “I Voted” (about 20% vs 18%). Further, after the researchers merged their data with publicly available voting records for about 6 million people they found that people in the info + social group were 0.39 percentage points more likely to actually vote than people in the control condition and that people in the info group just as likely to vote as people in the control condition (Figure 4.17).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/9969345f5b2085457dfe855371021a3c.png&#34; alt=&#34;fig4.17&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 4.17: Results from a get-out-the-vote experiment on Facebook (Bond et al. 2012). Participants in the info group voted at the same rate as people in the control condition, but people in the info + social group voted at a slightly higher rate. Bars represent estimated 95% confidence intervals. Results in the graph include about 6 million participants for whom researchers could match to voting records.&lt;/p&gt;

&lt;p&gt;This experiment shows that some online get-out-the-vote messages are more effective than others, and it shows that researcher’s estimate of the effectiveness of a treatment can depend on whether they study reported or actual behavior. This experiment unfortunately does not offer any clues about the mechanisms through which the social information—which some researchers have playfully called a “face pile”—increased voting. It could be that the social information increased the probability that someone noticed the banner or that it increased the probability that someone who noticed the banner actually voted or both. Thus, this experiment provides an interesting finding that further researcher will likely explore (see e.g., Bakshy, Eckles, et al. (2012)).&lt;/p&gt;

&lt;p&gt;In addition to advancing the goals of the researchers, this experiment also advanced the goal of the partner organization (Facebook). If you change the behavior studied from voting to buying soap, then you can see that the study has the exact same structure as an experiment to measure the effect of online ads (see e.g., Lewis and Rao (2015)). These ad effectiveness studies frequently measure the effect of exposure to online ads—the treatments in Bond et al. (2012) are basically ads for voting—on offline behavior. Thus, this study could advance Facebook’s ability to study the effectiveness of online ads and could help Facebook convince potential advertisers that Facebook ads are effective.&lt;/p&gt;

&lt;p&gt;Even though the interests of the researchers and partners were mostly aligned in this study, they were also partially in tension. In particular, the allocation of participants to the three conditions—control, info, and info + social—was tremendously imbalanced: 98% of the sample was assigned to info + social. This imbalanced allocation is inefficient statistically, and a much better allocation for the researchers would have have been &lt;sup&gt;1&lt;/sup&gt;&amp;frasl;&lt;sub&gt;3&lt;/sub&gt; of the participants in each group. But, the imbalanced allocation happened because Facebook wanted everyone to receive the info + social treatment. Fortunately, the researchers convinced them to hold back 1% for a related treatment and 1% of participants for a control group. Without the control group it would have been basically impossible to measure the effect of the info + social treatment because it would have been a “perturb and observe” experiment rather than a randomized controlled experiment. This example provides a valuable practical lesson for working with partners: sometimes you create an experiment by convincing someone to deliver a treatment and sometimes you create an experiment by convincing someone not to deliver a treatment (i.e., to create a control group).&lt;/p&gt;

&lt;p&gt;Partnership does not always need to involve tech companies and A/B tests with millions of participants. For example, Alexander Coppock, Andrew Guess, and John Ternovski (2016) partnered with an environmental NGO (League of Conservation Voters) to run experiments testing different strategies for promoting social mobilization. The researchers used the NGO’s Twitter account to send out both public tweets and private direct messages that attempted to prime different types of identities. The researchers then measured which of these messages were most effective for encouraging people to sign a petition and retweet information about a petition.&lt;/p&gt;

&lt;p&gt;Table 4.3: Examples of research that comes through partnership between researchers and organizations. In some cases, the researchers work at the organizations.
|Topic  |Citation|
|&amp;ndash;|:&amp;ndash;:|
|Effect of Facebook News Feed on information sharing|   Bakshy, Rosenn, et al. (2012)|
|Effect of partial anonymity on behavior on online dating website|  Bapna et al. (2016)|
|Effect of Home Energy Reports on electricity usage|    Allcott (2011); Allcott and Rogers (2014); Allcott (2015); Costa and Kahn (2013); Ayres, Raseman, and Shih (2013)|
|Effect of app design on viral spread   |Aral and Walker (2011)|
|Effect of spreading mechanism on diffusion|    Taylor, Bakshy, and Aral (2013)|
|Effect of social information in advertisements |Bakshy, Eckles, et al. (2012)|
|Effect of catalog frequency on sales through catalog and online for different types of customers   |Simester et al. (2009)|
|Effect of popularity information on potential job applications|    Gee (2015)|
|Effect of initial ratings on popularity|   Muchnik, Aral, and Taylor (2013)|
|Effect of message content on political mobilization|   Coppock, Guess, and Ternovski (2016)|&lt;/p&gt;

&lt;p&gt;Overall, partnering with the powerful enables to you operate at a scale that is hard to do otherwise, and Table 4.3 provides other examples of partnerships between researchers and organizations. Partnering can be much easier than building your own experiment. But, these advantages come with disadvantages: partnerships can limit the kinds of participants, treatments, and outcomes that you can study. Further, these partnerships can lead to ethical challenges. The best way to spot an opportunity for a partnership is to notice a real problem that you can solve while you are doing interesting science. If you are not used to this way of looking at the world, it can be hard to spot problems in Pasteur’s Quadrant, but with practice, you’ll start to notice them more and more.&lt;/p&gt;

&lt;h2 id=&#34;4-6-advice&#34;&gt;4.6 Advice&lt;/h2&gt;

&lt;p&gt;Whether you are doing it yourself or working with a partner, I’d like to offer two pieces of advice that I’ve found particularly helpful in my own work. First, think as much as possible before any data has been collected. This advice probably seems obvious to researchers accustomed to running experiments, but it is very important for researchers accustomed to working with big data sources (see Chapter 2). With big data sources most of the work happens after you have the data, but experiments are the opposite; most of the work should happen before you collect data. One of the best ways to force yourself to think carefully about your design and analysis is to create and register an analysis plan for your experiment. Fortunately, many of the best-practices for the analysis of experimental data have been formalized into reporting guidelines, and these guidelines are a great place to start when creating your analysis plan (Schulz et al. 2010; Gerber et al. 2014; Simmons, Nelson, and Simonsohn 2011).&lt;/p&gt;

&lt;p&gt;The second piece of advice is that no one experiment is going to be perfect, and because of that, you should try to design a series of experiments that reinforce each other. I’ve even heard this described as the armada strategy; rather than trying to build one massive battleship, you might be better building lots of smaller ships with complementary strengths. These kinds of multi-experiment studies are routine in psychology, but they are rare elsewhere. Fortunately, the low cost of some digital experiments makes these kind of multi-experiment studies easier.&lt;/p&gt;

&lt;p&gt;Also, I’d like to offer two pieces of advice that are less common now but are particularly important for designing digital age experiments: create zero marginal cost data and build ethics into your design.&lt;/p&gt;

&lt;h3 id=&#34;4-6-1-create-zero-variable-cost-data&#34;&gt;4.6.1 Create zero variable cost data&lt;/h3&gt;

&lt;p&gt;The key to running large experiments is driving your variable cost to zero. The best ways to do this are automation and designing enjoyable experiments.
Digital experiments can have dramatically different cost structures and this enables researchers to run experiments that were impossible in the past. More specifically, experiments generally have two main types of costs: fixed costs and variable costs. Fixed costs are costs that don’t change depending on how many participants you have. For example, in a lab experiment, fixed costs might be the cost of renting the space and buying furniture. Variable costs, on the other hand, change depending on how many participants you have. For example, in a lab experiment, variable costs might come from paying staff and participants. In general, analog experiments have low fixed costs and high variable costs, and digital experiments have high fixed costs and low variable costs (Figure 4.18). With appropriate design, you can drive the variable cost of your experiment all the way to zero, and this can create exciting research opportunities.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/0beabea46f340524c6cdc5b0121035c9.png&#34; alt=&#34;fig4.18&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 4.18: Schematic of cost structures in analog and digital experiments. In general, analog experiments have low fixed costs and high variable costs whereas digital experiments have high fixed costs and low variable costs. The different cost structures mean that digital experiments can run at a scale that is not possible with analog experiments.&lt;/p&gt;

&lt;p&gt;There are two main elements of variable cost—payments to staff and payments to participants—and each of these can be driven to zero using different strategies. Payments to staff stem from the work that research assistants do recruiting participants, delivering treatments, and measuring outcomes. For example, the analog field experiment of Schultz and colleagues (2007) on social norms and electricity usage required research assistants to travel to each home to deliver the treatment and read the electric meter (Figure 4.3). All of this effort by research assistants meant that adding a new household to the study would have added to the cost. On the other hand, for the digital field experiment of Restivo and van de Rijt (2012) on rewards in Wikipedia, researchers could add more participants at virtually no cost. A general strategy for reducing variable administrative costs is to replace human work (which is expensive) with computer work (which is cheap). Roughly, you can ask yourself: can this experiment run while everyone on my research team is sleeping? If the answer is yes, you’ve done a great job of automation.&lt;/p&gt;

&lt;p&gt;The second main type of variable cost is payments to participants. Some researchers have used Amazon Mechanical Turk and other online labor markets to decrease the payments that are needed for participants. To drive variable costs all the way to zero, however, a different approach is needed. For a long time, researchers have designed experiments that are so boring they have to pay people to participate. But, what if you could create an experiment that people want to be in? This may sound far fetched, but I’ll give you an example below from my own work, and there are more examples in Table 4.4. Note that this approach to designing enjoyable experiments echoes some of the themes in Chapter 3 regarding designing more enjoyable surveys and in Chapter 5 regarding the design of mass collaboration. Thus, I think that participant enjoyment—what might also be called user experience—will be an increasingly important part of research design in the digital age.&lt;/p&gt;

&lt;p&gt;Table 4.4: Examples of experiments with zero variable cost that compensated participants with a valuable service or an enjoyable experience.&lt;/p&gt;

&lt;p&gt;|Compensation|  Citation|
|&amp;ndash;|:&amp;ndash;|
|Website with health information|   Centola (2010)|
|Exercise program|  Centola (2011)|
|Free music |Salganik, Dodds, and Watts (2006); Salganik and Watts (2008); Salganik and Watts (2009b)|
|Fun game|  Kohli et al. (2012)|
|Movie recommendations  |Harper and Konstan (2015)|&lt;/p&gt;

&lt;p&gt;If you want to create zero variable costs experiments you’ll want to ensure that everything is fully automated and that participants don’t require any payments. In order to show how this is possible, I’ll describe my dissertation research on the success and failure of cultural products. This example also shows that zero variable cost data is not just about doing things cheaper. Rather, it is about enabling experiments that would not be possible otherwise.&lt;/p&gt;

&lt;p&gt;My dissertation was motivated by the puzzling nature of success for cultural products. Hit songs, best selling books, and blockbuster movies are much, much more successful than average. Because of this, the markets for these products are often called “winner-take-all” markets. Yet, at the same time, which particular song, book, or movie will become successful is incredibly unpredictable. The screenwriter William Goldman (1989) elegantly summed up lots of academic research by saying that, when it comes to predicting success, “nobody knows anything.” The unpredictability of winner-take-all markets made me wonder how much of success is a result of quality and how much is just luck. Or, expressed slightly differently, if we could create parallel worlds and have them all evolve independently, would the same songs become popular in each world? And, if not, what might be a mechanism that causes these differences?&lt;/p&gt;

&lt;p&gt;In order to answer these questions, we—Peter Dodds, Duncan Watts (my dissertation advisor), and I—ran a series of online field experiments. In particular, we built a website called MusicLab where people could discover new music, and we used it for a series of experiments. We recruited participants by running banner ads on a teen-interest website (Figure 4.19) and through mentions in the media. Participants arriving at our website provided informed consent, completed a short background questionnaire, and were randomly assigned to one of two experimental conditions—independent and social influence. In the independent condition, participants made decisions about which songs to listen to, given only the names of the bands and the songs. While listening to a song, participants were asked to rate it after which they had the opportunity (but not the obligation) to download the song. In the social influence condition, participants had the same experience, except they could also see how many times each song had been downloaded by previous participants. Furthermore, participants in the social influence condition were randomly assigned to one of eight parallel worlds each of which evolved independently (Figure 4.20). Using this design, we ran two related experiments. In the first, we presented participants the songs in an unsorted grid, which provided them a weak signal of popularity. In the second experiment, we presented the songs in a ranked list, which provided a much stronger signal of popularity (Figure 4.21).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/5f71d72d7ae7e652ad7afd0734cebc0b.png&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 4.19: An example of banner ad that my colleagues and I used to recruit participants for the MusicLab experiments (Salganik, Dodds, and Watts 2006).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/3771e0db83e0a618bf25b63b43c23c44.png&#34; alt=&#34;4.20&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 4.20: Experimental design for the MusicLab experiments (Salganik, Dodds, and Watts 2006). Participants were randomly assigned into one of two conditions: independent and social influence. Participants in the independent condition made their choices without any information about what other people had done. Participants in the social influence condition were randomly assigned into one of eight parallel worlds, where they could see the popularity—as measured by downloads of previous participants—of each song in their world, but they could not see any information, nor did they even know about the existence of, any of the other worlds.&lt;/p&gt;

&lt;p&gt;We found that the popularity of the songs differed across the worlds suggesting an important role of luck. For example, in one world the song “Lockdown” by 52Metro came in 1st, and in another world it came in 40th out of 48 songs. This was exactly the same song competing against all the same songs, but in one world it got lucky and in the others it did not. Further, by comparing results across the two experiments we found that social influence leads to more unequal success, which perhaps creates the appearance of predictability. But, looking across the worlds (which can’t be done outside of this kind of parallel worlds experiment), we found that social influence actually increased the unpredictability. Further, surprisingly, it was the songs of highest appeal that have the most unpredictable outcomes (Figure 4.22).&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/e94ce5d114bdd8c5211c9f999fa72e72.png&#34; alt=&#34;fig4.21&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 4.21: Screenshots from the social influence conditions in the MusicLab experiments (Salganik, Dodds, and Watts 2006). In the social influence condition in experiment 1, the songs, along with the number of previous downloads, were presented to the participants arranged in a 16 X 3 rectangular grid, where the positions of the songs were randomly assigned for each participant. In experiment 2, participants in the social influence condition were shown the songs, with download counts, presented in one column in descending order of current popularity.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/886b5f6274254b6bd0d4bb6a4449fc1e.png&#34; alt=&#34;fig4.22&#34; /&gt;&lt;/p&gt;

&lt;p&gt;Figure 4.22: Results from the MusicLab experiments showing the relationship between appeal and success (Salganik, Dodds, and Watts 2006). The x-axis is the market share of the song in the independent world, which serves as a measure of the appeal of the song, and the y-axis is the market share of the same song in the 8 social influence worlds, which serves as a measure of the success of the songs. We found that increasing the social influence that participants experienced—specifically, the change in layout from experiment 1 to experiment 2 (Figure 4.21)—caused success to become more unpredictable, especially for the highest appeal songs.&lt;/p&gt;

&lt;p&gt;MusicLab was able to run at essentially zero variable cost because of the way that it was designed. First, everything was fully automated so it was able to run while I was sleeping. Second, the compensation was free music so there was no variable participant compensation cost. The use of music as compensation also illustrates how there is sometimes a trade-off between fixed costs and variable costs. Using music increased the fixed costs because I had to spend time securing permission from the bands and preparing reports for the bands about participants’ reaction to their music. But, in this case, increasing fixed costs in order to decrease variables costs was the right thing to do; that’s what enabled us to run an experiment that was about 100 times larger than a standard lab experiment.&lt;/p&gt;

&lt;p&gt;Further, the MusicLab experiments show that zero variable cost does not have to be an end in itself; rather, it can be a means to running a new kind of experiment. Notice that we did not use all of our participants to run a standard social influence lab experiment 100 times. Instead, we did something different, which you could think of as switching from a psychological experiment to a sociological experiment (Hedström 2006). Rather than focusing on individual decision-making, we focused our experiment on popularity, a collective outcome. This switch to a collective outcome meant that we required about 700 participants to produce a single data point (there were 700 people in each of the parallel worlds). That scale was only possible because of the cost structure of the experiment. In general, if researchers want to study how collective outcomes arise from individual decisions, group experiments such as MusicLab are very exciting. In the past, they have been logistically difficult, but those difficulties are fading because of the possibility of zero variable cost data.&lt;/p&gt;

&lt;p&gt;In addition to illustrating the benefits of zero variable cost data, the MusicLab experiments also show a challenge with this approach: high fixed costs. In my case, I was extremely lucky to be able to work with a talented web developer named Peter Hausel for about six months to construct the experiment. This was only possible because my advisor, Duncan Watts, had received a number of grants to support this kind of research. Technology has improved since we built MusicLab in 2004, and it would be much easier to build an experiment like this now. But, high fixed cost strategies are really only possible for researchers who can somehow cover those costs.&lt;/p&gt;

&lt;p&gt;In conclusion, digital experiments can have dramatically different cost structures than analog experiments. If you want to run really large experiments, you should try to decrease your variable cost as much as possible and ideally all the way to 0. You can do this by automating the mechanics of your experiment (e.g., replacing human time with computer time) and designing experiments that people want to be in. Researchers who can design experiments with these features will be able to run new kinds of experiments that were not possible in the past.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>学校医保</title>
      <link>https://chengjunwang.com/en/note/2017-07-02-health-care/</link>
      <pubDate>Sun, 02 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>https://chengjunwang.com/en/note/2017-07-02-health-care/</guid>
      <description>

&lt;p&gt;南字发〔2017〕80号&lt;/p&gt;

&lt;p&gt;各院系、各单位：
为进一步加强我校公费医疗管理工作，结合工作实际，学校对《南京大学公费医疗管理暂行办法》（南字发〔2006〕151 号）进行了修订完善，形成了《南京大学教职工公费医疗管理办法》。现印发给你们，请遵照执行。
特此通知。&lt;/p&gt;

&lt;p&gt;附件：南京大学教职工公费医疗管理办法
南京大学
2017 年 6 月 26 日&lt;/p&gt;

&lt;p&gt;为进一步加强我校公费医疗管理工作，结合工作实际，特对《南京大学公费医疗管理暂行办法》（南字发〔2006〕151 号）进行修订和完善，形成本办法。
公费医疗遵循国家、单位和个人共同负担的基本原则，在国家给予拨款的基础上，部分自筹经费，保障教职工的基本医疗需求，合理使用有限的医疗经费。&lt;/p&gt;

&lt;h2 id=&#34;第一条-适用范围&#34;&gt;第一条 适用范围&lt;/h2&gt;

&lt;p&gt;1.我校事业在编教职工（留职停薪人员除外）和离退休人员。
2.按照学校人力资源处规定，给予享受学校公费医疗待遇的特殊人员。
3.由各单位交费参加公费医疗的企业编制职工的医疗待遇，按原办法执行，交费标准按人均公费医疗支出调整。&lt;/p&gt;

&lt;h2 id=&#34;第二条-挂钩医院&#34;&gt;第二条 挂钩医院&lt;/h2&gt;

&lt;blockquote&gt;
&lt;p&gt;江苏省人民医院、江苏省中医院、江苏省肿瘤医院、南京军区总医院、南京鼓楼医院、南京市口腔医院、江苏省口腔医院、南京市胸科医院、南京市脑科医院、南京市妇幼保健医院、南京市第二人民医院、泰康仙林鼓楼医院。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;第三条-特需医院&#34;&gt;第三条 特需医院&lt;/h2&gt;

&lt;p&gt;在紧急情况下，病人就近选择急诊的非挂钩医院以及医
保定点的康复病院、养老院。&lt;/p&gt;

&lt;h2 id=&#34;第四条-就诊的有关规定&#34;&gt;第四条 就诊的有关规定&lt;/h2&gt;

&lt;p&gt;1.教职工患病应首先在校医院就诊，凭本人校园卡挂号。
2.因病情需要，需转上级医院诊治的，必须经校医院转诊到挂钩医院就诊，否则医疗费用自理。
3.急诊病人无需转诊，挂钩医院按正常比例报销，非挂钩医院按特需医院报销。
4.住医保定点的康复病院、养老院产生的药费和检查费按特需医院报销，其他费用自理。
5.教职工因公出差在异地（境内）生病就诊，凭派遣函和所在单位证明，医疗费用按挂钩医院报销。
6.离退休人员回原籍、长期探亲或长期外派工作人员离开本市半年以上，须事先在医疗保险管理办公室申请、备案，选择当地一所社区医院和一所二级以上的公立医院定点，同时取消本地所有挂钩医院；当地医疗费用按挂钩医院报销。
7.特殊疑难病例，需到外地医院就诊或会诊者，须凭本市挂钩医院医务处的转诊证明，经医疗保险管理办公室批准后，可转院诊治并按挂钩医院报销，未办理转诊手续者，医疗费用自理。
8.各类人员出国（境）工作、学习、生活期间，发生的一切医疗费用全部自理。&lt;/p&gt;

&lt;h2 id=&#34;第五条-医疗费报销有关规定&#34;&gt;第五条 医疗费报销有关规定&lt;/h2&gt;

&lt;p&gt;1.公费医疗的药品报销范围按《南京大学公费医疗药品报销范围》执行。
2.各类人员医疗费用自付比例如下：&lt;/p&gt;

&lt;p&gt;校医院 挂钩医院 特需医院
在职人员 10% 15% 报销总额减半
退休人员 10% 15% 报销总额减半
离休人员、院士、
副省级干部 0 0 报销总额减半&lt;/p&gt;

&lt;p&gt;3.在南京市医保范围内的检查项目，按正常比例报销。
4.材料费单价在 100 元以下的按正常比例报销，单价在100 元以上的报销 50%，离休人员、院士、副省级干部报销60%。
5.特殊治疗（如射频消融、χ-刀、γ-刀、激光治疗、高压氧、粒子射线治疗、机器人手术以及医保范围的靶向治疗等新技术）给予报销 50%，离休人员、院士、副省级干部报销 60%；器官移植者的器官供体费用自理；透析治疗费按90%报销；特别护理、心电、血压、血糖等监护（测）费按增加自付比例 20%报销。
6.住院床位费规定：正常报销标准 50 元/日，副高（副处）及以上按照 80 元/日报销，院士、副省级按照 150 元/日报销。
7.自理费用：参照南京市医保。
8.经确诊为精神分裂症、癌症的病人，凭“三甲”医院疾病诊断证明、病理检查报告单，符合公费医疗报销范围内的住院医药费全额报销，门诊医药费按正常比例报销。
9.实行计划生育的育龄女教职工，上环、取环、人流、结扎、引产等符合计划生育技术服务基本项目的手术费及检查费，经计划生育办公室审定后，按 100%报销。孕期建小卡及药物流产等医疗费用自理。
10.个人自投商业医疗保险，按正常报销比例测算，原则上为公费医疗报销加保险赔款总额不超过患者公费医疗范围内医疗费总额。&lt;/p&gt;

&lt;h2 id=&#34;第六条&#34;&gt;第六条&lt;/h2&gt;

&lt;p&gt;教职工子女参加南京市城镇居民基本医疗保险，每年个人自付金额超过 1500 元（即从 1501 元起），按男单女双的原则，凭南京市医保定点医院就诊刷卡的发票，医保范围内的自付部分，由学校给予 50%的补助，全年最高补助额 5000 元。&lt;/p&gt;

&lt;h2 id=&#34;第七条&#34;&gt;第七条&lt;/h2&gt;

&lt;p&gt;除上述规定外，其它未尽事宜，参照南京市医保相关报销规定执行。&lt;/p&gt;

&lt;h2 id=&#34;第八条&#34;&gt;第八条&lt;/h2&gt;

&lt;p&gt;本办法由学校医疗保险工作领导小组负责解释。&lt;/p&gt;

&lt;h2 id=&#34;第九条&#34;&gt;第九条&lt;/h2&gt;

&lt;p&gt;本办法自发布之日起施行，我校被地方政府接纳参加城镇职工医保后，本办法自行废止。&lt;/p&gt;

&lt;p&gt;南京大学校长办公室 2017 年 6 月 26 日印发&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>紫金楼</title>
      <link>https://chengjunwang.com/en/note/2017-06-14-school-building/</link>
      <pubDate>Wed, 14 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>https://chengjunwang.com/en/note/2017-06-14-school-building/</guid>
      <description>&lt;p&gt;紫金楼终于来了，现在正在封顶。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/a7bebcfc69fce0b45bb6c304e76ef6f0.png&#34; alt=&#34;紫金楼&#34; /&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;根据中共江苏省委宣传部与南京大学签订的共建新闻传播学院协议，经教育部可研评估批复，南京大学新闻传播学院楼已完成开工前手续办理，于近日正式开工建设。&lt;/p&gt;

&lt;p&gt;2016年4月25日下午，江苏省委宣传部和南京大学新闻传播学院共建的紫金楼奠基仪式在仙林校区举行，省委宣传部部长王燕文、南京大学党委书记张异宾等领导和新闻传播学院师生代表共同见证紫金楼奠基。&lt;/p&gt;

&lt;p&gt;据了解，新闻传播学院楼坐落于仙林校区文科组团北部，总建筑面积&lt;strong&gt;10137.8&lt;/strong&gt;平方米，建筑高度17.3米，为地上&lt;strong&gt;四层&lt;/strong&gt;的框架结构。项目总投资6400万元，由江苏广播电视集团负责建设，预计于&lt;strong&gt;2018年3月底&lt;/strong&gt;竣工验收投入使用。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/fd096961204e0a0039e7e902db0f89ec.png&#34; alt=&#34;示意图&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/e370fd20ef0c33d7bf17948903164b29.png&#34; alt=&#34;示意图&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>实验中心2017年相关事项</title>
      <link>https://chengjunwang.com/en/note/2017-05-07-lab-thinktank/</link>
      <pubDate>Sun, 07 May 2017 00:00:00 +0000</pubDate>
      
      <guid>https://chengjunwang.com/en/note/2017-05-07-lab-thinktank/</guid>
      <description>

&lt;h2 id=&#34;国内外计算传播学方向发展的的观察&#34;&gt;国内外计算传播学方向发展的的观察&lt;/h2&gt;

&lt;p&gt;学校很关心我们的计算传播学科崛起，请近期考虑引进若干名专职科研岗的具体推进方法！包括广告内容、宣传渠道、工作任务。可否你在黄俊楠配合下，这两天给一个初步思路。我们来积聚资源。&lt;/p&gt;

&lt;p&gt;互联网大数据使得计算社会科学等相关领域迅猛发展，国内外传播学院校也开始拥抱这一发展潮流。南京大学新闻传播学院于2015年2月正式建立计算传播学实验中心，加州大学戴维斯分校传播系成立了&lt;a href=&#34;http://c2.ucdavis.edu/&#34; target=&#34;_blank&#34;&gt;计算传播研究实验室&lt;/a&gt;，领风气之先。其它高校虽然没有使用计算传播作为为名字，但是也大幅度招收采用大数据和计算科学进行传播学研究的学者。例如，宾州大学&lt;a href=&#34;https://www.asc.upenn.edu/people/faculty&#34; target=&#34;_blank&#34;&gt;安尼斯伯格传播学院&lt;/a&gt;也挖来了Damon Centola和Sandra González-Bailón；斯坦福大学传播系招来了&lt;a href=&#34;http://jenpan.com/&#34; target=&#34;_blank&#34;&gt;Jennifer Pan&lt;/a&gt;；国内高校以&lt;a href=&#34;http://sjc.bnu.edu.cn/sztd/index.html&#34; target=&#34;_blank&#34;&gt;北京师范大学&lt;/a&gt;（吴晔、徐敬宏、王蕊）、&lt;a href=&#34;https://mp.weixin.qq.com/s?__biz=MzA5MTExNDMyNw==&amp;amp;mid=210877062&amp;amp;idx=1&amp;amp;sn=e35b796e7fab3074221dd197a1bc4140&amp;amp;version=12020810&#34; target=&#34;_blank&#34;&gt;中山大学&lt;/a&gt;发展最为迅速，尤其是北京师范大学通过跨学科招人的方式获得跳跃式发展。&lt;/p&gt;

&lt;h2 id=&#34;关于公开招聘新闻传播学院专职科研人员的启事-计算传播学研究方向&#34;&gt;关于公开招聘新闻传播学院专职科研人员的启事（计算传播学研究方向）&lt;/h2&gt;

&lt;p&gt; 
南京大学新闻传播学院因工作需要，现公开招聘计算传播学研究方向专职科研人员3名。计算传播学实验研究中心隶属于南京大学新闻传播学院，创建于2015年2月，主要使命在于挖掘互联网数据，分析人类传播行为。我们的愿景是寻找人类传播行为的可计算基因，主要关注传播网络分析、媒介文本挖掘、数据新闻、计算广告、数学模型、编程工具以及开放数据。经过两年多的发展，目前中心已经建成基础计算平台，包括小型计算集群、大内存服务器、数据存储平台、网络服务器，同时储备了大量数字媒体相关大数据资源，在注意力流动网络等领域发表了一系列研究成果。&lt;/p&gt;

&lt;p&gt;计算传播是指数据驱动的、借助于计算方法所进行的传播过程。分析计算传播现象的研究领域即计算传播学。计算传播学是计算社会科学的重要分支，主要关注人类传播行为的计算基础；以传播网络分析、传播文本挖掘、数学建模等为主要分析工具；大规模地收集并分析人类传播行为数据；挖掘背后的模式和法则；分析模式背后的生成机制与基本原理；被广泛地应用于数据新闻和计算广告等场景；注重编程训练、数学建模、计算思维。将人类传播行为（开放数据）可计算化的转化过程依赖于我们的研究方法（数学模型、编程工具），专注于我们的研究领域（传播网络分析、媒介文本挖掘），而应用于数据新闻和计算广告领域。&lt;/p&gt;

&lt;p&gt;一、岗位描述：&lt;/p&gt;

&lt;p&gt;主要从事计算传播学相关研究和教学工作，独立开展科研工作以及研究课题，努力取得一定的基础和应用研究成果；具备一定的数据科学基础，熟悉Python或其它计算工具；积极与中心团队科研合作；每学期开设至少一门课程；每年以南京大学为第一单位，在相关领域发表SCI/SSCI论文不少于1篇，或CSSCI不少于3篇；参与组织计算传播学年会；积极开展国内外学术交流；积极参加科学普及与社会服务活动，积极承担校、所安排的其他工作。&lt;/p&gt;

&lt;p&gt;二、应聘资格：
1、具有较强的计算传播学研究能力和较好的论文写作能力。
2、具有新闻学、传播学、社会学、政治学、计算机科学、物理学或其它相关学科的博士学位，35周岁以下；
3、具备一定的数据科学基础，熟悉Python或其它用于海量数据分析的编程语言；&lt;/p&gt;

&lt;p&gt;三、工资待遇
本岗位为南京大学专职科研岗位，相关政策详见人力资源处网站《南京大学专职科研系列岗位聘任办法（试行）》文件。&lt;/p&gt;

&lt;p&gt;四、申请程序及提交材料
凡有意应聘者请将个人简历、学历学位证书复印件、科研成果目录等相关材料寄送至南京大学新闻传播学院。确定正式申请后再依规定提交其他材料。&lt;/p&gt;

&lt;p&gt;五、聘后管理
1. 对招聘到上述岗位的人员按学校相关规定管理，年薪??万不等，年均奖励或科研补助??万元左右。
2. 专职科研系列岗位以聘用制聘用，专职从事科学研究工作，聘期三年。优秀的专职科研系列人员，聘期内或聘期结束前，可以应聘学校副教授及以上岗位。&lt;/p&gt;

&lt;p&gt;五、联系方式
通信地址：南京市鼓楼区汉口路22号，南京大学新闻传播学院
邮政编码：210093
联系人：黄老师
联系电话：025-83686260&lt;/p&gt;

&lt;h1 id=&#34;大力建设具有学科领先意义的实验室&#34;&gt;大力建设具有学科领先意义的实验室&lt;/h1&gt;

&lt;p&gt;2）加大科研硬件投入。围绕不断发展的互联网技术，不断改造现有科研实验室，淘汰或彻底改造已经落后于技术发展的电话调查实验室，改进和完善正在建设的计算传播学实验室，兴建媒介认知心理学实验室与数据云实验室，以一系列在国内领域具有领先意义的学科实验室支持最前沿的新媒体研究。&lt;/p&gt;

&lt;p&gt;2）大力建设具有学科领先意义的实验室。以手机、互联网为代表的数字媒体技术迅猛发展，一方面为新闻传播研究方法提出巨大挑战，另一方面也提供了“弯道超车”的发展机遇。通过加大科研硬件投入，完善正在建设的计算传播学实验中心，兴建媒介认知心理学实验室、数据云实验室，彻底改造或淘汰不适应新媒介环境的电话调查实验室，增强新闻传播学研究对大数据的计算、分析和挖掘能力。&lt;/p&gt;

&lt;h1 id=&#34;全球华人思想库&#34;&gt;全球华人思想库&lt;/h1&gt;

&lt;blockquote&gt;
&lt;p&gt;全球华人思想库这个项目，资金到位了。现在需要开发一个工具，1、便于全球分主题投稿、整理、档案管理。2、自动生成分主题的元研究：学术搜索和整理综述。你看这个开发怎么做？需要多久？哪些成本？目前急于要开设备清单，以便李晓愚申述双创经费。你们可否在仙林或电话聊一下？只能买硬件。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;ul&gt;
&lt;li&gt;投稿系统，可以考虑博客系统&lt;/li&gt;
&lt;li&gt;Web服务器， 建设搜索引擎&lt;/li&gt;
&lt;li&gt;自动主题生成&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;硬件&#34;&gt;硬件&lt;/h3&gt;

&lt;p&gt;大数据实验室是计算传播学研究中心的一个重要组成部分。这个实验室目前落到了我和fm两个做project leader，系里从奥美研究院拿出50万支持这个实验室的硬件设施。&lt;/p&gt;

&lt;p&gt;它的组成包括了分布式数据抓取和计算平台、共享计算平台、数据存储平台和实验室网站服务器四个部分组成。另外，接受计算士的意见，我决定购买2-3台macbook。&lt;/p&gt;

&lt;h3 id=&#34;软件&#34;&gt;软件&lt;/h3&gt;

&lt;p&gt;巢老师的意见是要实验室表现出传播力。因为我们从论文发表到资源积累，都落后与其他文科院系。但是即便是这些院系中的相关实验室，也较少为外界所知。这不得不让我好奇@北大新媒体这个微博账号是如何运营的啦。&lt;/p&gt;

&lt;p&gt;同时他也告诉我近期的发展，实验室的人手缺乏是重要的问题。我想模仿城大的策略：吸引优秀的本系和外系本科生。目前南大鼓楼主要是商法传播物理这些院系。我想通过一些方式（读书会、沙龙）来吸引更多的本科生加入。采用一定的考核机制，实验室可以成为他们学习的空间，但他们必须参加实验室组会和其他活动，并承担一定的义务。&lt;/p&gt;

&lt;p&gt;按照他的设想，11月完成硬件配置；11月和12月共组织四次活动；我在明年准备开设计算传播学导论的课程。&lt;/p&gt;

&lt;p&gt;我将把我的一些合作者列为实验室的合作成员。参与weblab的组会、与计算士的午间讨论、与信纸的合作项目都可以展开了。&lt;/p&gt;

&lt;p&gt;同时我自己的论文也迫在眉睫啦。&lt;/p&gt;

&lt;h3 id=&#34;如何订购实验室电脑&#34;&gt;如何订购实验室电脑&lt;/h3&gt;

&lt;p&gt;按要求填写申购贵重仪器设备可行性论证报告，需要寻找三家销售代理，细化装备的金额情况。在Hp和dell官网打电话找销售代理，打电话。看了apple中国的报价，以老师或学生身份购买会便宜很多。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>计算传播学年会工作坊组办</title>
      <link>https://chengjunwang.com/en/note/2017-04-01-conference-host/</link>
      <pubDate>Sat, 01 Apr 2017 00:00:00 +0000</pubDate>
      
      <guid>https://chengjunwang.com/en/note/2017-04-01-conference-host/</guid>
      <description>

&lt;h1 id=&#34;2017年计算传播学年会暨工作坊&#34;&gt;2017年计算传播学年会暨工作坊&lt;/h1&gt;

&lt;p&gt;计算社会科学研究范式对传播学研究产生了重要影响。熟悉计算社会科学研究范式并掌握基本的计算技能，是传播学者进入计算计算传播学领域的基本条件。基于此，南京大学新闻与传播学院、百度、社会媒体处理专业委员会以及香港城市大学互联网挖掘实验室拟共同举办2017年计算传播学工作坊。本次计算传播学年会暨工作坊共2.5天时间，包括：一天半工作坊 + 一天会议。&lt;/p&gt;

&lt;h2 id=&#34;会议帐号&#34;&gt;会议帐号&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://nic.nju.edu.cn/9003/list.htm&#34; target=&#34;_blank&#34;&gt;https://nic.nju.edu.cn/9003/list.htm&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;仅事业在编教职工、博士后、预任职员可开通此功能。资费为：50 线程，200 元/ 天；200 线程，800 元/ 天；500 线程，2000 元/ 天&lt;/p&gt;

&lt;p&gt;办理办法：本人登录OA 、“请示报告”，将填写完整的《南京大学会议网络帐号申请表（ 2016 试用版）   》作为附件上传，提交网络信息中心审核。&lt;/p&gt;

&lt;h2 id=&#34;内容分工&#34;&gt;内容分工&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;网络科学基础&amp;amp;信息扩散的计算因素（python+networkx基础）成军&lt;/li&gt;
&lt;li&gt;单条信息传播的测量&amp;amp;机器学习框架 小可&lt;/li&gt;
&lt;li&gt;信息传播中的节点重要性&amp;amp;可视化 张伦&lt;/li&gt;
&lt;li&gt;行为的传播机制 海波&lt;/li&gt;
&lt;li&gt;信息传播模型 海波、成军、阮中远&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&#34;标准&#34;&gt;标准&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;以工作坊为契机，为教学课程、科研、书籍做准备。&lt;/li&gt;
&lt;li&gt;有一个编程实现的例子作为遴选标准&lt;/li&gt;
&lt;li&gt;使用notebook&lt;a href=&#34;http://nbviewer.jupyter.org/github/computational-class/cjc/blob/gh-pages/slides/01.slides.slides.html&#34; target=&#34;_blank&#34;&gt;做slides，一个例子&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;deadline&#34;&gt;Deadline&lt;/h2&gt;

&lt;p&gt;7月15日提交使用jupyter notebook&lt;/p&gt;

&lt;h3 id=&#34;使用notebook制作slides的方法&#34;&gt;使用notebook制作slides的方法&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;安装Anaconda Python&lt;/li&gt;
&lt;li&gt;Downnload from &lt;a href=&#34;https://github.com/damianavila/RISE&#34; target=&#34;_blank&#34;&gt;https://github.com/damianavila/RISE&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;open your teminal, cd to the RISE folder, e.g.,
&lt;code&gt;cd /github/RISE/&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;To install this nbextension, simply run
&lt;code&gt;python setup.py install&lt;/code&gt;
from the RISE repository.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In the notebook toolbar, a new button (&amp;ldquo;Enter/Exit Live Reveal Slideshow&amp;rdquo;) will be available. The notebook toolbar also contains a &amp;ldquo;Cell Toolbar&amp;rdquo; dropdown menu that gives you access to metadata for each cell.&lt;/p&gt;

&lt;p&gt;If you select the Slideshow preset, you will see in the right corner of each cell a little box where you can select the cell type (similar as for the static reveal slides with &lt;code&gt;nbconvert&lt;/code&gt;).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>大学财务报销</title>
      <link>https://chengjunwang.com/en/note/2017-03-20-ndcw/</link>
      <pubDate>Mon, 20 Mar 2017 11:18:42 +0800</pubDate>
      
      <guid>https://chengjunwang.com/en/note/2017-03-20-ndcw/</guid>
      <description>

&lt;h1 id=&#34;财务处网站&#34;&gt;财务处网站&lt;/h1&gt;

&lt;p&gt;蓝鲸大学财务网站 &lt;a href=&#34;http://ndcw.nju.edu.cn/&#34; target=&#34;_blank&#34;&gt;http://ndcw.nju.edu.cn/&lt;/a&gt; 终于搞明白了，好像只有校内网络才能登录!!!&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/fa541268765759fd854e4b24d77d0c59.png&#34; alt=&#34;网上自助平台&#34; /&gt;&lt;/p&gt;

&lt;p&gt;如何发放专家咨询费？&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;学生劳务费&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;我居然用了这么长的时间才搞明白这个系统，主要是自己一直在家里待着，用不了这个系统。2333&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;专家咨询费

&lt;ul&gt;
&lt;li&gt;在&lt;strong&gt;收入申报&lt;/strong&gt;部分填写!!!&lt;/li&gt;
&lt;li&gt;选择”校外专家”&lt;/li&gt;
&lt;li&gt;选择“校外人员信息采集”录入银行卡信息&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;已经填写了张伦的专家咨询费。填写了生成的表格，学院盖章后交给财务处报销。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/0a72061a524c8ac8211c8aa0ed0d6d32.png&#34; alt=&#34;收入申报&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;公务卡&#34;&gt;公务卡&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://pan.baidu.com/s/1dF5ShNz&#34; target=&#34;_blank&#34;&gt;公务卡报销过程中的问题解答&lt;/a&gt; 2017年3月16日&lt;/p&gt;

&lt;p&gt;2017-05-31在仙林校区外的工行激活了我的公务卡，在图书馆里使用公务卡买了火车票两张，还是挺方便的。后面去桔子酒店预订住宿一晚。希望早日学会使用公务卡消费。&lt;/p&gt;

&lt;p&gt;消费满25日内还款还是有压力的吖，要去跑财务处了。&lt;/p&gt;

&lt;p&gt;先在财务处网站，编制公务卡消费记录。然后，点击差旅生成公务卡消费表格。具体流程如下：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;登录“网上自助平台”&lt;/li&gt;
&lt;li&gt;点击”网上报账”&lt;/li&gt;
&lt;li&gt;点击“公务卡”&lt;/li&gt;
&lt;li&gt;然后，选择“日常报销”、”差旅报销”、“本市交通报销”中的一项生成表格，保存为pdf&lt;/li&gt;
&lt;li&gt;学院财务一支笔签字盖章“公务卡消费表格”&lt;/li&gt;
&lt;li&gt;出差的话，还需要填写”出差审批表”和“预算表”&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;校外接入功能的设置和使用&#34;&gt;校外接入功能的设置和使用&lt;/h2&gt;

&lt;p&gt;&lt;a href=&#34;https://nic.nju.edu.cn/e1/29/c8913a188713/page.htm&#34; target=&#34;_blank&#34;&gt;https://nic.nju.edu.cn/e1/29/c8913a188713/page.htm&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;电话咨询了网络中心技术人员，对方不熟悉mac电脑！！！说可能是其他vpn修改了注册表。建议我重新安装，我重新安装了之后，依然没有什么作用。要考虑在windows下来完成这个过程了。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/bf89148b1a6e799991676c29f2d37a96.png&#34; alt=&#34;virtualbox&#34; /&gt;&lt;/p&gt;

&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;p&gt;通过virtualbox可以方便地实现bras连接，如上图打开了财务处网站。设置了一个共享文件夹。&lt;/p&gt;

&lt;/div&gt;


&lt;blockquote&gt;
&lt;p&gt;In a Windows guest, shared folders are browseable and therefore visible in Windows Ex- plorer. So, to attach the host’s shared folder to your Windows guest, open Windows Ex- plorer and look for it under “My Networking Places” -&amp;gt; “Entire Network” -&amp;gt; “VirtualBox Shared Folders”.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;1）上宽带；
2）首次使用时，须先在电脑上建立“南大BRAS校外拨入”的VPN连接。根据您的操作系统，查看校外拨入连接的设置办法：WIN10系统 WIN8系统  WIN7系统 WIN XP系统      Macintosh 系统
  - 另：WINXP等旧系统可下载 BRAS客户端.rar ，解压后安装，并重启机器。注意：连接时，必须勾选 □off-campus 选项&lt;/p&gt;

&lt;p&gt;3）需在校外访问校内图书馆资源时，点击“南大BRAS校外拨入”进行连接。&lt;br /&gt;
4）无需访问校内图书馆资源时，请断开BRAS连接。&lt;/p&gt;

&lt;p&gt;注：本操作方法在苹果leopard10.5.5和10.5.6中测试通过。
如有疑问或无法成功设置，请联系网络信息中心 89683791 。&lt;/p&gt;

&lt;p&gt;当使用BRAS校外拨入功能时，您需要在Macintosh系统上如下操作：&lt;/p&gt;

&lt;p&gt;1、在菜单中点击System Preferences，打开Network选项,点击左下角“”＋”号，新建一个“VPN”连接；&lt;/p&gt;

&lt;p&gt;2、选择连接类型为“l2tp over ipsec”，写入该连接的名称（任意); 在“server address ”填入地址：218.94.142.114; 在“account name”中填入您的bras账号；&lt;/p&gt;

&lt;p&gt;3、打开“advanced”选项：并勾选上“send all traffic over vpn connection”；&lt;/p&gt;

&lt;p&gt;4、在/etc/ppp文件夹（可能需要提供系统安装时的密码）下建一个文件，文件名为：options  ( 注意，该文件名没有后缀！)，文件内容见options内容（两行文字）；
方法之一：打开MAC系统的“终端（Terminal）”，然后用命令行方式操作（如： cd /etc/ppp 目录，nano 或 vi 一个文件options，将两行文字写入该文件）。编辑文件前，可能需要先取得root权限。&lt;/p&gt;

&lt;p&gt;5、在系统网络连接中选择前面建立的连接，点击“connect”，在弹出对话框中输入您的上网账号密码即可上网。&lt;/p&gt;

&lt;p&gt;更新：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/52c7fd7e6fa6f96e160adcb6a9a5eeb3.png&#34; alt=&#34;下论文&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这样虽然不能进入财务处页面，但是可以下载论文。&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>博士后出站</title>
      <link>https://chengjunwang.com/en/note/2017-7-10-postdoc/</link>
      <pubDate>Wed, 15 Mar 2017 12:00:00 +0000</pubDate>
      
      <guid>https://chengjunwang.com/en/note/2017-7-10-postdoc/</guid>
      <description>

&lt;p&gt;居然有十八个表格要填！faint，暑期也搞不完了。&lt;/p&gt;

&lt;h1 id=&#34;博士后&#34;&gt;博士后&lt;/h1&gt;

&lt;ol&gt;
&lt;li&gt;魏强 8968559&lt;/li&gt;
&lt;li&gt;学院导师签字，政府管理学院张冰老师盖章。&lt;/li&gt;
&lt;li&gt;提交《中国博士后科学基金资助总结报告》&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
&lt;p&gt;您好！在站期间您获得了博士后基金资助。根据《中国博士后科学基金资助规定》，您需提交《中国博士后科学基金资助总结报告》。
请登录中国博士后科学基金管理信息系统填写《中国博士后科学基金资助总结报告》，并与&lt;strong&gt;出站报告&lt;/strong&gt;一并请&lt;strong&gt;设站单位管理人员网上审核&lt;/strong&gt;，审核通过后方可办理出站手续。
以上完成后可继续点击【提交申请】。&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&#34;博士后出站&#34;&gt;博士后出站&lt;/h2&gt;

&lt;ol&gt;
&lt;li&gt;流动站自主招收出站申请材料清单  2016-11-24&lt;/li&gt;
&lt;li&gt;流动站自主招收博士后出站申请流程图  2016-10-12&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;a href=&#34;http://hr.nju.edu.cn/6334/list.htm&#34; target=&#34;_blank&#34;&gt;http://hr.nju.edu.cn/6334/list.htm&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/c8b912ac26c1680f0a73b8df7f405355.png&#34; alt=&#34;博士后出站&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.chinapostdoctor.org.cn/WebSite/program/InfoCate_Show.aspx?INFOCATEGORYID=website003005001&#34; target=&#34;_blank&#34;&gt;http://www.chinapostdoctor.org.cn/WebSite/program/InfoCate_Show.aspx?INFOCATEGORYID=website003005001&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;注：办理进(出、退)站手续时，请您按要求携带“上传材料”和“纸质材料”中列出各项材料（&lt;strong&gt;1份原件和2份复印件&lt;/strong&gt;），并且按要求上传原件电子数据扫描件。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;红色&lt;/strong&gt;上传 为必需上传材料;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;灰色&lt;/strong&gt;上传为&lt;strong&gt;可选&lt;/strong&gt;上传材料;&lt;/li&gt;
&lt;li&gt;没有上传按钮的为个人携带材料。&lt;/li&gt;
&lt;li&gt;上传后的材料可删除后重新上传。为确保顺利办理进出站手续，请下载并使用系统“电子数据”和“纸质材料”中的表格。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;发表论文&#34;&gt;发表论文&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Wang, C.J., Wu, L*, Zhang, J., Janssen, M. (2016) The Collective Direction of Attention Diffusion. Scientific Reports. 6: 34059. doi:10.1038/srep34059&lt;/li&gt;
&lt;li&gt;Wang, C.J., Wu, L.*(2016) The Scaling of Attention Networks. Physica A: Statistical Mechanics and its Applications.448:196–204, doi: 10.1016/j.physa.2015.12.081&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;博士后退站&#34;&gt;博士后退站&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;http://hr.nju.edu.cn/6342/list.htm&#34; target=&#34;_blank&#34;&gt;http://hr.nju.edu.cn/6342/list.htm&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&#34;办理落户&#34;&gt;办理落户&lt;/h2&gt;

&lt;p&gt;今天青椒村外来务工人员一个狗去公社人力资源部办理了申请集体户口迁移的第一道手续，路过公社保卫处户籍科门口的时候，一只红脖子的鸟站在树上红了眼眶使劲叫，一个狗听了半天没听懂，当然因为它说的是鸟语。&lt;/p&gt;

&lt;p&gt;到鼓楼校区北京西路附近的机械大厦找鲜峰办理博士后集体户口申请，去鼓楼保卫处户籍科办理户籍介绍的证明，然后去鼓楼公安局的办证大厅办理迁入申请，然后回老家迁移户口，最后到大方巷的鼓楼派出所办理入户。&lt;/p&gt;

&lt;h2 id=&#34;流动站自主招收博士后出站申请材料清单&#34;&gt;流动站自主招收博士后出站申请材料清单&lt;/h2&gt;

&lt;p&gt;发布者：贾静发布时间：2017-04-18浏览次数：294&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;博士后研究人员工作 期满登记表 （一式3份）
在“中国博士后”网站网上填报，自动生成下载后打印； 其中，该表中的出站单位与地址必须填写，若尚未落实单位，须填写档案寄往单位（外籍人员回国的除外）。&lt;/li&gt;
&lt;li&gt;接收单位意见表/接收函 （一式3份）
统招统分博士后由出站单位出具，需加盖接收单位人事公章；在职博士后回原单位，不需提供。&lt;/li&gt;
&lt;li&gt;身份证复印件（正反）&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;博士后研究人员工作期满 业务考核表 （一式3份）&lt;/li&gt;
&lt;li&gt;博士后研究人员工作期满 审批表 （一式3份）&lt;/li&gt;
&lt;li&gt;联系导师对博士后研究人员研究 工作评价表 （一式2份）
由流动站合作导师签字盖章&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;南京大学博士后期满离站科研 工作评审表 （一式2份）
由流动站组织答辩&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;南京大学博士后离站 个人总结表 （一式2份）
流动站责任人签署意见，签字并盖章&lt;/li&gt;
&lt;li&gt;《博士后研究人员更换资助项目负责人申请表》 （一式2份）&lt;/li&gt;
&lt;li&gt;《江苏省博士后科研资助计划项目总结报告》 （一式2份）&lt;/li&gt;
&lt;li&gt;《江苏省博士后科研资助计划项目经费决算表》 （一式2份）
须到学校财务处盖章确认&lt;/li&gt;
&lt;li&gt;《中国博士后科学基金资助总结报告》 （一式2份）；
提交网上申请，并到学校财务处盖章确认，报校人资处审核（该表审核通过后方可提交网上出站申请）&lt;/li&gt;
&lt;li&gt;南京大学教职工离校手续表 （一式1份）
 需相关部门盖章&lt;/li&gt;
&lt;li&gt;南京大学博士后研究工作报告 （一式两份）
与博士毕业论文格式相似，包括封面（胶装）、目录、内容&lt;/li&gt;
&lt;li&gt;出站博士后科研 成果汇总表 （一式1份）&lt;/li&gt;
&lt;li&gt;在站期间发表论文 （一式1份）
包括封皮、目录、首页复印件及获奖情况&lt;/li&gt;
&lt;li&gt;结婚证、独生子女证（或子女出生证）复印件（各2份）；&lt;br /&gt;
统招博士后请提供3  份身份证复印件 （身份证须正反复印）
18.博士学位证书复印件（一式1份）；博士后工作证原件&lt;br /&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;注：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1）中国博士后系统的附件必须为原件 的扫描或拍照上传；&lt;/li&gt;
&lt;li&gt;2）表格4-11、13、15可到南京大学人力资源处官网-博士后-表格下载处下载打印；&lt;/li&gt;
&lt;li&gt;3) 把第1、2、3、4、5项表格装订成份,共上报3份，至少一份为原件；&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;4）表格6、7、8装订成份，共两份，至少一份为原件；&lt;/li&gt;
&lt;li&gt;5）表格9，博士后在站期间取得江苏省博士后科研资助计划或中国博士后面上资助，且出站时未完成资助项目的情况下填写；&lt;/li&gt;
&lt;li&gt;6）表格9、10、11、12，如博士后在站期间未获得过江苏省博士后科研资助计划和中国博士后基金项目，不必填写；&lt;/li&gt;
&lt;li&gt;7）表格13，文科类博士后不需要盖“科技处”公章；理工科类不需要盖“社会科学处”公章；&lt;/li&gt;
&lt;li&gt;9）所有书面材料请用A4纸打印复印，保证至少一份是原件。&lt;/li&gt;
&lt;li&gt;10）出站申请递交至省人社厅审批通过后，会以短信形式告知申请人通过审批，并携材料办理相关手续。而申请人本人不用再次携带材料办理手续，已经代为办理。&lt;/li&gt;

&lt;li&gt;&lt;p&gt;8）关于迁移户口的说明：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;A、落本人房产人员须提交《不动产权证书》或《房屋产权证》,尚未取得《不动产权证书》或《房屋产权证》的可提供《购房合同》和物业公司出具的《入住证明》。&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;B、配偶、子女办理落户所需材料：&lt;/li&gt;
&lt;li&gt;（1）配偶、子女《户口簿》首页及《常住人口登记卡》；&lt;/li&gt;
&lt;li&gt;（2）配偶“居民身份证”、《结婚证》；&lt;/li&gt;
&lt;li&gt;（3）子女《出生医学证明》、《独生子女证》或《生育服务证》（说明：多胎子女及2016年1月1日后出生的子女提供生育服务证）；&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;C、离异博士后研究人员办理子女落户需提交《离婚证》、离婚判决书或经当地民政局盖章（公证处公证）的《离婚协议书》；&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;D、在港、澳、台及国外出生的婴儿，还须提交国外或境外医疗机构出具的出生证明原件、复印件，翻译机构出具的出生证明翻译件以及我驻外使领馆签发的《中华人民共和国旅行证》或《护照》。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/e6bcbf8165d82f1f54ad0fdcf584f6ae.png&#34; alt=&#34;办户口材料&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;今日单词&#34;&gt;今日单词&lt;/h2&gt;

&lt;p&gt;集体户口 collective registered residences.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/b999b0e1ab9238e3d85cd69ebf37913a.png&#34; alt=&#34;博士后办理落户&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/5e047fc89d894689ebf4f06ad5e9a180.png&#34; alt=&#34;户口&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/25fa2aaaf74fa15345273794f8afcd69.png&#34; alt=&#34;身份证&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>找工作</title>
      <link>https://chengjunwang.com/en/note/2017-03-15-job/</link>
      <pubDate>Wed, 15 Mar 2017 12:00:00 +0000</pubDate>
      
      <guid>https://chengjunwang.com/en/note/2017-03-15-job/</guid>
      <description>

&lt;h1 id=&#34;关于2017年度增列硕士生导师工作的通知&#34;&gt;关于2017年度增列硕士生导师工作的通知&lt;/h1&gt;

&lt;p&gt;7月30日收到胡老师电话，提醒我提交硕士生资格申请，而8月31日是最后期限。&lt;/p&gt;

&lt;p&gt;根据《南京大学关于增列硕士生导师资格评定的暂行办法》（修订稿）精神，2017年度硕士生导师的申报正常进行，申请人6月30日前将材料提交所在院系学位评定分委员会，院系8月31日前将讨论通过的申请材料交学位办。&lt;/p&gt;

&lt;p&gt;硕导申报需要提交以下纸质材料：
(1）增列硕士生导师表决票(附统计表)；&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;(2)《南京大学申请培养硕士学位研究生指导教师审批表》一式三份；&lt;/li&gt;
&lt;li&gt;(3)申请人正式发表或出版的代表作的复印件及国内、外对本人从事或指导的有代表性的研究成果的评价(如有，提供复印件)；&lt;/li&gt;
&lt;li&gt;（4）发表文章的检索证明。理工科SCI图书馆查新中心出具的报告（包括清单），文科社科评价中心出具的CSSCI检索报告（包括清单）；&lt;/li&gt;
&lt;li&gt;（5）承担的科研项目任务批准书（复印件）。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;材料2、3、4、5由申请人提交给院系。&lt;/p&gt;

&lt;p&gt;院系讨论后将表决票及通过的申请人材料送学位办　&lt;/p&gt;

&lt;p&gt;增列办法及申报表详见附件。&lt;/p&gt;

&lt;p&gt;　　　　　　　　　　　　　　　　　　　　　　      研究生院学位办
　　　　　　　　　　　　　　　　　　　　　　　　　2017.5.25&lt;/p&gt;

&lt;h1 id=&#34;公示&#34;&gt;公示&lt;/h1&gt;

&lt;p&gt;20170712，请于2017年7月19日前与南京大学人力资源处联系。&lt;/p&gt;

&lt;p&gt;63 新闻传播学院 副教授 王成军 AP20162051&lt;/p&gt;

&lt;p&gt;具体信息见&lt;a href=&#34;https://hr.nju.edu.cn/_upload/article/files/8c/6c/a093aa634e38b13caaea36b32734/f0b31a9f-784d-4936-ad28-6e2ea64812bb.pdf&#34; target=&#34;_blank&#34;&gt;这里&lt;/a&gt;。&lt;/p&gt;

&lt;h1 id=&#34;高级职称评审会议&#34;&gt;高级职称评审会议&lt;/h1&gt;

&lt;p&gt;各院系、各单位：
2016年教师高级岗位专业技术职务述职通知时间如下：&lt;/p&gt;

&lt;p&gt;新闻传播学院  AP20162051 副教授  王成军 1986/10/11  15:06   仙林校区行政北楼409&lt;/p&gt;

&lt;p&gt;教授述职时间共12分钟（8分钟述职+4分钟提问），副教授述职时间共8分钟（6分钟述职+2分钟提问）。时间安排在本周六（7月1日），地点在仙林校区行政北楼会议室。上午8:30开始教授述职，下午13:30开始副教授述职。请各单位务必通知到上述每一位申报教师按预计时间提前15分钟到场做好准备，还没有发送ppt的老师请尽快将ppt于明天中午12点前发送至邮箱huwener@nju.edu.cn，过期不再接收，如有疑问请及时跟我们联系。谢谢！&lt;/p&gt;

&lt;p&gt;联系人：周恒、刁忠弈
联系电话：89683309、89683046&lt;/p&gt;

&lt;p&gt;人力资源处培训发展办公室&lt;/p&gt;

&lt;p&gt;各位老师好：
    现通知本周六（7月1日）上午8:30开始，在仙林校区进行2016年度教师高级职称述职，上午教授，下午副教授，请各位老师通知相关申报人做好准备，具体述职时间、地点我们明后天会再通知。谢谢！
人力资源处&lt;/p&gt;

&lt;p&gt;拟定于六月底、七月初进行各系列高级职称评审会议（全部在仙林校区），述职与提问时间&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;教授：8+4min，&lt;/li&gt;
&lt;li&gt;副教授、教育管理、思政和其他系列高级述职时间：6+2min。&lt;/li&gt;
&lt;li&gt;PPT文件名请人事秘书老师按“申报序列编号_申报职称_单位名称_姓名”存放并打包

&lt;ul&gt;
&lt;li&gt;申报序列编号：教师01，实验工程02，图书编辑03，教育管理04，思想政治05；&lt;/li&gt;
&lt;li&gt;例如：01_教授_文学院_XXX&lt;/li&gt;
&lt;li&gt;如有视频，不要通过邮件发送。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;另外，请通知述职人自备优盘和述职材料述职当天带到述职现场，以备不时之需。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;请各位老师及时通知到各系列申报人员，做好述职准备以及ppt，各单位人事秘书老师邮件于&lt;strong&gt;6月27日中午12点前&lt;/strong&gt;发送至huwener@nju.edu.cn。具体安排述职地点及时间另行通知，谢谢！&lt;/p&gt;

&lt;p&gt;人力资源处培训发展办公室&lt;/p&gt;

&lt;h2 id=&#34;主要优势&#34;&gt;主要优势&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;王成军博士毕业于香港城市大学，曾在澳大利亚国立大学访学半年和在腾讯研究院实习数月，从中广泛接触到国际学术界和互联网产业的一系列前沿理论、方法、工具和经典应用案例。&lt;/li&gt;
&lt;li&gt;读博时间曾在SSCI传播学顶级期刊Cyberpsychology上发表了一篇有关Twitter网上社会运动信息扩散的论文；在南京大学任职之后，又先后发表了多篇论文，合著了《社交网络上的计算传播学》。其中包括在自然出版集团旗下的刊物Scientific Reports上发表的关于注意力流动的论文,这也是国内新闻传播学领域的学者首次在这样的刊物中发表论文。&lt;/li&gt;
&lt;li&gt;这些论文和专著的数量和质量，如严谨性、创新性、挑战性等，不仅在国内传播学青年教师中、甚至在亚太地区相同传播学者中，都是名列前茅的。&lt;/li&gt;
&lt;li&gt;“计算传播学”是一个新闻传播学、计算机科学、网络科学等多学科交叉的研究领域,也是国际学术界较为关注的一个重要领域。&lt;/li&gt;
&lt;li&gt;成军是国际传播学界内最早提出“计算传播学”（Computational Communication）概念的学者之一，为宣传、普及和发展计算传播学作出了诸多重要贡献。他参与创建了国内第一家计算传播学研究机构:南京大学计算传播学实验中心,并担任其中奥美数据科学实验室主任。自2016年起，每年在南京大学负责筹备一次全国计算传播学年会，有效地奠定了南京大学在计算传播学领域的领先地位。&lt;/li&gt;
&lt;li&gt;王成军博士治学严谨、成果丰硕，已在国内传播学界崭露头角，符合并超越了一流大学副教授的标准。&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;述职报告&#34;&gt;述职报告&lt;/h1&gt;

&lt;p&gt;各位评委老师，大家好。我是王成军，来自南京大学新闻传播学院。&lt;/p&gt;

&lt;p&gt;我的述职报告的题目是《计算社会科学视角下的传播学研究》，具体包括四个部分：个人简介、研究成果、教学服务、工作计划。&lt;/p&gt;

&lt;h3 id=&#34;第一部分个人简介&#34;&gt;第一部分个人简介&lt;/h3&gt;

&lt;p&gt;我本科毕业于兰州大学，硕士毕业于北京大学，博士毕业于香港城市大学。
2014年9月加入南京大学新闻传播学院，担任奥美数据科学实验室主任，计算传播学实验中心成员。
我的主要研究兴趣聚焦于计算传播学，包括但不限于：注意力流动网络、公共讨论，团队科学。
更多信息参见我的个人网站。&lt;/p&gt;

&lt;h3 id=&#34;第二部分是研究成果&#34;&gt;第二部分是研究成果&lt;/h3&gt;

&lt;p&gt;共发表了六篇英文论文，包括四篇SCI/SSCI一作或通讯、两篇SCI/SSCI三作。此外，还有十六篇国际会议论文以及部分中文论文。
四篇论文当中包括两篇自然出版集团期刊Scientific reports，一篇Physica A， 还有一篇Cyberpyschology。
这些期刊的影响因子均较高，例如，Scientific Reports的影响因子是5.228， Cyberpyschology在2013年影响因子是2.41，在传播学72本国际期刊中排名第四。&lt;/p&gt;

&lt;p&gt;其中，Tracing the attention of moving citizens一文是中国新闻传播学科在自然出版集团旗下刊物发表的第一篇研究成果，也是在注意力流动网络方向的第一篇正式研究成果。&lt;/p&gt;

&lt;p&gt;瓦茨在2007年提出“基于因特网的传播和互动数据可变革我们对于人类群体行为的理解”，他认为社会科学将成为21世纪的科学。
Lazer等一批学者2009年在Science上发表题为“计算社会科学”的文章，标志着计算社会科学的出现；无独有偶数字人文也开始出现。
如何将从计算社会科学的视角出发进行传播学研究成为重要问题？
对此我提出了“计算传播学”的发展思路，发表了几篇关于计算传播学的中文论文，尝试确立计算传播学的概念和目标。&lt;/p&gt;

&lt;p&gt;参与翻译了使用Python进行社会网络分析的书籍。
合著了《社交网络上的计算传播学》一书。&lt;/p&gt;

&lt;p&gt;探索计算传播学的研究路径，即从社会科学的重大问题出发，收集大规模的数据，采用成熟的算法，建立简单的模型，解决实际问题。例如：
- 这四篇文章分别试图总结注意力流动的模式、方向、和人类移动的关联、公共讨论的异质性；
- 分别采用了百度贴吧的数据、digg社交新闻网站数据、北京移动用户数据、推特数据；
- 开发或运用已经成熟的算法，尤其是流网络分析的算法；
- 建立了一系列简单的数学或物理学模型，例如tracing一文采用的是随机几何图模型。&lt;/p&gt;

&lt;p&gt;研究项目方面，我主持了一项国家社会科学基金青年项目、一项博士后科学基金面上项目，并参与了一个自然科学基金项目。&lt;/p&gt;

&lt;h3 id=&#34;第三部分是教学和社会服务&#34;&gt;第三部分是教学和社会服务。&lt;/h3&gt;

&lt;p&gt;同样主要围绕建设计算传播学社区展开
在过去三年里为研究生和本科生课设《计算传播》、《数据新闻》、《计算广告》相关课程。&lt;/p&gt;

&lt;p&gt;2015年参与创建了计算传播学实验中心，
并发起建立了计算传播网。&lt;/p&gt;

&lt;p&gt;2015年主办了集智注意力科学年会；
2016年主办了第一届计算传播学论坛；
2017年该会议发展为计算传播学年会，为期3天，并包括一个两天的工作坊。&lt;/p&gt;

&lt;p&gt;积极参与计算传播学跨学科的交流。例如：
- 组织sunbelt2017计算传播学分论坛；
- 组织中国信息学会SMP计算传播组征文；
- 此外我参加了南京大学数字人文小组，参与组织南京大学数字人文大会。&lt;/p&gt;

&lt;p&gt;在期刊审稿方面，担任多家国内外期刊的审稿人；
在软件开发方面，开发了三个python软件包和一个R软件包。&lt;/p&gt;

&lt;h3 id=&#34;最后一部分是未来的工作计划&#34;&gt;最后一部分是未来的工作计划。&lt;/h3&gt;

&lt;p&gt;除完成副高岗位要求之外，我打算在以下方面做出贡献：
教学方面，继续开设《数据新闻》、《计算传播》等课程。还将于2017年开设233改革硕士品牌课程 《大数据挖掘与分析》；
科研方面，定位发表高影响因子期刊的英文论文、兼顾中文论文；出版《计算传播学导论》一书。
社会服务方面，继续举办“计算传播学工作坊和年会”，积极参与学校智库和双创建设。
在力所能及的范围内，冲击登峰B计划\青年长江等人才计划，为学科发展做出贡献。&lt;/p&gt;

&lt;p&gt;我的述职汇报完毕，感谢聆听，欢迎提问！&lt;/p&gt;

&lt;h1 id=&#34;找工作&#34;&gt;找工作&lt;/h1&gt;

&lt;p&gt;蓝鲸大学&lt;a href=&#34;http://hr.nju.edu.cn/&#34; target=&#34;_blank&#34;&gt;人力资源处&lt;/a&gt;3月8日发布&lt;a href=&#34;http://hr.nju.edu.cn/c6/33/c5979a181811/page.htm&#34; target=&#34;_blank&#34;&gt;2016年教授、副教授招聘公告&lt;/a&gt;, 其中新传院的招聘公告包括了&lt;a href=&#34;http://hrmis.nju.edu.cn/urp-data/rss/post-detail.jsp?post=P20161044&amp;amp;isElse=&#34; target=&#34;_blank&#34;&gt;教授要求&lt;/a&gt; 和&lt;a href=&#34;http://hrmis.nju.edu.cn/urp-data/rss/post-detail.jsp?post=AP20162051&amp;amp;isElse=&#34; target=&#34;_blank&#34;&gt;副教授（校外）要求&lt;/a&gt;。看了一下细节，达到了副教授的基本要求。15日问了下学院黄老师，直接按照网页的要求填写即可，学院还没有收到具体通知。&lt;/p&gt;

&lt;p&gt;首先按照这个&lt;a href=&#34;http://219.219.114.94/urp-pg/&#34; target=&#34;_blank&#34;&gt;网址&lt;/a&gt;填写校外申报的材料,  使用Winebottler的IE浏览器未果，结果发现使用FireFox可以填写！但是不能打印。IE7或者IE11都不能打印。最后发现图书馆里用的浏览器是Win10系统里的&lt;strong&gt;Microsoft EDGE 20&lt;/strong&gt;，可以打印, 原来也不是IE。&lt;/p&gt;

&lt;p&gt;一天后（3月16日），学院人事通知最好能在3月24日前交齐材料，后面院系还有一系列公示，并给了一系列可以下载填写的资料和详细纸质版本报送方式，详见&lt;a href=&#34;http://hr.nju.edu.cn/cb/85/c5977a183173/page.htm&#34; target=&#34;_blank&#34;&gt;关于2016年教师高级职务岗位评聘有关工作的通知中的&lt;strong&gt;下载资料&lt;/strong&gt;&lt;/a&gt;，其中需要注意的是，有关应聘人员做好准备（提供PPT），述职时间6分钟。&lt;/p&gt;

&lt;h2 id=&#34;to-do-list&#34;&gt;To-do list&lt;/h2&gt;

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; 1. &lt;del&gt;填写任职资格&lt;strong&gt;申报表&lt;/strong&gt;&lt;/del&gt;

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; &lt;del&gt;格式调整，解决方法：在word里填写表格并打印&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled class=&#34;task-list-item&#34;&gt; 打印2份&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; 2. &lt;del&gt;推荐信&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; 3. &lt;del&gt;填写教师系列&lt;strong&gt;申请表&lt;/strong&gt;&lt;/del&gt;

&lt;ul class=&#34;task-list&#34;&gt;
&lt;li&gt;&lt;input type=&#34;checkbox&#34; disabled class=&#34;task-list-item&#34;&gt; 打印6份&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; 4. &lt;del&gt;打印5篇代表作6份&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; 5. &lt;del&gt;科技查新&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; 6. &lt;del&gt;学历学位&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;&lt;input type=&#34;checkbox&#34; checked disabled class=&#34;task-list-item&#34;&gt; 7. &lt;del&gt;教师资格&lt;/del&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;[ ] &lt;strong&gt;述职6分钟&lt;/strong&gt;，准备PPT&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;科技查新 &lt;a href=&#34;http://lib.nju.edu.cn/html/article.htm?id=30&amp;amp;fid=24&#34; target=&#34;_blank&#34;&gt;http://lib.nju.edu.cn/html/article.htm?id=30&amp;amp;fid=24&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;校内访问 &lt;a href=&#34;http://cx.nju.edu.cn/&#34; target=&#34;_blank&#34;&gt;http://cx.nju.edu.cn/&lt;/a&gt; 状态 &lt;a href=&#34;http://cx.nju.edu.cn/novelty/status.php&#34; target=&#34;_blank&#34;&gt;http://cx.nju.edu.cn/novelty/status.php&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;1、请用校内经费卡转账至图书馆账号0301 0011 3101。&lt;/li&gt;
&lt;li&gt;2、携转账发票黄色联至鼓楼图书馆104室或仙林图书馆2310室领取报告。祝您一切顺利！（鼓楼取报告的需多等待1~2个工作日）。&lt;/li&gt;
&lt;li&gt;3、如需要，可将此邮件打印提供给校财务处作检索记录证明清单。 【在本次委托完成前请将以往尚未交费或缴纳过押金但未转账，以及尚未提供转账单黄色联的委托记录结算完毕后再领取此次查新报告，如无欠费情况可忽略此信息。谢谢您的合作。】&lt;/li&gt;
&lt;li&gt;ISI Web of Knowledge &lt;a href=&#34;http://lib.nju.edu.cn/html/database.htm?id=6&amp;amp;fid=3&#34; target=&#34;_blank&#34;&gt;http://lib.nju.edu.cn/html/database.htm?id=6&amp;amp;fid=3&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;C刊被引用情况 &lt;a href=&#34;http://cssci.nju.edu.cn/&#34; target=&#34;_blank&#34;&gt;http://cssci.nju.edu.cn/&lt;/a&gt;  &lt;strong&gt;鼓楼&lt;/strong&gt;图书馆505&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;注意事项&#34;&gt;注意事项&lt;/h2&gt;

&lt;p&gt;最好能在3月24日前交齐材料，后面院系还有一系列公示啥啥的
- 关于2016年教师高级职务岗位评聘有关工作的通知.rar&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://hr.nju.edu.cn/cb/85/c5977a183173/page.htm&#34; target=&#34;_blank&#34;&gt;http://hr.nju.edu.cn/cb/85/c5977a183173/page.htm&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;有关应聘人员做好准备（提供PPT），述职时间6分钟。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;一、根据学校专业职务评聘工作的计划安排，2016年度教师高级职务招聘岗位已经发布，请各院系根据教师高级职务岗位要求进行公开招聘，并按规定接受应聘后，对申报人员材料进行公开展示、成果审查核对，以及思想政治、教学与科研等业务能力与潜力评审等工作。在评审结果公示后，&lt;code&gt;院系推荐候选人的评审材料须于4月5日前报送人力资源处培训发展办公室（个人网上申报、单位审核推荐工作须同步完成）&lt;/code&gt;。逾期不报送者，责任由相关院系单位承担。&lt;/li&gt;
&lt;li&gt;二、申报教师高级职务人员所提供的研究成果必须为任现职以来的成果，同时提交的检索证明中，需提供申报人成果“被收录情况”和“被引用情况”，并分别标出“自引”和“他引”,成果截止至&lt;code&gt;2016年12月31日&lt;/code&gt;。&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;关于报送教师高级职务岗位评聘材料的要求&#34;&gt;关于报送教师高级职务岗位评聘材料的要求&lt;/h3&gt;

&lt;p&gt;3、聘任候选人员应交的材料：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;①高等学校教师职务任职资格申报表（一式两份）（表格中照片、个人签名、院系领导签字盖章等手续须完备）&lt;/li&gt;
&lt;li&gt;②2位专家推荐意见(每个专家各两套，对应①) （须由推荐专家亲笔签名，每套分别附于任职资格申报表后）&lt;/li&gt;
&lt;li&gt;③《南京大学教师高级职务岗位申请表》（一式六份）&lt;/li&gt;
&lt;li&gt;④反映个人水平的代表作5篇（部）（六套）&lt;/li&gt;
&lt;li&gt;⑤任现职以来代表成果的引用情况和检索证明（需提供申报人成果“被收录情况”和“被引用情况”，并分别标出“自引”和“他引”）。（一份）&lt;/li&gt;
&lt;li&gt;⑥学历学位证书复印件（一份）&lt;/li&gt;
&lt;li&gt;⑦教师资格证书复印件（一份）&lt;/li&gt;
&lt;li&gt;⑧海外研修经历证明材料&lt;/li&gt;
&lt;li&gt;⑨校外申请者如已具有专业技术职务，请提供相应证书或文件等证明材料&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;以上材料请装档案袋、粘贴封面提交，封面以A4纸打印，申请材料格式见第2页（包括①②③④⑤⑥⑦⑧⑨）。今年开始，所有岗位申报人员还需另外提供五个档案袋作为鉴定材料，鉴定材料档案袋封面格式见第3页，每个档案袋包括③④成套装好。&lt;/p&gt;

&lt;p&gt;注：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;报送材料电子文本接收信箱：zydiao@nju.edu.cn（院系报送）&lt;/li&gt;
&lt;li&gt;文科检索可到图书馆五楼“中国社会科学研究评价中心”，联系电话：025-83595841。&lt;/li&gt;
&lt;li&gt;理科检索可到图书馆一楼“教育部科技查新工作站（南京大学站）”联系电话：025-83593401（鼓楼），025-89683401（仙林），或其他经国家认定的科技查新站。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;四、 应聘所须材料：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;1、应聘者可通过互联网或邮件、传真向我校人力资源处提出应聘申请，所需提供的材料如下：

&lt;ul&gt;
&lt;li&gt;①《高等学校教师职务任职资格申报表（两份） 下载&lt;/li&gt;
&lt;li&gt;②《南京大学教师专业技术职务高级岗位申请表》 (由系统填报后自动生成,调整格式后打印，四份)&lt;/li&gt;
&lt;li&gt;③两封专家推荐信（两套）

&lt;ul&gt;
&lt;li&gt;注:1、申请人员的推荐信可直接转至申报岗位所在院系；2、请将两份专家推荐意见以扫描或电子转换方式生成JPG或PDF格式文档上传至系统内相应位置&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;④代表性论文论著5篇（部）（原件1份，复印件3份）&lt;/li&gt;
&lt;li&gt;注:请将个人近五年内最具代表性的论文或论著以扫描或电子转换方式生成PDF格式文档上传至系统内相应位置，其中论文请扫描全文，&lt;/li&gt;
&lt;li&gt;&lt;del&gt;&lt;code&gt;论著请扫描著作的封面、书名页、封底三页&lt;/code&gt;&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;⑤其他成果证明复印件&lt;/li&gt;
&lt;li&gt;⑥学历学位证书复印件 &lt;del&gt;&lt;strong&gt;放在哪里？&lt;/strong&gt;&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;⑦专业职务聘任证书复印件 &lt;del&gt;&lt;strong&gt;放在哪里？&lt;/strong&gt;&lt;/del&gt;&lt;/li&gt;
&lt;li&gt;⑧外语能力证明复印件、教师资格证、海外研修证明材料等 &lt;del&gt;&lt;strong&gt;放在哪里？&lt;/strong&gt;&lt;/del&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;2、应聘者提供材料必须真实准确。&lt;/li&gt;
&lt;li&gt;3、所有应聘材料一般不予退还，需要退还的材料请特别说明。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;一、教学要求&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;（1） 每年至少为本科生独立开设一门课，周学时不少于 2 学时。&lt;/li&gt;
&lt;li&gt;（2） 在 5 分制的课堂教学质量测评中，测评得分不能低于 4.5分。&lt;/li&gt;
&lt;li&gt;（3） 积极参与本科生的实验、实习、学年论文和毕业论文等指导工作；协助指导过硕士研究生培养。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;二、正常申报科研要求 总体要求：专注于本学科有关方向的研究，能独立开展科研工作，取得了具有一定质量和数量的研究成果。 具体要求：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;（1） 任现职以来，主持 1 项省部级课题或主持 1 项国家社科基金重大招标、教育部人文社科重大招标课题的子课题（以批准的项目投标书为依据）。&lt;/li&gt;
&lt;li&gt;（2） 作为横向课题负责人，社会科学学科申请人获得经费 30 万元以上、人文科学学科申请人获得经费 15 万元以上，可以折算为 1 个省部级课题（以实际到帐金额为准）。&lt;/li&gt;
&lt;li&gt;（3） 任现职以来，至少 8 篇论文被 CSSCI 收录

&lt;ul&gt;
&lt;li&gt;如果成果中包含 1 篇 SSCI/A&amp;amp;HCI 期刊论文或 CSSCI 一流期刊论文，则数量要求为 6 篇，并且在同等条件下优先。&lt;/li&gt;
&lt;li&gt;专著（第一作者）可折算 2 篇 CSSCI 期刊论文，有多部专著时仅可折算1 部专著。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;（4） 积极参加学校与院系单位的发展规划、学科建设、平台建设等工作；积极参加国际国内学术交流活动；积极参加社会服务活动。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;副教授岗位职责&#34;&gt;副教授岗位职责&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;承担传播学专业本科生2门以上专业课程教学。&lt;/li&gt;
&lt;li&gt;成为硕导后，承担传播学专业研究生1门以上专业课程教学，指导硕士生若干名。&lt;/li&gt;
&lt;li&gt;定期参加全国范围内相关学术研讨会，同时参与新闻传播业界的相关事务。&lt;/li&gt;
&lt;li&gt;任期内主持省级以上课题1项。&lt;/li&gt;
&lt;li&gt;每三年内发表SCI A区及以上级别期刊论文至少1篇，或SCI B区期刊论文至少1篇；或SSCI&amp;amp; HCI 论文、CSSCI一流期刊论文至少1篇，或CSSCI论文至少3篇，或有影响的学术专著1部（由院学术委员会认定）。成果均以本人为&lt;code&gt;第一作者或通讯作者&lt;/code&gt;，第一署名单位为南京大学。&lt;/li&gt;
&lt;li&gt;担负传播学专业本科生专业辅导工作，完成院里或系里交给的相关工作。积极参加学校与院系单位的发展规划、学科建设、平台建设等工作；积极参加国际国内学术交流活动；积极参加社会服务活动。&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;申请教授的要求&#34;&gt;申请教授的要求&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;（1） 任现职以来，主持 1 项国家级课题或 2 项省部级课题。&lt;/li&gt;
&lt;li&gt;（2） 作为横向课题负责人，社会科学学科申请人获得横向经费60 万元以上、人文学科申请人获得横向经费 30 万元以上，可以折算为 1 个省部级课题（以实际到帐金额为准）。&lt;/li&gt;
&lt;li&gt;（3） &lt;code&gt;任现职&lt;/code&gt;以来，至少 &lt;strong&gt;12&lt;/strong&gt; 篇论文被 SSCI/A&amp;amp;HCI 或者 CSSCI收录，

&lt;ul&gt;
&lt;li&gt;其中必须至少包含 1 篇 SSCI/A&amp;amp;HCI 期刊论文或CSSCI 一流期刊论文，SSCI 发文期刊分区要求在三区以上。&lt;/li&gt;
&lt;li&gt;专著（第一作者）可折算 2 篇 CSSCI 期刊论文，有多部专著时最多可折算 3 部专著。&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;（4） 积极参加学校与院系单位的发展规划、学科建设、平台建设等工作；积极参加国际国内学术交流活动；积极参加社会服务活动。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/c87c77e50bb3d0f15f33f560e038976b.png&#34; alt=&#34;equal vs fair&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;助理研究员管理办法修订&#34;&gt;助理研究员管理办法修订&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://hr.nju.edu.cn/_upload/article/files/0a/06/4770fc374cb1bc758a322e02be26/cf48adbc-9466-48d6-9614-06b40a851462.pdf&#34; target=&#34;_blank&#34;&gt;https://hr.nju.edu.cn/_upload/article/files/0a/06/4770fc374cb1bc758a322e02be26/cf48adbc-9466-48d6-9614-06b40a851462.pdf&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>我的女儿</title>
      <link>https://chengjunwang.com/en/note/2017-03-21-my-children/</link>
      <pubDate>Sat, 11 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>https://chengjunwang.com/en/note/2017-03-21-my-children/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;https://gallery.mailchimp.com/c007b6835f6475cf470f6e0ef/images/a4918492-d8c8-4a18-9e1f-93716e8dc526.gif&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;米果&#34;&gt;米果&lt;/h1&gt;

&lt;p&gt;2017年4月10日，我的第二个孩子二女儿&lt;code&gt;米果&lt;/code&gt;出生了。这个名字是暮白给孩子取得，我和暮白经常记错。暮白对米果说：爱你就像爱生命。米果的大名是&lt;strong&gt;知乐&lt;/strong&gt;。&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:1&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:1&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;《论语》·雍也篇，子曰：“知（zhì）者乐（yào）水，仁者乐（yào）山；知（zhì）者动，仁者静；知者乐（lè），仁者寿。”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;pre&gt;&lt;code&gt;译义有多种。孔子说：“智慧的人喜爱水，仁义的人喜爱山；智慧的人懂得变通，仁义的人心境平和。智慧的人快乐，仁义的人长寿。” 另一理解为 “智者乐，水” ——智者之乐，就像流水一样，阅尽世间万物、悠然、淡泊。“仁者乐，山” ——仁者之乐，就像大山一样，岿然矗立、崇高、安宁。
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;a href=&#34;https://chengjunwang.com/data/miguo.html&#34; target=&#34;_blank&#34;&gt;https://chengjunwang.com/data/miguo.html&lt;/a&gt;&lt;/p&gt;

&lt;iframe src=&#34;https://chengjunwang.com/data/miguo.html&#34; scrolling=&#34;no&#34; width=&#34;300&#34; height=&#34;60&#34;&gt;&lt;/iframe&gt;

&lt;p&gt;米果有肠绞痛，休息不好，不太好带。暮白说米果是树懒熊，因为她经常挂在妈妈身上，必须要抱着才能安静。&lt;/p&gt;

&lt;h2 id=&#34;宝华卫生院&#34;&gt;宝华卫生院&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/957e47189f14eba13fdf8b6b77a78dc8.png&#34; alt=&#34;宝华卫生院&#34; /&gt;
&lt;a href=&#34;http://j.map.baidu.com/scsII&#34; target=&#34;_blank&#34;&gt;地图链接&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;需要找一个打预防针和体检的地方。4月18日，我打车去了宝华卫生院，在二楼妇产科办理了&lt;code&gt;回执卡&lt;/code&gt;，但是没有去后面的楼预约打预防针，而满月后第一周就要打针了。&lt;/p&gt;

&lt;p&gt;走玉兰路，经过大宅门，过了沪霍线，就到了宝华镇所在，沿着159线路走就可以到达宝华卫生院，也叫宝华医院。赶不上159路，我徒步走到狮子山站公交坐D5路公交回到恒大雅苑。&lt;/p&gt;

&lt;p&gt;5月8日，米果28天，我下午两点多打滴滴去了宝华卫生院，办理了&lt;strong&gt;接种证书&lt;/strong&gt;，4天后，本周五，给知乐打预防针，滴滴打车到宝华卫生院，去公共卫生楼一楼大厅排队，不要手机预约，10点之前到。下载了一个叫“金苗宝”的app &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:app&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:app&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;。&lt;/p&gt;

&lt;h2 id=&#34;米果降临&#34;&gt;米果降临&lt;/h2&gt;

&lt;p&gt;2016年8月20日发现米粒妈妈怀孕了，我给肚子里的小朋友取名米奇或米琪，就是那个漂亮的米老鼠Mickey。小米还不到一岁半。得知这个消息，我和晓青都很崩溃：怎么又这么容易？米粒妈妈和我面对一个困境：生还是不生？！他来的太突然了。&lt;/p&gt;

&lt;p&gt;我意识到这个责任只能我和晓青来抗。父母都表示不干涉，没有意见。时代真得变了。或许是因为要担责任，所以没有心绪。晓青一开始坚决不生，多次落泪。我们去市妇幼做了检查，上网搜索了一些细节。人工流产对子宫是不小的伤害，要越早越好，最好在50-55天的范围内。&lt;/p&gt;

&lt;p&gt;想到这个还没有成形的小人的命运，晓青又转念想生下来。因为如果我们想要二胎，那么晚不如早。但是做出这个决定并不容易，因为：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;一、生下ta将是艰苦时代的到来，米粒一个本来就够缠人的了，再来一个将会耗费很多时间。十月怀胎，十月养育。朝夕陪伴，十分辛苦。&lt;/li&gt;
&lt;li&gt;二、米粒妈妈的工作就毁了，又要耗费大半年时间在孩子身上。&lt;/li&gt;
&lt;li&gt;三、不确定性，不知道是男孩还是女孩。二胎政策只能生两个。如果想要一个男孩，那么就只有这一次机会了。于是乎变得很纠结。清宫图说女孩；金钱卦是一个夬卦，据说是男孩。抛硬币三局两胜说生。做判断和做决定是两回事.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;米果的做法&#34;&gt;米果的做法&lt;/h2&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/c9d73c9c817ba742d08ca68de6ff1516.png&#34; alt=&#34;米果&#34; /&gt;&lt;/p&gt;

&lt;p&gt;米果是一种以米为主要原料制成的点心。在松阳一般称米果，糕是甜的，而米果是不甜的，在乡下，米果还是用人工锤打出来的，因此称打米果。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;主料：糯米粉 (200G)&lt;/li&gt;
&lt;li&gt;调料：黄豆粉 (20G), 辣椒粉3G，白砂糖15G。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;步骤&lt;/p&gt;

&lt;p&gt;　　1、先将糯米粉放入洗干净的电饭锅，倒入适量水，用力揉搓糯米粉，直至面饼状，再揉搓成条状。
　　2、揪成小米团，揉搓成汤圆大小的团子。
　　3、煮锅中加水，水开后将糯米团子放入锅中煮沸，直至团子浮上水面，即可捞出。
　　4、在煮团子的时间内，可以准备裹团子的外衣-米果豆粉+白砂糖，放入干净的容器中，糖的比例看个人口味调节。
　　5、 团子煮好后，捞出，放入准备好的豆粉中，晃动容器，直至团子被外衣全部裹住，即是成品米果。
　　6、用筷子将米果夹出，放入盘中，就可以享用了。&lt;/p&gt;

&lt;h1 id=&#34;米粒&#34;&gt;米粒&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;https://chengjunwang.com/data/mili.html&#34; target=&#34;_blank&#34;&gt;https://chengjunwang.com/data/mili.html&lt;/a&gt;&lt;/p&gt;

&lt;iframe src=&#34;https://chengjunwang.com/data/mili.html&#34; scrolling=&#34;no&#34; width=&#34;300&#34; height=&#34;60&#34;&gt;&lt;/iframe&gt;

&lt;p&gt;2017年7月1日晚上米粒开始反复发热，最高到40多度，家里却没有剩下的退烧贴了。7月2日带米粒去仙林鼓楼医院挂了专家号。打车过去20多块钱，开了发炎药，消除炎症很重要，才能抑制反复发热。&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:-&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:-&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;以后小孩得病就不去鼓楼儿童医院了，来回距离很长。&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;子曰：“岁寒，然后知松柏之后凋也。”&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;《梅花》 唐 崔道融&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;数萼初含雪，孤标画本难。&lt;/li&gt;
&lt;li&gt;香中别有韵，清极不&lt;strong&gt;知寒&lt;/strong&gt;。&lt;/li&gt;
&lt;li&gt;横笛和愁听，斜枝倚病看。&lt;/li&gt;
&lt;li&gt;朔风如解意，容易莫摧残。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;《淮阴夜宿二首》 唐 孙逖&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;水国南无畔，扁舟北未期。乡情淮上失，归梦郢中疑。&lt;/li&gt;
&lt;li&gt;木落&lt;strong&gt;知寒&lt;/strong&gt;近，山长见日迟。客行心绪乱，不及洛阳时。&lt;/li&gt;
&lt;li&gt;永夕卧烟塘，萧条天一方。秋风淮水落，寒夜楚歌长。&lt;/li&gt;
&lt;li&gt;宿莽非中土，鲈鱼岂我乡。孤舟行已倦，南越尚茫茫。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;《红线毯》 唐 白居易&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;红线毯，择茧缲丝清水煮，拣丝练线红蓝染。&lt;/li&gt;
&lt;li&gt;染为红线红于蓝，织作披香殿上毯。披香殿广十丈馀，&lt;/li&gt;
&lt;li&gt;红线织成可殿铺。彩丝茸茸香拂拂，线软花虚不胜物。&lt;/li&gt;
&lt;li&gt;美人蹋上歌舞来，罗袜绣鞋随步没。太原毯涩毳缕硬，&lt;/li&gt;
&lt;li&gt;蜀都褥薄锦花冷，不如此毯温且柔，年年十月来宣州。&lt;/li&gt;
&lt;li&gt;宣城太守加样织，自谓为臣能竭力。百夫同担进宫中，&lt;/li&gt;
&lt;li&gt;线厚丝多卷不得。宣城太守知不知，一丈毯，千两丝。&lt;/li&gt;
&lt;li&gt;地不&lt;strong&gt;知寒&lt;/strong&gt;人要暖，少夺人衣作地衣。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;故事从2015年3月11日早上十一点五分开始讲起，在这一刻我的女儿在东南大学附属中大医院降生了。我给女儿取名叫米粒，大名叫知寒，希望她可以“知寒问暖”。她的到来令我和妻子万分高兴。然而就在一个多月前，我和妻子还在为异地生产的问题焦头烂额。&lt;/p&gt;

&lt;p&gt;2014年我来南京工作，然而短期之内无法解决户口问题。而妻子在湖南生产则面临产后坐月子我无法陪护的问题，不得已我们只能选择在南京生孩子。然而做出这选择并不容易，因为我和妻子对于南京并不熟悉，在这里我们的朋友也不多，也许更为严重的问题是我们对于异地生产所存在的问题缺乏清晰的认识。断断续续地，我咨询了单位和一些医院，然而得到的回复并不乐观。正当我不知所措的时候，我听别人介绍了中大医院的产科门诊。怀着忐忑的心情，我拨通了中大医院产科的电话。顾晓霞护士长耐心地回答了我的各种问题，提醒我办理各种必要的手续，并指导我的妻子进行产前准备。一个月后，孩子出生了，然而我和妻子对于如何照顾孩子却并不擅长；幸亏有值班护士在，她们充满耐心得进行指导、不断鼓励，从各个方面照顾着产妇和孩子。在很多微小的细节当中，我感受到了医护人员对于工作的尽职尽责。&lt;/p&gt;

&lt;p&gt;如今，我的爱人和孩子顺利出院了。围绕着孩子，家里多了很多欢声笑语。很多时候，我为自己这样一个小家庭感到幸运，更深感自己应该感谢中大医院产科的医护人员。正因为有了你们辛勤的付出，很多崭新的小生命所降临的世界变得更加美好。&lt;/p&gt;

&lt;p&gt;[2015-06-14]作为一个爸爸，我挺失败的。因为没有办法将老婆孩子留在自己身边。暮白的产假时间快到了，24日打过第三次疫苗之后，米粒就要开始跟暮白和奶奶去长沙了。这是我对自己的孩子的亏欠。作为爸爸，自己工作不努力，让孩子受到这些折磨，于我是很大的愧疚。但我有时候会忘记这些愧疚。这么多年，我对家庭说不上好，29岁结婚，为了家庭（很大程度上）我回到了南京。却忽略了南京的高房价、雾霾、暮白在长沙的工作。半年之后，孩子出生，我和暮白也是学着做爸爸妈妈。女儿的出生的确让我的学术研究变得更加雪上加霜。但造成这一问题的主要责任在我。我必须要抓紧一切时间来弥补这一缺失，否则孩子就不是没有户口那么简单。电视一度占用了很多时间，我也体会到一个独立的空间的重要性，它不能是卧室，不能是客厅，就是独立的书房，我现在在厨房外的桌子上。&lt;/p&gt;

&lt;h2 id=&#34;米粒百日礼&#34;&gt;米粒百日礼&lt;/h2&gt;

&lt;p&gt;带米粒去拍照，米粒妈妈在美团上团的券。照例是在一个小区里。打车过去，司机欺负我不识路想绕路，被我用百度地图拆穿（是的，移动互联网让很多坏人很头痛）。我们没有带U盘，所以没有办法拷照片，于是我跟对方要求网络传输，最后还是传到了手机里。给米粒选了三套衣服拍。一个人鱼，一个萝莉，一个袜子一样的，结果很失败的是米粒第二套衣服忘了带帅气的小领结，结果拍出来显得很土。然后，米粒不会做，趴也不成，看镜头就瞪着眼很严肃。所以整个拍摄很痛苦。这类拍摄都是热心的妈妈折腾孩子。所以，我们庆幸只拍了72张。&lt;/p&gt;

&lt;p&gt;带米粒体检，可气的是体检时医生说米粒是&lt;code&gt;慢慢性子&lt;/code&gt;，原因是米粒对于医生手上测智力的红色环子不感冒。她瞪着眼瞅自己想看的地方。打疫苗时因为红屁股没有成功，直接打车去了金盛华东家具城。理发、游泳。理掉了头发的米粒又恢复了靓丽的形象。回家后，突然发觉米粒可以趴，但是要垫高，手臂的摆放很重要，并排在胸前。&lt;/p&gt;

&lt;h2 id=&#34;长沙廿日&#34;&gt;长沙廿日&lt;/h2&gt;

&lt;p&gt;转眼之间在长沙过了二十余天。一到长沙就感觉很疲惫，想睡觉。看女儿的日子充满了挑战和乐趣。米粒喜欢出去玩，主要是睁大眼睛看。她对世界很好奇。每天一早起来，米粒妈妈就带着米粒出去，去研究所外面的小花园看老人舞剑。米粒妈妈说“耍剑”。回来吃过了奶，米粒就要睡觉。我妈特别辛苦，负责哄米粒睡觉。有时候不得要领，小坏蛋就苦啊哭啊。到了十一点半，我就去食堂打饭。四个菜两盘子端回来。一般中间十点半的时候，米粒喂过了一次水就饿了，要电话米粒妈妈回来喂奶。中午吃饭后，午睡。米粒也睡。下午米粒妈妈三点去上班。一般四点半之后还要喂奶。五点半我去打饭。我妈会带着米粒出去。有一段时间是站在实验楼旁等米粒妈妈。后来，去小操场看小伙伴们。因为都是一个单位的，所以大家高度信任，非常友好。&lt;/p&gt;

&lt;p&gt;米粒不好哄。小家伙脾气挺大的。当然，吃饱了就好了。但是，往往有吃不饱的时候。她就哭。我妈很喜欢米粒，经常惯着她。常说米粒“跟着大鱼上串”。米粒嗓子粗，她哼哼的时候，我妈就说“米粒，你又再嘿呼谁？”我妈还喜欢说“米粒是个大孩子，米粒可管了。”我妈对米粒是隔代亲。&lt;/p&gt;

&lt;p&gt;米粒有一个可爱的日本风的小帽子。戴着它，没有人说米粒是小男孩了。长沙人说米粒长得极好，很漂亮。但也有说好胖啊，小胖子。米粒妈妈和我妈都不喜欢别人说米粒胖。后来自我解嘲也叫米粒小胖子了。&lt;/p&gt;

&lt;p&gt;米粒五个半月了，我也要回南京了。很怀念抱着米粒在芙蓉树边、樟树下、操场上、房间里。米粒对我刚刚熟悉，现在又得分开了。&lt;/p&gt;

&lt;p&gt;我这段时间的效率也不行，执行力很差。主要是因为兴趣突然被时间序列给带走了。尝试了好几种贝叶斯建模的方法。一开始是给米粒妈妈的数据建模，后来是给美国2012大选建模。关于公众注意力部门的模型算清楚了。媒介注意力、舆论支持率、公众情绪都会影响到公众注意力。使用公众注意力预测公众注意力当然效果不错，比如用query预测tweets。但是poll本身因为具有很强的trend和seasonality所以不需要采用regression预测。而sentiment和news则主要受到query的影响。&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:2&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:2&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;h2 id=&#34;米粒回南京&#34;&gt;米粒回南京&lt;/h2&gt;

&lt;p&gt;2015年12月19日，米粒从长沙回到了南京。米粒妈妈辞掉了在长沙中科院的工作，下定决心回到南京了。我要能见到亲爱的女儿了。除了高兴，也感觉到肩上的担子更重了。&lt;/p&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:1&#34;&gt;知寒、知乐是我对两个女儿的寄托。希望她们能够晓得民间疾苦，可以享受知识带来的乐趣。我想的是论语里的意思：智者乐，“忍者瘦”😀。暮白想的是“知道享乐”。社区探访的阿姨听了小孩的名字说“真简单”。
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:1&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:app&#34;&gt;为什么现在app都比网站好用啊？我猜是因为背后
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:app&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:-&#34;&gt;这也回答了我以前的问题，为什么一些表面上看起来没事的人，在傍晚会莫名其妙的发热
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:-&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:2&#34;&gt;我觉得自己很长时间都没有进入写作的流程，而是将精力浪费在了很多类似这种数据探索上面。
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:2&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>团队合作中的任务分配</title>
      <link>https://chengjunwang.com/en/note/2017-02-17-data-insight/</link>
      <pubDate>Fri, 17 Feb 2017 00:00:00 +0000</pubDate>
      
      <guid>https://chengjunwang.com/en/note/2017-02-17-data-insight/</guid>
      <description>

&lt;p&gt;最近一年多一直和令飞折腾&lt;code&gt;团队合作&lt;/code&gt;的东西，也在思考如何进行团队合作。之前是把开会当成头脑风暴，后来发现太浪费时间，尤其是团队成员超过三个人之后（也要兼顾别人没有那么多时间），合作的成本开始上升，如何高效地进行合作就成了一个重要的问题。我们研究的主题也就是这个东西。&lt;/p&gt;

&lt;h1 id=&#34;直接表达claim&#34;&gt;直接表达claim&lt;/h1&gt;

&lt;p&gt;直接地表达claim，然后使用数据证明自己的观点，就会发现其中的断裂的逻辑链条，进而不断完善。&lt;strong&gt;Polish your claim&lt;/strong&gt; 比如，我去统计了用户在不同的repos里的push的次数计算其multi-tasking的香农熵和core team ratio，发现：
&amp;gt; Github用户倾向于比较平均但深入地参与到团队项目（&lt;code&gt;claim&lt;/code&gt;）。&lt;/p&gt;

&lt;h2 id=&#34;为什么水分可以增加影响&#34;&gt;为什么水分可以增加影响？&lt;/h2&gt;

&lt;p&gt;找一个novelty的点，需要一个narrow down的concrete的research question。要避免一开始要有一个想法，somehow connect to something, 挖来挖去做出来很多unconnected的findings。apparent team不是真实的team， 我们想要找到real team， 因此可以去分析workload distribution and skill set overlapped，及其对于impact的影响。我们想要找一个metric to correct team size. team的&lt;strong&gt;充水&lt;/strong&gt;程度。story！为什么充了水的team的impact更大？what is a real team is research topic。怎么定义水分就靠近hypothesis。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/64335d80b0fc78f1bdaef76612e11750.png&#34; alt=&#34;water team&#34; /&gt;&lt;/p&gt;

&lt;p&gt;我之前说的一大堆只是想说一个论点：因为我们不是duncan watts，所以我们还是要用正轨的research道路。define topics, narrow it down, implement, provide prospects四部曲的方式。令飞之前说的“什么样是真正有效的合作，从技能的重合度和贡献量来衡量”是research topic, 不是一篇文章的research question。从workload的分布定义一个metric，core team size or effective team size&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;加一个时间维度， 新陈代谢， m(t)&lt;/li&gt;
&lt;li&gt;diversity，做过多少个不同的项目，experience好测量&lt;/li&gt;
&lt;li&gt;repeated collaboration&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;组织为什么会失败。junior和senior scholar合作效果不错。&lt;/p&gt;

&lt;h3 id=&#34;task-assignment&#34;&gt;Task Assignment&lt;/h3&gt;

&lt;p&gt;勾勒一个图景，早期/核心用户对任务的分配是如何切割了任务空间的。我们其实是观察不到用户之间的交流，所以只能通过用户各自的工作来推测他们协商任务分配的结果。一种分析角度是，大的任务会由核心用户在早起就“认领”吗？还是大任务（大修改）随时间分配是均匀的。&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;http://www.robertasinatra.com/&#34; target=&#34;_blank&#34;&gt;Roberta Sinatra&lt;/a&gt;等人2016年发表Quantifying the evolution of individual scientific impact &lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:Sinatra&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:Sinatra&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;， 把一个科学家当一个sample,发现impact的分布无法简单用sample size N（发文章数量）rescale到一起。于是给每个sample一个参数Qi，这样就能把所有科学家的所有论文的影响力分布collapse 到一起，解释了为什么高量产的科学家，论文影响力极大值比根据样本规模随机从系统取还要大的问题-其实没有解释，就是管这个叫Q 了。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;ttp://sciencepaths.kimalbrecht.com/&#34; target=&#34;_blank&#34;&gt;Science paths&lt;/a&gt; 网页可视化&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=qlnxM-ld4BU&#34; target=&#34;_blank&#34;&gt;Is a scientific career predictable?&lt;/a&gt; nature视频&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;科学探索和合作写代码还不一样，前者有可能真的不知道最重要的工作什么时候出现，后者的任务空间是个有限集合，任务的重要性和完成度是排他性的&lt;/p&gt;

&lt;p&gt;Team Assembly Mechanisms Determine Collaboration Network Structure and Team Performance&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:Guimer&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:Guimer&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; 一文题目应该叫“学术圈”。三个结论1. ）科学家是有圈子的，类似于渗流模型的giant cluster 好几个学科都是这样，越是好刊物的合作者数据，越能看到这个圈子存在2. ）模型参数m 团队规模, p 选择已经发表过文章老鸟合作的概率，q 选择已经合作过的人再次合作的概率，发现要想发高影响力的文章，p要大，q要小，也就是不要轻易和菜鸟合作，但也不要总是和同一批人合作；3）该模型可以复现团队规模的相变-稳定增长到收敛在一个优化值，和学科内部合作网络出现精英圈子的消息。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;哈佛商业评论 Using the Crowd as an Innovation Partner&lt;/li&gt;
&lt;li&gt;Management Science上的Incentives and Problem Uncertainty in Innovation Contests: An Empirical Analysis&lt;/li&gt;
&lt;li&gt;Uzzi B, Mukherjee S, Stringer M, Jones B. 2013. Atypical combinations and scientific impact. Science 342:468–72&lt;/li&gt;
&lt;li&gt;Lee Y N, Walsh J P, Wang J. Creativity in scientific teams: Unpacking novelty and impact. Research Policy, 2015, 44(3):684-697.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;拟合代码量的累积增长&#34;&gt;拟合代码量的累积增长&lt;/h2&gt;

&lt;p&gt;可以先把10个人的team的代码总量随（绝对）时间的变化看一下C是t时刻的代码总量，A是无穷远时代码总量，tau大概就是半衰期。然后画一个所有大team的tau的分布，如果是双峰的分布，说明确实有两种team。20120311之后出现的team好像都有显示代码量。&lt;/p&gt;

&lt;p&gt;$$C(t)=A(1-e^{-t/\tau})$$&lt;/p&gt;

&lt;p&gt;两边求导再离散化,先求导的：$\frac{dC (t)}{dt} =\frac{A}{\tau} e^{-t/\tau}$&lt;/p&gt;

&lt;p&gt;左边的导数可以用两点间的斜率近似， $\frac{dC (t)}{dt} =\frac{(C (t+\delta t)-C (t))}{\delta t} $，离散一下 $\delta t =1$，即：&lt;/p&gt;

&lt;p&gt;$$C(t+1)-C(t) = \frac{A}{\tau} e^{-t/\tau}$$&lt;/p&gt;

&lt;p&gt;两边取对数，$log(C(t+1)-C(t))= log\frac{A}{\tau} - t/\tau$&lt;/p&gt;

&lt;p&gt;令$\beta = -1/\tau$，$y = log(C(t+1)-C(t))$, $constant = log\frac{A}{\tau}$, 则存在:&lt;/p&gt;

&lt;p&gt;$y = constant + \beta t$&lt;/p&gt;

&lt;p&gt;对数据进行处理得到$y$和$t$，采用线性回归进行拟合，即可得到constant和$\beta$。那么显然：$\tau = -1/\beta$, $ A = e^{constant} $, $\tau = - e^{constant}/\beta $&lt;/p&gt;

&lt;p&gt;我尝试去拟合了tau和A, 结果产生了大量的负的tau和A，另外，多数R方都不大，效果不好。这里的$\tau$与这个代码增长速度随时间变化是负的反比关系。因为挺多项目的代码增长速度是非线性的，比如相邻两次的变化量是先递增，后减小，或者一直增加。都会导致线性拟合的效果不好。&lt;/p&gt;

&lt;p&gt;一个例子见以下代码：&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import statsmodels.api as sm
import numpy as np
import matplotlib.cm as cm
from datetime import datetime

def interpolating(data):
    # To interpoloate data
    bad_indexes = np.isnan(data)
    good_indexes = np.logical_not(bad_indexes)
    good_data = data[good_indexes]
    interpolated = np.interp(bad_indexes.nonzero()[0], good_indexes.nonzero()[0], good_data)
    data[bad_indexes] = interpolated
    return data


d = [&#39;2014-01&#39;, &#39;2012-09&#39;, &#39;2013-08&#39;, &#39;2013-09&#39;, &#39;2012-10&#39;, &#39;2013-05&#39;, &#39;2013-06&#39;]
v = [&#39;1192&#39; ,&#39;163&#39;, &#39;605&#39;, &#39;1189&#39;, &#39;167&#39; ,&#39;199&#39;, &#39;199&#39;]
d = [datetime.strptime(i+&#39;-01&#39;, &amp;quot;%Y-%m-%d&amp;quot;) for i in d]
d = [(i - np.min(d)).days/30 + 1 for i in d]
v = [int(i) for i in v]
dv = zip(d, v)
dv = sorted(dv,key=lambda x:x[0])
d, v = np.array(dv).T
yt = np.array(v[1:]) - np.array(v[:-1])
t = d[:-1]
x = t#np.log(t)
y = np.log(yt+1)
y = interpolating(y)
# fit
xx = sm.add_constant(x, prepend=True)
res = sm.OLS(y,xx).fit()
constant, beta = res.params
r2 = res.rsquared
tau = - 1.0/beta
A = tau*np.exp(constant)
print &#39;Tau = &#39;,tau,&#39;A=&#39;,  A, &#39;constant = &#39;,constant, &#39;beta =&#39;, beta,&#39;rsquare = &#39;, r2

fig = plt.figure(figsize=(10, 5))
ax = plt.subplot(1,2,1)  
plt.plot(d, v, &#39;r-s&#39;)
plt.xlabel(r&#39;month&#39;);plt.ylabel(r&#39;size&#39;)
ax = plt.subplot(1,2,2)  
plt.plot(x, y, &#39;rs&#39;)
plt.plot(xx, constant+xx*beta,&amp;quot;red&amp;quot;)
plt.xlabel(r&#39;t&#39;);plt.ylabel(r&#39;log(c(t+1)-c(t))&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/19d58b4deb8a292c287dc692898a89af.png&#34; alt=&#34;tau&#34; /&gt;&lt;/p&gt;

&lt;p&gt;To-do:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;最大的push次数的人和最多push代码量的人是否一个人，计算一个rate&lt;/li&gt;
&lt;li&gt;算sleeping beauty index&lt;/li&gt;
&lt;li&gt;如果把每个人的工作量换算成绝对代码增减量, 还是可以仿照m的定义计算熵。熵小的大石头放好了，大部分人都是添沙子。大石头什么时候放进去的也很有信息量。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;尽快确定一下最大代码贡献者和最频繁贡献者的重合率，感觉这个还是比较影响整个故事走向的。另外，试一下最大代码出现时间的分布。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/bd3743ee8842b10dcd0e33142af77b5e.png&#34; alt=&#34;overlap rate&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/c315dfd9d7cbab768b9c5615813d00df.png&#34; alt=&#34;overlap rate single size&#34; /&gt;&lt;/p&gt;

&lt;p&gt;结论：随着团队规模增加，团队内部的确变得多元化，有人频繁参与，有人则高强度参与，二者未必相同。的确可能出现大象🐘和老鼠🐭并存的情况。按照&lt;code&gt;“固定”任务空间&lt;/code&gt;的假设，在位者有累积优势（经验、任务分配、责任心），但人多了竞争激烈，也会给在位者带来冲击，尤其是大象的进入，他们可以靠少数的push，贡献大量的代码。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/71219d7043e62aed77d1761afceb73cf.png&#34; alt=&#34;water&#34; /&gt;&lt;/p&gt;

&lt;p&gt;水分不仅在活跃次数衡量的有效团队中发挥作用，同样在代码量衡量的有效团队中发挥作用，应该有点深刻的东西在这里。Lakhani (2011)发表在Management Science上的文章要回答一个问题&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:Lakhani&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:Lakhani&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;，像Netflix 这样的竞赛，团队多好还是少好？传统经济学理论认为参与团队越多，单个团队获奖概率越低，团队越没有积极性，平均表现越差。他们分析数据发现，扩大参赛团队数后，平均表现变差，但最强团队的表现变好。&lt;/p&gt;

&lt;h2 id=&#34;参与的均匀性&#34;&gt;参与的均匀性&lt;/h2&gt;

&lt;p&gt;昨天看了一点Alex &amp;lsquo;SANDY&amp;rsquo; Pentland的书social physics，关于如何合作Sandy有很多看似惊世骇俗的观点，其中提到team的collective intelligence他说social sensitivity、话轮的均匀性可以提到团队绩效（群体智慧），见Evidence for a Collective Intelligence Factor in the Performance of Human Groups一文，sandy还在哈佛商业评论吹了一通The New Science of Building Great Teams，强调face-to-face的团队互动、均匀的参与对于team performance的重要性：&lt;/p&gt;

&lt;blockquote&gt;
&lt;p&gt;(Group intelligence) was negatively correlated with the variance in the number of speaking turns by group members.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;他们就用variance测量的参与的均匀程度，方差越小越好。variance越大，香农熵越大，m越大，m/M越大，绩效越低，与实际数据发现一致。&lt;/p&gt;

&lt;h3 id=&#34;数据&#34;&gt;数据&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;arXiv bulk data access amazon S3&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&#34;因公出国护照办理&#34;&gt;因公出国护照办理&lt;/h1&gt;

&lt;p&gt;&lt;a href=&#34;http://wb.nju.edu.cn/zzbl/list.htm&#34; target=&#34;_blank&#34;&gt;http://wb.nju.edu.cn/zzbl/list.htm&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;一、公务（普通）护照办理
（一）办证要求
1、在办结因公出国校内审批后，由申请者本人到仙林校区行政楼818室采集指纹，并提交以下材料：
（1）身份证原件
（2）身份证复印件（标明出生地所在省、直辖市或自治区名）
（3）两寸白底彩照1张
2、现无公务（普通）护照/护照过期/护照有效期不足（本次出访后，距护照到期不足6个月；部分国家所需有效期更长）
（二）办理时间：1至2周（不含节假日）&lt;/p&gt;

&lt;h3 id=&#34;因公出国人员申报流程及材料须知&#34;&gt;因公出国人员申报流程及材料须知&lt;/h3&gt;

&lt;p&gt;&lt;a href=&#34;http://wb.nju.edu.cn/splc/list.htm&#34; target=&#34;_blank&#34;&gt;http://wb.nju.edu.cn/splc/list.htm&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;因公临时出国人员根据“OA申报材料清单”,通过OA系统提交申报材料；&lt;/p&gt;

&lt;h1 id=&#34;reference&#34;&gt;Reference&lt;/h1&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:Sinatra&#34;&gt;Sinatra, R., Wang, D., Deville, P., Song, C., &amp;amp; Barabási, A. L. (2016). Quantifying the evolution of individual scientific impact. Science, 354(6312), aaf5239-aaf5239.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:Sinatra&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:Guimer&#34;&gt;Guimerà R, Uzzi B, Spiro J, et al. Team Assembly Mechanisms Determine Collaboration Network Structure and Team Performance. Science, 2005, 308(5722):697-702
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:Guimer&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:Lakhani&#34;&gt;Boudreau, K. J., Lacetera, N., &amp;amp; Lakhani, K. R. (2011). Incentives and problem uncertainty in innovation contests: an empirical analysis. Management Science, 57(5), 843-863.
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:Lakhani&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>2016年总结</title>
      <link>https://chengjunwang.com/en/note/2016-12-31-summary/</link>
      <pubDate>Sat, 31 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>https://chengjunwang.com/en/note/2016-12-31-summary/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/d79656403a9b53627af688a06c9a3432.png&#34; alt=&#34;leaf&#34; /&gt;&lt;/p&gt;

&lt;p&gt;宁在一思进，莫在一思停。&lt;/p&gt;

&lt;p&gt;1月19日
今天老爸回老家了，我要负责做饭了。压力好大。今天上午打车去仙林医院，说儿科只有周三周四上午才有，仙林鼓楼医院的儿科也只有内科，没有皮肤科。住在仙林，小孩看病还有去鼓楼儿童医院。往返打车画了61块钱。&lt;/p&gt;

&lt;p&gt;1月20日&lt;/p&gt;

&lt;p&gt;今天写了一点创业项目的基本情况，还差挺多的。卸载了好几个app，才安装了新版支付宝。今天上午我和暮白带着米粒去儿童医院，结果挂错科室了，转过去之后今天没号了，打电话给鼓楼医院不看14岁以下病人，只好去迈皋桥妇幼保健院。吃过淮南牛肉汤之后又折腾回来。&lt;/p&gt;

&lt;p&gt;1月21日&lt;/p&gt;

&lt;p&gt;下午趁着米粒睡着，我开始写了一点论文，看了会文献。突然想起，今天么有拿快递。唯有写作使人心安。&lt;/p&gt;

&lt;p&gt;1月22日&lt;/p&gt;

&lt;p&gt;team formation仅仅是group formation的一种类型，big team never expand，为什么我却觉得如此困难呢。因为我对这个领域可以验证的理论还不了解。我在过去的几年里学到的一点是，不管有多少人质疑一个有潜力的方向，作为年轻人你要做的不是插着腰加入他们，而是沿着那个方向做出有价值的东西，让他们闭嘴。年轻人要想出头，必须冒险。&lt;/p&gt;

&lt;p&gt;1月22日&lt;/p&gt;

&lt;p&gt;‘friends of friends’. 三角闭合显然是group formation的重要动力。写东西很像修行，你会被磨砺着动心忍性，不为所动，神思千里。&lt;/p&gt;

&lt;p&gt;1月22日回复顶转发&lt;/p&gt;

&lt;p&gt;在达沃斯论坛，马云接受纽约时报专栏作家索尔金采访，提出三个30改变世界的观点 &lt;a href=&#34;http://weibo.com/1642634100/ErzQLt8yd?type=comment&#34; target=&#34;_blank&#34;&gt;http://weibo.com/1642634100/ErzQLt8yd?type=comment&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;1月23日回复顶转发&lt;/p&gt;

&lt;p&gt;Decision and hard work based on enduring passion will never fail you. Edward E. Wilson ​​​​&lt;/p&gt;

&lt;p&gt;1月23日回复顶转发&lt;/p&gt;

&lt;p&gt;今天下午写到了6000字，争取今天写到8000字。一篇文章写十天，真实不容易。写到了六点多，才写了6800字。&lt;/p&gt;

&lt;p&gt;1月23日回复顶转发&lt;/p&gt;

&lt;p&gt;今天下午写到了6000字，争取今天写到8000字。一篇文章写十天，真实不容易。写到了六点多，才写了6800字。写到晚上，写了7544字。&lt;/p&gt;

&lt;p&gt;1月23日回复顶转发&lt;/p&gt;

&lt;p&gt;今天超过9700字了，其实正文只有8544字。&lt;/p&gt;

&lt;p&gt;1月24日回复顶转发&lt;/p&gt;

&lt;p&gt;尤克里里完全入门24课 &lt;a href=&#34;http://list.youku.com/albumlist/show?id=18501294&amp;amp;ascending=1&amp;amp;page=1&#34; target=&#34;_blank&#34;&gt;http://list.youku.com/albumlist/show?id=18501294&amp;amp;ascending=1&amp;amp;page=1&lt;/a&gt;
练习了一个小时，看了同名的视频和书，终于知道do re mi fa sol la ti do怎么弹了。&lt;/p&gt;

&lt;p&gt;1月24日回复顶转发&lt;/p&gt;

&lt;p&gt;妈妈今天早上离开了，今天我和晓青在家看米粒。我要把论文审核一遍，发了邮件提交。松一口气。&lt;/p&gt;

&lt;p&gt;1月25日回复顶转发&lt;/p&gt;

&lt;p&gt;1月31日，初四，今天去超市买了电池，给尤克里里调了弦，今天妈妈从老家回来，晚上自习看了F调音阶，弹了一会祝你生日快乐。&lt;/p&gt;

&lt;p&gt;1月31日回复顶转发&lt;/p&gt;

&lt;p&gt;Ukulele零基础入门视频教程 &lt;a href=&#34;http://www.tanukulele.com/?tag=xsrm&#34; target=&#34;_blank&#34;&gt;http://www.tanukulele.com/?tag=xsrm&lt;/a&gt; &lt;a href=&#34;http://www.ukulelefan.com/?p=3432&#34; target=&#34;_blank&#34;&gt;http://www.ukulelefan.com/?p=3432&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;2月1日回复顶转发&lt;/p&gt;

&lt;p&gt;Ukulele零基础入门视频教程
&lt;a href=&#34;http://www.tanukulele.com/?tag=xsrm&#34; target=&#34;_blank&#34;&gt;http://www.tanukulele.com/?tag=xsrm&lt;/a&gt;
&lt;a href=&#34;http://www.ukulelefan.com/?p=3432&#34; target=&#34;_blank&#34;&gt;http://www.ukulelefan.com/?p=3432&lt;/a&gt;
&lt;a href=&#34;http://rockv.net/e/action/ShowInfo.php?classid=40&amp;amp;id=208&#34; target=&#34;_blank&#34;&gt;http://rockv.net/e/action/ShowInfo.php?classid=40&amp;amp;id=208&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;2月1日回复顶转发&lt;/p&gt;

&lt;p&gt;下载team science的文章，From Solo Investigator to Team Scientist: Trends in the Practice and Study of Research Collaboration，发现很多其他的文章。team science这是一个被广泛关注的领域，已经积累了大量的工作在里面。&lt;/p&gt;

&lt;p&gt;2月1日回复顶转发&lt;/p&gt;

&lt;p&gt;子曰：“知之者不如好之者，好之者不如乐之者。” 子曰：“知者乐水，仁者乐山；知者动，仁者静；知者乐，仁者寿。”  [礼物] 王知乐 [心] 名字的来源。&lt;/p&gt;

&lt;p&gt;2月3日回复顶转发&lt;/p&gt;

&lt;p&gt;越来越感觉到重要的是你急切的想要做什么，否则就会丧失战斗力。 ​​​​&lt;/p&gt;

&lt;p&gt;2月6日回复顶转发&lt;/p&gt;

&lt;p&gt;science也曾说将github作为研究的一环，来开放代码，这个是否可以识别出来呢？&lt;/p&gt;

&lt;p&gt;2月6日回复顶转发&lt;/p&gt;

&lt;p&gt;把合作的图做到claim的高度，方便团队成员理解。&lt;/p&gt;

&lt;p&gt;2月10日回复顶转发&lt;/p&gt;

&lt;p&gt;愈发感觉到我是一个肤浅的人，对于做网站天然没有抵抗力。或许，骨子里是想做得更好，让别人看到眼前一亮。其实是一种无奈的求胜的欲望。失去抵抗力，几乎飞蛾扑火地搞了个人新网站之后却依旧感觉少了点什么，什么呢？显摆的东西啊！因为没有成果，所以还是要滚去写论文啊。放弃是一种很无奈的事情，总是期待能够拾起来。另外就是学到东西。&lt;/p&gt;

&lt;p&gt;2016年就这么过去了，我也已经32岁了，变成了大叔，胖得有些可恶，面目狰狞，内心可憎。青椒两年半了，不知道未来会怎样，生出莫名的烦恼。青椒是一段非常折磨人的时光。家庭、论文、项目、教学，一股脑全来了。我的博士四年多多少少是失败的，略高于平均水平吧。生活也是一样，暮白和我对现状都不满意。我们对于南京市没有一丁点儿的归属感。赶上了这个城市狂飙突进式的房价飞涨，就像玩一个比大小的游戏，一轮输掉之后就完全挤不进去了。对于南京市政府卖地度日的虚假繁荣，我感到非常无语。大城市的发展瓶颈开始凸显出来，过去的人文文化、教育优势、医疗优势，在资本主义的房地产市场面前变得非常无力。工作和博后的生活相比混杂了太多。心态不一样了，干的事情也不一样了，唯有忍耐。或许一切都有点晚，2015年做的最正确的事情是买了句容的房子，否则就要漂泊在南京三年了；而做的最错误的事情是没有及时买南京的房子。&lt;/p&gt;

&lt;p&gt;&lt;code&gt;2014年是转折之年&lt;/code&gt;，实习、毕业、结婚、暮白怀孕、工作，全在这一年。&lt;code&gt;2015年是危机之年&lt;/code&gt;，因为自己没有什么收获，几乎一无所获，还要在行政类的工作中颠簸。2015年米粒出生了；申请了国家社科项目；买了九曲花苑的房子；卷入了智库的工作。&lt;code&gt;2016年是转机之年&lt;/code&gt;，我和令飞的3篇东西陆续出来，和crystal合作的东西也出来了。搬到宝华后忙着装修房子。松了一口气之后，还没缓过来，暮白又怀孕了。&lt;/p&gt;

&lt;p&gt;米粒已经21个月了，她进入了人生当中的叛逆期，执着于融入三岁少年的群体，迷恋&lt;code&gt;游乐场&lt;/code&gt;、游戏机，&lt;a href=&#34;http://www.iqiyi.com/a_19rrhai7jd.html?vfm=2008_aldbd&#34; target=&#34;_blank&#34;&gt;小老虎丹尼尔&lt;/a&gt;。管教米粒的时候，我意识到自己非常没有耐心，非常粗鲁。&lt;/p&gt;

&lt;iframe frameborder=&#34;no&#34; border=&#34;0&#34; marginwidth=&#34;0&#34; marginheight=&#34;0&#34; width=&#34;330&#34; height=&#34;86&#34; src=&#34;http://music.163.com/outchain/player?type=2&amp;id=784553&amp;auto=0&amp;height=66&#34;&gt; &lt;/iframe&gt;

&lt;p&gt;&lt;strong&gt;从松散到紧凑&lt;/strong&gt; 最近这段时间过着猪一样的生活，也没有照顾米粒，一个人晚上睡书房、看电视剧，人也没有精神，不想对着电脑工作，经常看电视，欠了一堆工作没有做。我需要一种可以持续紧凑的生活方式。要牢记：没有危机感就是最大的危机，认为人会懈怠！&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/c120a0afca363631e723b122d1080bfa.png&#34; alt=&#34;sijin&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;博士后办理落户&lt;/strong&gt; 费劲心力，终于办理了博士后落户。对于南京大学的集体户口，我只能说没有任何诚意。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;涂鸦&lt;/strong&gt; 米粒周岁之后，经常给她临摹一些小画，画的都很普通，还不如希特勒的水平，聊以自娱。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;懦弱的拖延症&lt;/strong&gt; 开始陷入了严重的拖延症。当我无法克服困难的时候，就会陷入这种拖延症，开始做别的事情打发时间，结果就更浪费时间了。想到这里，我就会觉得自己太不给力，也太自私了。我觉得问题是自己懦弱的性格。2016年大年初二，去了红山动物园，米粒妈妈感冒了。今年马上也要开学了，回首这段时间我可以说是一无所成。我不知道为什么，我陷入了这种绝境，这段时间确实不好过。时间总在看手机、看米粒的碎片过程中过去了。writing a lot给了我很多启发：任务要小，不要一口吃个胖子，有明确的步骤和要求，比如写完50个字。循序渐进。流动的状态，乐而忘忧废寝忘食，这是做自己擅长的事情的体验。比如我编写程序的时候，克服一个个具体的小问题。写作是第一生产。首先写作，然后其他。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;要更有意义的学习和生活！&lt;/strong&gt; 【2016-07-16】知道自己前进的方向非常重要。每天练习四组平板撑对于自己非常有帮助，这个和健腹轮有异曲同工之妙。加油！⛽️ 【2016-08-02】这个平板撑的计划根本没有实行下去。体育锻炼是我青椒三年的噩梦吧。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;年终奖&lt;/strong&gt;年终奖评奖标准主要看上课、带学生、发论文、主持讲座活动（绩效）。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/de6b12b8f802196b47273dae12d6f0c1.png&#34; alt=&#34;yearly-award&#34; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;骗子公司Lambert图书出版&lt;/strong&gt; 最终放弃了lambert的出版计划，主要是它们实质是一个骗子公司，并不对书籍进行任何有价值的编辑工作。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;计算传播学论坛&lt;/strong&gt; 小可老师提了几个会议改进方案，非常有启发，与大家分享：- 增加期刊合作，推荐期刊发表；- 稿件被接收者可以选择自己承担差旅费用+领取会议补贴 或者 由会议承担差旅而不提供会议补贴；- 会议延长为两天，第一天为工作坊时间，安排网络科学家为传播学研究者讲网络科学的研究方法，或者传播学者为网络科学研究者讲社会科学的理论和方法；第二天为主题发言+分组讨论。基于研究项目的合作计划：合作计划的目标就是促进跨学科的合作，吸引其他学科、其他学校学者的注意力，将研究项目尽快转化为研究成果。在无法引进人才的条件下，依然可以“为我所用”。成功合作的基础在于匹配双方的利益，尤其是在数据、资金、论文署名三个方面，达到最大化的效果。- 找到双方都关注的研究问题和好的数据是吸引合作者的第一步。- 提供适当的资金补助是必要的一环。- 双方在论文署名方面按照通行的规则。- 明确项目需求，比如共同发表几篇国际论文。&lt;/p&gt;

&lt;p&gt;合作计划分为两个阶段：项目合作、访问计划。- 项目合作：这是合作的初级阶段，我方提供问题和数据，双方一起参与，共同分析，承担写作的一方一般为一作。- 访问计划：针对具体的项目和数据，组织短期的访问项目，时间长度为1-2个月，主要任务为共同讨论和动手解决数据分析和研究设计问题。- 第一个阶段项目合作是广撒网，访问计划是精准定位。二者可以结合项目合作+访问计划。&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;百度阅读研究计划&lt;/strong&gt; 研究问题和视角：本研究试图从网络科学的角度分析用户的注意力如何在书籍构成的人类知识版图当中流动。研究方法：构建以书籍为节点、以用户的阅读行为为链接的人类阅读行为的注意力流动网络。同时，每一天构建一个注意力流动网络，这样可以观察到每一天的活跃用户数量和阅读规模，进而可以分析用户的数量（UV）和用户的浏览量（PV）之间的异速增长关系。研究意义：基于这个网络可以观察到人类在知识获取过程，将书籍嵌入到一个流动网络所对应的几何空间中，进而可以进行节点的聚类，计算任意两个节点的相似性，划分以书籍为单位的知识版图的层级关系。数据需求：1. 数据抽样：- 首先，从总体当中随机抽取10万用户；- 然后，对于每一个用户抽取2016年1月1日到6月30日之间的所有阅读行为记录。格式如下：用户id、时间、书籍id、阅读量（字数等指标）、持续时间（秒钟）- 此外，抽取这10万用户所阅读的所有的图书的基本信息，例如：书籍id、书名、作者、出版社、年份、定价，等。- 最后，对于这10万用户的图书购买行为进行记录，格式如下：用户id、时间、书籍id、花费金额。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/67d2a8bc0f1dca86a4cf4d541574e518.png&#34; alt=&#34;elephant_smoking&#34; /&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>记凯风-集智训练营</title>
      <link>https://chengjunwang.com/en/note/2016-10-13-swarma-camp/</link>
      <pubDate>Thu, 13 Oct 2016 00:00:00 +0000</pubDate>
      
      <guid>https://chengjunwang.com/en/note/2016-10-13-swarma-camp/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/ed6c9d46333416093c4d0fd0e9712dc4.png&#34; alt=&#34;缪斯&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;二十一世纪的科学&#34;&gt;二十一世纪的科学&lt;/h3&gt;

&lt;p&gt;邓肯*瓦茨在2007年的时候写了一篇名为《二十一世纪的科学》的文章，提出了”二十一世纪的科学“的概念。瓦茨认为二十一世纪的科学不是自然科学，而是社会科学。因为社会科学涉及到了海量的异质化个体之间的互动，所以研究社会科学所面临的挑战特别大；但是基于数字化媒体（如互联网、手机）等手段收集的人类行为数据则为这种研究提供了丰富的数据。类似这种视角，Lazer等人（2009）更完整地表达为”计算社会科学”研究的思路。基于计算社会科学的研究思路为分析社会现象（尤其是人类传播行为）提供了丰富的可能性，因而我们见证了最近十年里的社会科学的飞跃。&lt;/p&gt;

&lt;p&gt;但是计算社会科学的发展思路依然留下了很多问题，比如如何来把握不同学科的资源来发展计算社会科学。研究者习惯于使用韦恩图来表达这种跨学科的研究的特点，比如讲数据科学表达为科学（一个领域的知识）、统计、机器学习等算法三个领域的重叠。我自己越来越倾向于认为计算社会科学是社会科学的领域知识、物理学模型、计算机算法三个领域的重叠。但是这种重叠不是完全平等的重合。因为几乎没有一个人可以熟练地掌握三个领域的知识，这种人非常罕见，因而可以称之为“独角兽”。多数人其实只是在一个方面发展得比较好，比如传统的社会科学研究者其实是对本领域的知识方面比较了解，而对于计算机算法和物理学模型方面则有所欠缺。这个时候有两个平行的发展思路：一个是扬长避短、一个是取长补短。&lt;/p&gt;

&lt;p&gt;在读博士等初级阶段往往是一个取长补短的这样一个阶段，这个时候的研究者往往广泛涉猎其它学科的知识，尤其是物理学的理论模型和计算机算法的知识，所以可以说这是一个做加法的过程。但是科学研究往往是在一个点上突破的，为了论文发表，研究者必须要有所取舍，不可能什么都去研究，后者会导致研究者非常疲惫。持之以恒的将研究兴趣聚焦于一个点就是一个做减法的过程。这往往出现在科研职业的阶段当中。当你博士毕业进入到了一个职业阶段，你就必须考虑聚焦兴趣、做减法。&lt;/p&gt;

&lt;p&gt;我个人正是经历了这样的一个先做加法后做减法的过程。读博士的时候对各种编程语言和工具感兴趣，广泛地接触各种计算机方面的基础知识。但这也是一个让人疲惫的阶段，因为种种原因，我们不断地被吸引到各种看似非常有吸引力的话题上面，于是出现注意力耗散的现象（当然，你不学习看电影其实也很累），这些都是不利于发表的。当你进入研究的职业生涯之后，你就不许考虑产出的问题，被迫地做减法。我一直觉得或许此后也将处于做减法的过程，将自己的研究聚焦于可以产出论文的领域，知道参加了第一届“凯风-集智训练营”。&lt;/p&gt;

&lt;h3 id=&#34;凯风-集智训练营&#34;&gt;凯风-集智训练营&lt;/h3&gt;

&lt;p&gt;2016年集智俱乐部与凯风合作，组织了第一届“凯风-集智训练营”，这次训练营于2016年10月7日集合，10月12日结束，10月13日训练营正式结束。&lt;/p&gt;

&lt;h3 id=&#34;作为团队的微信群&#34;&gt;作为团队的微信群&lt;/h3&gt;

&lt;p&gt;将微信群看为一个知识分享的团队，不同团队的类型。我们假设存在着一个有效团队， 这里面存在着一种集体智慧，是一个crowdsoucing众包的过程，团队的成功，活跃性。Pentland所思考的是团队内部的沟通。规模和消息数量，指数增加。贡献的有效，这个和团队的绩效或者成功是不容易的。引用带来成功！&lt;/p&gt;

&lt;p&gt;解决业务问题，搞营销和赚钱，推荐系统。创新的扩散。进群的过程。关键节点，群的影响力最大化。&lt;/p&gt;

&lt;p&gt;激活微信群&lt;/p&gt;

&lt;p&gt;知识传播、流动网络&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Github个人网站维护</title>
      <link>https://chengjunwang.com/en/note/2016-08-03-github/</link>
      <pubDate>Wed, 03 Aug 2016 00:00:00 +0000</pubDate>
      
      <guid>https://chengjunwang.com/en/note/2016-08-03-github/</guid>
      <description>

&lt;h2 id=&#34;hugo-on-github&#34;&gt;Hugo on github&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Create on GitHub &lt;code&gt;mywebsite-hugo&lt;/code&gt; repository (it will host Hugo’s content)&lt;/li&gt;
&lt;li&gt;Create on GitHub &lt;code&gt;chengjun.github.io&lt;/code&gt; repository (it will host the public folder: the static website)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cd github&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;git clone https://github.com/chengjun/mywebsite-hugo.git&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;hugo new site mywebsite-hugo --force&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;cd mywebsite-hugo&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;download &lt;a href=&#34;https://github.com/gcushen/hugo-academic/archive/master.zip&#34; target=&#34;_blank&#34;&gt;Academic&lt;/a&gt; and extract it into a themes/academic folder within your Hugo website.&lt;/li&gt;
&lt;li&gt;If you are creating a new website, copy the contents of the exampleSite folder to your website root folder, overwriting existing files if necessary.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;hugo server --watch&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Once you are happy with the results, &lt;code&gt;Ctrl+C&lt;/code&gt; (kill server) and &lt;code&gt;rm -rf public&lt;/code&gt; (don’t worry, it can always be regenerated with hugo -t &lt;yourtheme&gt;)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;git submodule add https://github.com/chengjun/chengjun.github.io.git public --force&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Almost done: add a deploy.sh script to help you&lt;/li&gt;
&lt;li&gt;and make it executable: &lt;code&gt;chmod +x deploy.sh&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;./deploy.sh &amp;quot;Your optional commit message&amp;quot;&lt;/code&gt; to send changes to &lt;username&gt;.github.io (careful, you may also want to commit changes on the &lt;your-project&gt;-hugo repo).&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-bash&#34;&gt;#!/bin/bash

echo -e &amp;quot;\033[0;32mDeploying updates to GitHub...\033[0m&amp;quot;

# Build the project.
hugo # if using a theme, replace by `hugo -t &amp;lt;yourtheme&amp;gt;`

# Go To Public folder
cd ..

cd chengjun.github.io

cp -av /Users/chengjun/github/mywebsite-hugo/public/* .


# Add changes to git.
git add -A

# Commit changes.
msg=&amp;quot;rebuilding site `date`&amp;quot;
if [ $# -eq 1 ]
  then msg=&amp;quot;$1&amp;quot;
fi
git commit -m &amp;quot;$msg&amp;quot;

# Push source and build repos.
git push origin master

# Come Back
cd ..
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;domain-name&#34;&gt;domain name&lt;/h2&gt;

&lt;p&gt;在godaddy上花了122块钱买了两年的域名 &lt;a href=&#34;http://chengjunwang.com&#34; target=&#34;_blank&#34;&gt;chengjunwang.com&lt;/a&gt;。简单设置cname和goddy上的a record和cname www record就好了。主要参考了&lt;a href=&#34;http://andrewsturges.com/blog/jekyll/tutorial/2014/11/06/github-and-godaddy.html&#34; target=&#34;_blank&#34;&gt;这里&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;我对于html的无知可以说是到了某种无以附加的程度。虽然我投入了很多时间，但往往都是干中学。而不是系统的模仿。通常会发现很多困惑，然后每次对我而言都是一个个小trick就解决了问题。&lt;/p&gt;

&lt;h2 id=&#34;平行的文本框&#34;&gt;平行的文本框&lt;/h2&gt;

&lt;p&gt;如何添加一个平行的文本框？这个困惑了我很久的问题，解决方法是设置position为absolute。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#textbox {
  position:absolute;
  top: 200px;
  width: auto;
  margin-left: 50px;
  background-color: rgba(0,0,0,0.2);
  opacity:1;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;修改之后又觉得不好看。又尝试了iframe，也不是很合适。&lt;/p&gt;

&lt;p&gt;之后发现一篇好文章&lt;a href=&#34;http://css-tricks.com/examples/ShapesOfCSS/&#34; target=&#34;_blank&#34;&gt;The Shapes of CSS&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;修改了其中的talkbubble，放在application.css中。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#talkbubble {
   position:absolute;
   top: 250px;
   margin-left: 50px;
   width: 300px;
   height: 150px;
   background-color: rgba(0,0,0,0.2);
   -moz-border-radius:    10px;
   -webkit-border-radius: 10px;
   border-radius:         10px;
}
#talkbubble:before {
   content:&amp;quot;&amp;quot;;
   position: absolute;
   margin-left: 300px;
   top: 80px;
   width: 0;
   height: 0;
   background-color: transparent;
   #border-top: 5px solid transparent;
   border-bottom: 30px solid rgba(0,0,0,0.2);
   border-right: 120px solid transparent;
}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;在index.html里加入talkbubble来显示新闻。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;div id=&amp;quot;talkbubble&amp;quot;&amp;gt;
  &amp;lt;h2&amp;gt;&amp;lt;a href=&amp;quot;http://chengjun.github.io/cv/news.html&amp;quot;&amp;gt;News&amp;lt;/a&amp;gt;&amp;lt;/h2&amp;gt;
  &amp;lt;p&amp;gt;[Feb 23] &amp;lt;a href=&amp;quot;https://pypi.python.org/pypi/scholarNetwork/&amp;quot;&amp;gt;scholarNetwork&amp;lt;/a&amp;gt;--a python package which can help crawl and visualize the coauthor network of Google Scholar is released to Pypi. &amp;lt;/p&amp;gt;
&amp;lt;/div&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;谷歌字体&#34;&gt;谷歌字体&lt;/h3&gt;

&lt;p&gt;俺天朝跟谷歌成了死敌。背后的很多想法让人很无奈。虽然米果是两党制，但是谷歌据说也在检查之列。到了俺朝，美利坚放不下身段了，天朝也放不下，于是就僵住了。于是很多谷歌的服务都不好用了。比如谷歌学术，谷歌邮箱。哎妈呀，老伤心老。我这里要说的却是谷歌的字体格式。&lt;/p&gt;

&lt;p&gt;在天朝载入我的博客慢的原因就是因为style.css里（media/css/style.css）调用里谷歌字体。好伤心，每次访问速度都很慢，非得翻墙才行。可是不能让俺的读者们也翻墙吧，这也太虎了。反正我的审美要求也没那么高。雪藏了吧。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#@import url(http://fonts.googleapis.com/css?family=Galdeano);
#@import url(http://fonts.googleapis.com/css?family=Electrolize);
#@import url(http://fonts.googleapis.com/css?family=Cuprum);
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;改过来之后&#34;&gt;改过来之后&lt;/h3&gt;

&lt;p&gt;我了个去，怎么英文字体变这么大了，BIG好吧，只能骗自己感觉萌萌哒啦。&lt;/p&gt;

&lt;h3 id=&#34;把谷歌字体放在本站&#34;&gt;把谷歌字体放在本站&lt;/h3&gt;

&lt;p&gt;其实很早就注意到andy wang的这个解决方法，一直比较懒。还是去在mac上安装了github for mac，于是把andy wang的media文件夹偷到我这里来用。&lt;/p&gt;

&lt;p&gt;Andy Wang:
&lt;code&gt;你好，我扒走了你的模板，然后在天朝用发现非常慢，然后调试了一下，发现是在style.css和home.css里面有从别的网站下载字体文件的, http://fonts.googleapis.com/这个网站在天朝访问非常不稳定，所以做了一些小改动，把字体文件都下载到网站上了，页面渲染速度也快了很多。https://github.com/synckey/synckey.github.io/tree/master/media/css&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;终于，世界再次变得一片清凉了。真好。&lt;/p&gt;

&lt;h2 id=&#34;categories&#34;&gt;categories&lt;/h2&gt;

&lt;p&gt;我喜欢在网页端更新github上的日志，通过设置只显示categories为blog的文档，我可以每次修改一点东西就完全在网页端写日志。图片附件什么的可以放在七牛网站，感觉非常好。&lt;/p&gt;

&lt;p&gt;下面是我用来复制和重命名1000个md文本的python代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;        # -*- coding: utf-8 -*-
        &amp;quot;&amp;quot;&amp;quot;
        Created on Thu Aug 07 14:05:09 2014

        @author: chengjun
        &amp;quot;&amp;quot;&amp;quot;

        import shutil
        path = &amp;quot;D:/github/chengjun.github.io/_posts/&amp;quot;   

        demo = path + &amp;quot;2010-01-01-demo.md&amp;quot;

        for i in range(1000):
            newFile = path + &amp;quot;2010-01-&amp;quot; + str(i) + &amp;quot;.md&amp;quot;
            print newFile
            shutil.copy(demo, newFile)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;http://chengjun.qiniudn.com/7.jpg&#34; alt=&#34;&#34; /&gt;&lt;/p&gt;

&lt;h2 id=&#34;jekyll3-0&#34;&gt;jekyll3.0&lt;/h2&gt;

&lt;p&gt;这个是一个测试，jekyll更新到了3.0版本，所以github也相应发生了改变（见&lt;a href=&#34;https://github.com/blog/2100-github-pages-now-faster-and-simpler-with-jekyll-3-0&#34; target=&#34;_blank&#34;&gt;这里&lt;/a&gt;）。这导致了我的文章无法显示，解决的方法是去掉表头的时间一行，就可以了，我还没有明白原因。&lt;/p&gt;

&lt;h2 id=&#34;某一类的博文目录&#34;&gt;某一类的博文目录&lt;/h2&gt;

&lt;p&gt;主要参考 &lt;a href=&#34;http://stackoverflow.com/questions/12008108/filtering-posts-using-categories-in-jekyll-bootstrap&#34; target=&#34;_blank&#34;&gt;filtering-posts-using-categories-in-jekyll-bootstrap&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;这个是使用liquid写的，介绍见&lt;a href=&#34;http://liquidmarkup.org/&#34; target=&#34;_blank&#34;&gt;这里&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;主要添加两行liquid代码：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;if post.categories contains &#39;生活&#39;
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;不要忘记endif&lt;/p&gt;

&lt;p&gt;由于liquid语言本身不能在本网站呈现，读者可以查看&lt;a href=&#34;https://github.com/chengjun/chengjun.github.io/blob/master/life/index.html&#34; target=&#34;_blank&#34;&gt;这里&lt;/a&gt;。&lt;/p&gt;

&lt;p&gt;琢磨出一个不用每次在jekyll上写博客都要上传的方法：将要显示的博客的categories设为blog，一次上传n个草稿（不指定categories），使用liquid语言指定在博客列表中只显示blog类的。这样草稿不会被显示，每次在线登录github，更改一个草稿的标题和categories就可以了。图片用外链flickr。&lt;/p&gt;

&lt;h2 id=&#34;paginator&#34;&gt;paginator&lt;/h2&gt;

&lt;p&gt;今天下午鼓捣了半天jekyll的设置，最初是想弄一个&lt;a href=&#34;http://jekyllrb.com/docs/pagination/&#34; target=&#34;_blank&#34;&gt;paginator&lt;/a&gt;给文章列表分页，结果搞了半天，没搞定。各种问题。&lt;a href=&#34;http://patrick-mckinley.com/tech/jekyll-pagination.html&#34; target=&#34;_blank&#34;&gt;Patrick McKinley&lt;/a&gt;说是因为不是网站第一层的index.html，所以jekyll官方设置时没有用的。于是，直接用他的方法，发现没有办法在我的网站上实现。&lt;/p&gt;

&lt;p&gt;Patrick McKinley的博客以图片来显示博客文章的方法似乎很酷。感兴趣，去研究了下他的&lt;a href=&#34;https://github.com/lilmuckers/lilmuckers.github.com&#34; target=&#34;_blank&#34;&gt;代码&lt;/a&gt;，大致是设置了post.image，图片存放在assets中的图片文件夹。我弄了下，也没弄成功。&lt;/p&gt;

&lt;p&gt;又看到&lt;a href=&#34;tmtxt.github.com&#34; target=&#34;_blank&#34;&gt;Mr Trường at RMIT&lt;/a&gt;的网页，翻了半天，加了related_posts功能。&lt;/p&gt;

&lt;p&gt;没想到晚上更惨，被python的encoding error搞得很头大！&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>测试Atom软件</title>
      <link>https://chengjunwang.com/en/note/2016-05-04-atom/</link>
      <pubDate>Wed, 04 May 2016 00:00:00 +0000</pubDate>
      
      <guid>https://chengjunwang.com/en/note/2016-05-04-atom/</guid>
      <description>

&lt;p&gt;使用&lt;strong&gt;Atom&lt;/strong&gt;来编写markdown非常不错，按ctr+shift+m就可以进入预览模式，我想markdownpad可以抛弃掉了。这个软件更流畅，非常棒！因为我经常要写github的博客，使用atom时最容易的形式。减少了甚多负担，非常的自由，如果它具有上传功能就好了。&lt;/p&gt;

&lt;h1 id=&#34;数学公式&#34;&gt;数学公式&lt;/h1&gt;

&lt;p&gt;安装了markdown-preview-enhanced，可以很方便的展示数学公式，比如 $x = y^2$。&lt;/p&gt;

&lt;p&gt;$E = MC^2$&lt;/p&gt;

&lt;p&gt;需要按 ctrl+ shift+ x，按ctrl+ shift+ m无法展示数学公式。&lt;/p&gt;

&lt;h1 id=&#34;参考文献&#34;&gt;参考文献&lt;/h1&gt;

&lt;p&gt;插入一个参考文献&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:a&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:a&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:b&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:b&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:c&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:c&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;再次插入一个参考文献&lt;sup class=&#34;footnote-ref&#34; id=&#34;fnref:b&#34;&gt;&lt;a rel=&#34;footnote&#34; href=&#34;#fn:b&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;插入一个参考文献[^a][^b][^c]

[^a]: author1, 2014, This is a title. Journal. 00-00
[^b]: author2, 2014, This is a title. Journal. 00-00
[^c]: author5, 2014, This is a title. Journal. 00-00

再次插入一个参考文献[^b]
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;图片上传功能&#34;&gt;图片上传功能&lt;/h1&gt;

&lt;p&gt;结果就发现阿里巴巴程序员knightli的两个atom插件：qiniu-uploader和markdown-assistant。设置好七牛的账号既可以非常方便地使用图床了。赞。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/1895b6670f7276a1f10903cf1305e686.png&#34; alt=&#34;qiniu-uploader&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;git上传功能&#34;&gt;Git上传功能&lt;/h1&gt;

&lt;p&gt;接着又找到了&lt;a href=&#34;https://atom.io/packages/git-plus&#34; target=&#34;_blank&#34;&gt;git-plus&lt;/a&gt;这个强大的插件，可以直接在atom里上传了。好吧！社区的力量真强大。太好了。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;按 *command+shift+H*打开操作界面。&lt;/li&gt;
&lt;li&gt;选择&lt;code&gt;add all commit+push&lt;/code&gt;就可以上传了！

&lt;ul&gt;
&lt;li&gt;填写commit的内容&lt;/li&gt;
&lt;li&gt;command+s保存即可上传&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/7497f687c919c447882fd0abd4d68bc2.png&#34; alt=&#34;surprise&#34; /&gt;&lt;/p&gt;

&lt;p&gt;震惊脸！！！!&lt;/p&gt;

&lt;h1 id=&#34;atom-html-preview&#34;&gt;atom-html-preview&lt;/h1&gt;

&lt;p&gt;如果你需要修改的html文件的话，可以很方便地使用atom-html-preview插件。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;快捷键&lt;code&gt;ctr+shift+h&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/d832f904c15079ed7f38816a5407c1b6.png&#34; alt=&#34;atom-html-preview&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;script&#34;&gt;script&lt;/h1&gt;

&lt;p&gt;安装了script插件，可以运行包括shell等多种语言。我使用它来运行deploy.sh脚本，实现半自动上传功能。&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/2df996e188f3365d7c8cdf08c930486e.png&#34; alt=&#34;script&#34; /&gt;&lt;/p&gt;

&lt;h1 id=&#34;参考文献-1&#34;&gt;参考文献&lt;/h1&gt;
&lt;div class=&#34;footnotes&#34;&gt;

&lt;hr /&gt;

&lt;ol&gt;
&lt;li id=&#34;fn:a&#34;&gt;author1, 2014, This is a title. Journal. 00-00
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:a&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:b&#34;&gt;author2, 2014, This is a title. Journal. 00-00
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:b&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:c&#34;&gt;author5, 2014, This is a title. Journal. 00-00
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:c&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li id=&#34;fn:b&#34;&gt;author2, 2014, This is a title. Journal. 00-00
 &lt;a class=&#34;footnote-return&#34; href=&#34;#fnref:b&#34;&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>语言是一道门槛</title>
      <link>https://chengjunwang.com/en/note/2016-05-04-sr-revising/</link>
      <pubDate>Wed, 04 May 2016 00:00:00 +0000</pubDate>
      
      <guid>https://chengjunwang.com/en/note/2016-05-04-sr-revising/</guid>
      <description>

&lt;h2 id=&#34;billing&#34;&gt;Billing&lt;/h2&gt;

&lt;p&gt;Bill to&lt;/p&gt;

&lt;p&gt;Cheng-Jun Wang
wangchengjun@nju.edu.cn
+8617751010496&lt;/p&gt;

&lt;h3 id=&#34;billing-method&#34;&gt;Billing method&lt;/h3&gt;

&lt;p&gt;Invoice
Billing address&lt;/p&gt;

&lt;p&gt;Nanjing University
22 Hankou Road, Gulou District, Nanjing University
A306 Feiyimin Building, School of Journalism and Communication
Nanjing
China
China
210093&lt;/p&gt;

&lt;h3 id=&#34;scientific-reports-出版业的一个新时代&#34;&gt;Scientific Reports —出版业的一个新时代&lt;/h3&gt;

&lt;p&gt;Scientific Reports 是来自 Nature 杂志出版者的一个发表原始研究工作的刊物，在线出版，公开访问，内容涉及自然科学所有领域。托管在 nature.com（该网站是由Springer Nature出版的80多种刊物的共同门户，每月全球有数百万科学家访问）上， Scientific Reports 是任何人都可以公开访问的，发表在技术上可靠的、各领域内的专业人员感兴趣的原始研究论文，其相关内容的访问不受任何限制。&lt;/p&gt;

&lt;p&gt;文章处理费自2015年1月起将按如下标准执行，以下费用包括增值税或其他当地税费。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;£990 (UK &amp;amp; RoW)&lt;/li&gt;
&lt;li&gt;$1,495 (The Americas)&lt;/li&gt;
&lt;li&gt;€1,165 (Europe)&lt;/li&gt;
&lt;li&gt;¥170,000 (Japan)&lt;/li&gt;
&lt;li&gt;¥9,900 (China)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Scientific Reports 是：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;快速的（Fast）—迅速审稿和发表&lt;/li&gt;
&lt;li&gt;严格的（Rigorous）—由学术界至少一个成员来进行同行评审&lt;/li&gt;
&lt;li&gt;公开的（Open）—文章对所有人都是免费的，作者保留版权&lt;/li&gt;
&lt;li&gt;可见的（Visible）—增强的浏览和搜索功能，确保您的文章能够被注意到&lt;/li&gt;
&lt;li&gt;互联的（Interlinked）—与整个 nature.com 网站上的相关文章相互之间建立链接&lt;/li&gt;
&lt;li&gt;全球的（Global）— 托管在 nature.com 上，全球媒体覆盖&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;根据&lt;a href=&#34;http://www.docin.com/p-1309834502.html&#34; target=&#34;_blank&#34;&gt;南京大学超一流、学科群一流、SCI A 区和 B 区期刊目录&lt;/a&gt;，Scientific reports属于SCI索引期刊的A区。2016年公布的Scientific Reports 影响因子5.228*
*2015 Journal Citation Reports® Science Edition (Thomson Reuters, 2016)&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/5e1e31a8ec89e5fa3374b23baa8eadde.png&#34; alt=&#34;SR&#34; /&gt;&lt;/p&gt;

&lt;p&gt;南京大学关于Scientific Reports论文发表的新闻：
&lt;a href=&#34;http://news.nju.edu.cn/searchsite.php?r=scientific%20reports&#34; target=&#34;_blank&#34;&gt;http://news.nju.edu.cn/searchsite.php?r=scientific%20reports&lt;/a&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;2015-11-26 Scientific Reports刊登我校博士生俞恂大陆火山成因研究新成果&lt;/li&gt;
&lt;li&gt;2014-10-29 大气科学学院符淙斌院士团队在Scientific Reports杂志发表最新研究成果&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;8月19日&#34;&gt;8月19日&lt;/h2&gt;

&lt;h3 id=&#34;hidden-direction&#34;&gt;hidden direction&lt;/h3&gt;

&lt;p&gt;中间因为editor换了，又更新过一次reviewer，耽搁了时间。2015年12月10日进入decision started的状态；过了一个半月，也就是2016年1月24日到了2月2日才收到editor回复，之后又过了&lt;strong&gt;十天&lt;/strong&gt;时间收到SR的邮件。所以概括一下：这个editor的速度灰常慢，再等半个月。&lt;/p&gt;

&lt;p&gt;突然发现9月2日editor提交了审稿意见，经过了一个多月终于Manuscript Submitted。一般而言进入此环节表示editor做好了决定，Finger crossed！耐心地等待下一周吧。9月5日，文章被接收。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Manuscript Submitted&lt;/strong&gt;  2nd September 16&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Decision Started  26th July 16&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Manuscript assigned to peer-reviewer/s    26th July 16&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Manuscript Assigned to Peer-Reviewer/s    14th June 16&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Manuscript Assigned to Editor 11th June 16&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Submission Not Complete   11th June 16&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;二审论文提交是在6月11日（中间隔了四个月），14日送审，到7月26日审完，发给编辑做决定。编辑似乎需要一个月才愿意给决定。到本月26日，编辑就消耗了一个月了，拭目以待。&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Decision Started  26th July 16&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Manuscript assigned to peer-reviewer/s    26th July 16&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Manuscript Assigned to Peer-Reviewer/s    14th June 16&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Manuscript Assigned to Editor 11th June 16&lt;br /&gt;
Submission Not Complete 11th June 16&lt;/li&gt;
&lt;li&gt;Manuscript Submitted  24th January 16&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Decision Started  10th December 15&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Manuscript assigned to peer-reviewer/s    10th December 15&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Manuscript Assigned to Peer-Reviewer/s    17th November 15&lt;/li&gt;
&lt;li&gt;Manuscript Assigned to Editor 30th September 15&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Manuscript Submitted  29th September 15&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Submission Not Complete   22nd September 15&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Quality Check Started 9th September 15&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Submission Not Complete   29th August 15&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;tracing&#34;&gt;tracing&lt;/h3&gt;

&lt;p&gt;二审8月5日提交修改版本，进入quality check环节，8月12日收到邮件说要修改格式，当晚修改完毕提交。quality check到8月18日，进入decision started环节。没有想到当天就进入了Manuscript Submitted环节。一般而言进入此环节表示editor做好了决定。果然，晚上十一点半收到了acceptance letter。&lt;/p&gt;

&lt;p&gt;Current Stage:  Decision Started (1 days)&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Manuscript Submitted&lt;/strong&gt;  18th August 16&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Decision Started  18th August 16&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Manuscript Assigned to Editor 5th August 16&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Submission Not Complete   9th July 16&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Manuscript Submitted&lt;/strong&gt;  6th July 16&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Decision Started  6th July 16&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Manuscript Assigned to Editor 2nd June 16&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Manuscript Submitted&lt;/strong&gt;  2nd June 16&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Quality Check Started 29th May 16&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Submission Not Complete   24th May 16&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;sr的latex模板&#34;&gt;SR的LaTeX模板&lt;/h2&gt;

&lt;p&gt;8月12日，SR邮件说我的reference不合规范，发现只有严格按照overleaf上的模板才能生成，索性直接使用这个模板&lt;/p&gt;

&lt;p&gt;&lt;a href=&#34;https://www.overleaf.com/5874225tbgnxc#/19389854/&#34; target=&#34;_blank&#34;&gt;https://www.overleaf.com/5874225tbgnxc#/19389854/&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;点击project，点击 download as zip即可下载。&lt;/p&gt;

&lt;p&gt;因为不允许上传bib和cls文件，所以每次validation的时候还可以选择自己上传一个pdf。修改时只需要删除原来的tex文件，重新上传tex，并在validation的时候重新上传一个pdf即可。&lt;/p&gt;

&lt;h2 id=&#34;积极向上的心智&#34;&gt;积极向上的心智&lt;/h2&gt;

&lt;p&gt;突然我挺烦“正能量”这个词，因为使用这个词的地方往往是政府缺位的地方。&lt;/p&gt;

&lt;p&gt;但是对于个体而言，找到正能量确实非常有必要的。每一个人都会面对生活的各种挑战，我们无法预测未来会是怎样，上天将会如何试探我们。&lt;/p&gt;

&lt;p&gt;面对生活，积极向上的精神是不可或缺的。&lt;/p&gt;

&lt;p&gt;这是一种心智的力量，需要个体去修炼。禅宗讲行走坐卧皆是修行是有道理的，因为每一个细节都在锤炼着我们的意志。我喜欢心智这个词，因为当我们面对挑战的时候，仅仅强调勇气、勤奋是远远不够的。因为我们可能是那个面对风车的堂吉诃德；因为我们可能已经在错误的道路上渐行渐远。要有智慧来面对挑战。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;不仅仅是蔑视困难，而且要能积极地应对挑战。
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;自强不息&#34;&gt;自强不息&lt;/h2&gt;

&lt;p&gt;2016年8月3日，最近比较累。lingfei将tracing一文全面改了一遍，增加了可读性，文章的长度大幅度降低。我却变得更懒了，没有动力去修改。感觉我的个性里消极的成分太多了。&lt;/p&gt;

&lt;p&gt;自强不息是一个多么艰难的目标。但是我应当牢记，这是最困难的阶段，跨过这个阶段，我要做的更好。放下那些不必要的压力，做回来自我。&lt;/p&gt;

&lt;p&gt;要善于给自己鼓劲，在黑暗里看到希望的光。加油！&lt;/p&gt;

&lt;h2 id=&#34;the-collective-direction-of-attention-diffusion&#34;&gt;The collective direction of attention diffusion&lt;/h2&gt;

&lt;p&gt;2016年7月26日，晚上习惯性地刷了下sr，发现42天后，状态变为decision started。于是我想说要cross my fingers，没想到在西方是这样交叉手指祈祷的，姿势是把食指放在中指前面。&lt;/p&gt;

&lt;p&gt;关于cross fingers的解释如下图：&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/99886fb7ed981357171ef53f332a1ac3.png&#34; alt=&#34;cross_fingers&#34; /&gt;&lt;/p&gt;

&lt;p&gt;这一张图片更有爱！
&lt;img src=&#34;http://oaf2qt3yk.bkt.clouddn.com/5d47fa9f9d4e6f775c5f68aec81f7452.png&#34; alt=&#34;cross_fingers_girl&#34; /&gt;&lt;/p&gt;

&lt;p&gt;到现在快有一个月了，还是这个状态：没有消息就是好消息。不要杞人忧天，庸人自扰。&lt;/p&gt;

&lt;h2 id=&#34;proofread&#34;&gt;Proofread&lt;/h2&gt;

&lt;p&gt;We find that the flow of attention on the Web forms a directed, tree-like structure. Using the data of a news sharing website, we construct clickstream networks in which nodes are news stories and edges represent the consecutive clicks between two stories. To identify the flow direction of clickstreams, we define the “flow distance” of nodes (Li), which measures the average number of steps a random walker takes to reach the ith node. It is observed that Li is related with with the clicks (Ci) to news stories and the age (Ti) of stories. Putting these three variables together help us understand the rise and decay of news stories from a network perspective. We also find that the studied clickstream networks preserve a stale structure over time, leading to the scaling between users and clicks. The universal scaling behavior is confirmed by the data from 1,000 Web forums. We suggest that the tree-like, stable structure of clickstream networks reveals the time-sensitive preference of users in online browsing. To test our assumption, we discuss three models on individual browsing behavior, and compare the simulation results with empirical data.&lt;/p&gt;

&lt;p&gt;我们发现万维网上的注意力流动构成了一个有向的树状结构。使用一个新闻分享类网站的数据，我们构造了点击流网络：节点是新闻，链接是两个新闻之间的点击流。为了确认点击流的流动方向，我们定义了节点的流距离Li：它测量了一个随机游走者需要多少步才能走到该节点。研究发现Li与新闻点击量Ci和新闻的年龄Ti有关。同时分析这三个变量可以帮助我们从一个网络的视角理解新闻的出现和消亡过程。我们也发现构造的点击流网络在不同的时间可以维持一个稳定的结构，而这个稳定的结构解释了用户和点击流之间的标度关系。1000个网络论坛的数据验证了这种普世的标度行为。我们认为点击流网络的这种树状的稳定结构表明了用户浏览行为的时间敏感性偏好。为了验证这个假设，我们讨论了三种用户浏览行为的模型，并比较了计算机模拟的结构与实证数据之间的关系。&lt;/p&gt;

&lt;p&gt;念兹在兹，我手写我心真的是不容易实现的状态。语言是一道门槛，只有越过它才能更好地与人交流。&lt;/p&gt;

&lt;h2 id=&#34;arxiv&#34;&gt;arxiv&lt;/h2&gt;

&lt;p&gt;5月27日，我开始尝试&lt;a href=&#34;http://www.bbioo.com/people/108-480562-1.html&#34; target=&#34;_blank&#34;&gt;将bib嵌入tex的方法&lt;/a&gt;：&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;正常编译tex和bib文件生成bbl文件，&lt;/li&gt;
&lt;li&gt;复制bbl文件所有内容放进tex文件&lt;/li&gt;
&lt;li&gt;编译一次bbl，然后一次latex，就可以显示正确了。&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;其中，复制bbl文件所有内容放进tex文件的格式如下：&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;\section*{Competing financial interests}
The authors declare no competing financial interests.

%\section*{References}
% The bibtex filename apalike unsrt ieeetr acm alpha plain
\bibliographystyle{unsrt}
\begin{thebibliography}{10}

\bibitem{ginsberg2009detecting}
Jeremy Ginsberg, Matthew~H Mohebbi, Rajan~S Patel, Lynnette Brammer, Mark~S
  Smolinski, and Larry Brilliant.
\newblock Detecting influenza epidemics using search engine query data.
\newblock {\em Nature}, 457(7232):1012--1014, 2009.

\end{thebibliography}
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;之后，我按这个方法将转化后的tex文件，图片文件上传到了arxiv上面。You may update your submission at: &lt;a href=&#34;https://arxiv.org/submit/1571860&#34; target=&#34;_blank&#34;&gt;https://arxiv.org/submit/1571860&lt;/a&gt; Your article is scheduled to be announced at Mon, 30 May 2016 00:00:00 GMT. 之前遇到的问题说上传的latex不符合规定，修改半天也不得要领。&lt;/p&gt;

&lt;p&gt;更新arXiv的论文版本也挺方便，重新上传上述内容即可较快更新。&lt;/p&gt;

&lt;h3 id=&#34;bibitem&#34;&gt;bibitem&lt;/h3&gt;

&lt;p&gt;bibtex被编译之后其实是这些bibitem。它们的格式其实很简单，如果google scholar可以直接另存为bibitem就省事了。&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;\bibitem{penrose2003random}
Mathew Penrose.
\newblock {\em Random geometric graphs}.
\newblock Number~5. Oxford University Press, 2003.

\bibitem{papadopoulos2012popularity}
Fragkiskos Papadopoulos, Maksim Kitsak, M~{\&#39;A}ngeles Serrano, Mari{\&#39;a}n Bogun{\&#39;a}, and Dmitri Krioukov.
\newblock Popularity versus similarity in growing networks.
\newblock {\em Nature}, 489(7417):537--540, 2012.

\bibitem{brockmann2013hidden}
Dirk Brockmann and Dirk Helbing.
\newblock The hidden geometry of complex, network-driven contagion phenomena.
\newblock {\em Science}, 342(6164):1337--1342, 2013.

\bibitem{eagle2010network}
Nathan Eagle, Michael Macy, and Rob Claxton.
\newblock Network diversity and economic development.
\newblock {\em Science}, 328(5981):1029--1031, 2010.
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
  </channel>
</rss>
