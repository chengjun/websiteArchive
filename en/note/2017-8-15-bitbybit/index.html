<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.18.1" />
  <meta name="author" content="Cheng-Jun Wang">
  <meta name="description" content="Assistant Research Fellow">

  
  
  
    
  
  
    
    
    <link rel="stylesheet" href="/css/highlight.min.css">
    
  
  <link rel="stylesheet" href="/css/fontsgoogle.css">
  <link rel="stylesheet" href="/css/bootstrap.min.css">
  <link rel="stylesheet" href="/css/font-awesome.min.css">
  <link rel="stylesheet" href="/css/academicons.min.css">

  <link rel="stylesheet" href="/css/hugo-academic.css">
  

  <link rel="alternate" href="https://chengjunwang.com/index.xml" type="application/rss+xml" title="">
  <link rel="feed" href="https://chengjunwang.com/index.xml" type="application/rss+xml" title="">

  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/apple-touch-icon.png">

  <link rel="canonical" href="https://chengjunwang.com/en/note/2017-8-15-bitbybit/">

  

  <title> Bit by Bit: Social Research in the Digital Age | </title>

</head>
<body id="top" data-spy="scroll" data-target="#navbar-main" data-offset="71">

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="/"></a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      <ul class="nav navbar-nav navbar-right">
        

        

        <li class="nav-item">
          <a href="/en/#about">
            
            <span>Home</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/en/#publications_selected">
            
            <span>Publications</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/en/#projects">
            
            <span>Projects</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/en/#posts">
            
            <span>News</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/en/#teaching">
            
            <span>Teaching</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/en/#contact">
            
            <span>Contact</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/zh">
            
            <span>中文</span>
          </a>
        </li>

        
        
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">



  


  <div class="article-container">
    <h1 itemprop="name"> Bit by Bit: Social Research in the Digital Age</h1>
    

<div class="article-metadata">

  <span class="article-date">
    <time datetime="2017-08-15 12:00:00 &#43;0000 UTC" itemprop="datePublished">
      Tue, Aug 15, 2017
    </time>
  </span>

  

  

  
  
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fchengjunwang.com%2fen%2fnote%2f2017-8-15-bitbybit%2f"
         target="_blank">
        <i class="fa fa-facebook"></i>
      </a>
    </li>
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=%20Bit%20by%20Bit%3a%20Social%20Research%20in%20the%20Digital%20Age&amp;url=https%3a%2f%2fchengjunwang.com%2fen%2fnote%2f2017-8-15-bitbybit%2f"
         target="_blank">
        <i class="fa fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fchengjunwang.com%2fen%2fnote%2f2017-8-15-bitbybit%2f&amp;title=%20Bit%20by%20Bit%3a%20Social%20Research%20in%20the%20Digital%20Age"
         target="_blank">
        <i class="fa fa-linkedin"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=https%3a%2f%2fchengjunwang.com%2fen%2fnote%2f2017-8-15-bitbybit%2f&amp;title=%20Bit%20by%20Bit%3a%20Social%20Research%20in%20the%20Digital%20Age"
         target="_blank">
        <i class="fa fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=%20Bit%20by%20Bit%3a%20Social%20Research%20in%20the%20Digital%20Age&amp;body=https%3a%2f%2fchengjunwang.com%2fen%2fnote%2f2017-8-15-bitbybit%2f">
        <i class="fa fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>


    
      <div id="toc" class="well col-md-12">
      <h2 itemprop="name">Table of Content</h2>
      <nav id="TableOfContents">
<ul>
<li><a href="#bit-by-bit-social-research-in-the-digital-age">Bit by Bit: Social Research in the Digital Age</a></li>
<li><a href="#preface">Preface</a></li>
<li><a href="#1-introduction">1 Introduction</a>
<ul>
<li><a href="#1-1-an-ink-blot">1.1 An ink blot</a></li>
<li><a href="#1-2-welcome-to-the-digital-age">1.2 Welcome to the digital age</a></li>
<li><a href="#1-3-research-design">1.3 Research design</a></li>
<li><a href="#1-4-themes-of-this-book">1.4 Themes of this book</a>
<ul>
<li><a href="#1-4-1-readymades-and-custommades">1.4.1 Readymades and Custommades</a></li>
<li><a href="#1-4-2-simplicity-over-complexity">1.4.2 Simplicity over complexity</a></li>
<li><a href="#1-4-3-ethics-everywhere">1.4.3 Ethics everywhere</a></li>
</ul></li>
<li><a href="#1-5-outline-of-the-book">1.5 Outline of the book</a></li>
<li><a href="#2-1-introduction">2.1 Introduction</a></li>
<li><a href="#2-2-big-data">2.2 Big data</a></li>
<li><a href="#2-3-common-characteristics-of-big-data">2.3 Common characteristics of big data</a>
<ul>
<li><a href="#2-3-1-characteristics-that-are-generally-good-for-research">2.3.1 Characteristics that are generally good for research</a>
<ul>
<li><a href="#2-3-1-1-big">2.3.1.1 Big</a></li>
<li><a href="#2-3-1-2-always-on">2.3.1.2 Always-on</a></li>
<li><a href="#2-3-1-3-non-reactive">2.3.1.3 Non-reactive</a></li>
</ul></li>
<li><a href="#2-3-2-characteristics-that-are-generally-bad-for-research">2.3.2 Characteristics that are generally bad for research</a>
<ul>
<li><a href="#2-3-2-1-incomplete">2.3.2.1 Incomplete</a></li>
<li><a href="#2-3-2-2-inaccessible">2.3.2.2 Inaccessible</a></li>
<li><a href="#2-3-2-3-non-representative">2.3.2.3 Non-representative</a></li>
<li><a href="#2-3-2-4-drifting">2.3.2.4 Drifting</a></li>
<li><a href="#2-3-2-5-algorithmically-confounded">2.3.2.5 Algorithmically confounded</a></li>
<li><a href="#2-3-2-6-dirty">2.3.2.6 Dirty</a></li>
<li><a href="#2-3-2-7-sensitive">2.3.2.7 Sensitive</a></li>
</ul></li>
</ul></li>
<li><a href="#2-4-research-strategies">2.4 Research strategies</a>
<ul>
<li>
<ul>
<li><a href="#2-4-1-counting-things">2.4.1 Counting things</a></li>
<li><a href="#2-4-1-1-taxis-in-new-york-city">2.4.1.1 Taxis in New York City</a></li>
<li><a href="#2-4-1-2-friendship-formation-among-students">2.4.1.2 Friendship formation among students</a></li>
<li><a href="#2-4-1-3-censorship-of-social-media-by-the-chinese-government">2.4.1.3 Censorship of social media by the Chinese government</a></li>
<li><a href="#2-4-2-forecasting-and-nowcasting">2.4.2 Forecasting and nowcasting</a></li>
</ul></li>
<li><a href="#2-4-3-approximating-experiments">2.4.3 Approximating experiments</a>
<ul>
<li><a href="#2-4-3-1-natural-experiments">2.4.3.1 Natural experiments</a></li>
<li><a href="#2-4-3-2-matching">2.4.3.2 Matching</a></li>
</ul></li>
</ul></li>
<li><a href="#2-5-conclusion">2.5 Conclusion</a></li>
<li><a href="#technical-appendix">Technical appendix</a></li>
<li><a href="#further-commentary">Further commentary</a></li>
<li><a href="#activities">Activities</a></li>
</ul></li>
<li><a href="#3-asking-questions">3 Asking questions</a>
<ul>
<li><a href="#3-1-introduction">3.1 Introduction</a></li>
<li><a href="#3-2-asking-vs-observing">3.2 Asking vs. observing</a>
<ul>
<li><a href="#3-3-the-total-survey-error-framework">3.3 The total survey error framework</a></li>
<li><a href="#3-3-1-representation">3.3.1 Representation</a></li>
<li><a href="#3-3-2-measurement">3.3.2 Measurement</a></li>
<li><a href="#3-3-3-cost">3.3.3 Cost</a></li>
</ul></li>
<li><a href="#3-4-who-to-ask">3.4 Who to ask</a>
<ul>
<li><a href="#3-4-1-probability-sampling-data-collection-and-data-analysis">3.4.1 Probability sampling: data collection and data analysis</a></li>
<li><a href="#3-4-2-non-probability-samples-weighting">3.4.2 Non-probability samples: weighting</a></li>
<li><a href="#3-4-3-non-probability-samples-sample-matching">3.4.3 Non-probability samples: sample matching</a></li>
</ul></li>
<li><a href="#3-5-new-ways-of-asking-questions">3.5 New ways of asking questions</a>
<ul>
<li><a href="#3-5-1-ecological-momentary-assessments">3.5.1 Ecological momentary assessments</a></li>
<li><a href="#3-5-2-wiki-surveys">3.5.2 Wiki surveys</a></li>
<li><a href="#3-5-3-gamification">3.5.3 Gamification</a></li>
</ul></li>
<li><a href="#3-6-surveys-linked-to-other-data">3.6 Surveys linked to other data</a>
<ul>
<li><a href="#3-6-1-amplified-asking">3.6.1 Amplified asking</a></li>
<li><a href="#3-6-2-enriched-asking">3.6.2 Enriched asking</a></li>
</ul></li>
<li><a href="#3-7-conclusion">3.7 Conclusion</a></li>
<li><a href="#technical-appendix-1">Technical appendix</a>
<ul>
<li><a href="#further-commentary-1">Further commentary</a></li>
</ul></li>
<li><a href="#activities-1">Activities</a></li>
</ul></li>
<li><a href="#4-running-experiments">4 Running experiments</a>
<ul>
<li><a href="#4-1-introduction">4.1 Introduction</a></li>
<li><a href="#4-2-what-are-experiments">4.2 What are experiments?</a></li>
<li><a href="#4-3-two-dimensions-of-experiments-lab-field-and-analog-digital">4.3 Two dimensions of experiments: lab-field and analog-digital</a></li>
<li><a href="#4-4-moving-beyond-simple-experiments">4.4 Moving beyond simple experiments</a>
<ul>
<li><a href="#4-4-1-validity">4.4.1 Validity</a></li>
<li><a href="#4-4-2-heterogeneity-of-treatment-effects">4.4.2 Heterogeneity of treatment effects</a></li>
<li><a href="#4-4-3-mechanisms">4.4.3 Mechanisms</a></li>
</ul></li>
<li><a href="#4-5-making-it-happen">4.5 Making it happen</a>
<ul>
<li><a href="#4-5-1-just-do-it-yourself">4.5.1 Just do it yourself</a>
<ul>
<li><a href="#4-5-1-1-use-existing-environments">4.5.1.1 Use existing environments</a></li>
</ul></li>
<li><a href="#4-5-1-2-build-your-own-experiment">4.5.1.2 Build your own experiment</a>
<ul>
<li><a href="#4-5-1-3-build-your-own-product">4.5.1.3 Build your own product</a></li>
</ul></li>
<li><a href="#4-5-2-partner-with-the-powerful">4.5.2 Partner with the powerful</a></li>
</ul></li>
<li><a href="#4-6-advice">4.6 Advice</a>
<ul>
<li><a href="#4-6-1-create-zero-variable-cost-data">4.6.1 Create zero variable cost data</a></li>
</ul></li>
</ul></li>
</ul>
</nav>
      </div>
    
    
    <div class="article-style" itemprop="articleBody">
      

<p>authors: Matthew J. Salganik</p>

<h1 id="bit-by-bit-social-research-in-the-digital-age">Bit by Bit: Social Research in the Digital Age</h1>

<p><a href="http://www.bitbybitbook.com/" target="_blank">http://www.bitbybitbook.com/</a></p>

<h1 id="preface">Preface</h1>

<p>For me, this book began in 2005, when I was working on my dissertation. I was running an online experiment, which I’ll tell you all about in Chapter 4, but now I’m going to tell you something that is not in any academic paper. And, it’s something that fundamentally changed how I think about research. One morning, when I checked the web-server, I discovered that overnight about 100 people from Brazil had participated in my experiment. This experience had a profound impact on me. At that time, I had friends who were running traditional lab experiments, and I knew how hard they had to work to recruit, supervise, and pay people to participate in their experiments; if they could run 10 people in a single day, that was good progress. But, with my online experiment, 100 people participated while I was sleeping. Doing your research while you are sleeping might sound too good to be true, but it isn’t. Changes in technology—specifically the transition from the analog age to the digital age—mean that we can now collect and analyze social data in new ways. This book is about doing social research in these new ways.</p>

<p>This book is for two different communities. It is for social scientists that want to do more data science, and it is for data scientists that want to do more social science. I spend time in both of these communities, and this book is my attempt to bring their ideas together in a way that avoids the quirks and jargon of either. Given the communities that this book is for, it should go without saying that this book is not just for students and professors. I’ve worked some in government (at the US Census Bureau) and in the tech industry (at Microsoft Research), and I know that there is lots of exciting research happening outside of universities. So, if you think of what you are doing as social research, then this book is for you, no matter where you work or what kind of techniques you currently use.</p>

<p>We are still in the early days of social research in the digital age, and I’ve seen some misunderstandings that are so fundamental and so common that it makes the most sense for me to address them here, in the preface. From data scientists, I’ve seen two common misunderstandings. The first is thinking that more data automatically solves problems. But, for social research that has not been my experience. In fact, for social research new types of data, as opposed to more of the same data, seems to be most helpful. The second misunderstanding that I’ve seen from data scientists is thinking that social science is just a bunch of fancy-talk wrapped around common sense. Of course, as a social scientist—more specifically as a sociologist—I don’t agree with that; I think that social science has a lot of to offer. Smart people have been working hard to understand human behavior for a long time, and it seems unwise to ignore the wisdom that has accumulated from this effort. My hope is that this book will offer you some of that wisdom in a way that is easy to understand.</p>

<p>From social scientists, I’ve also seen two common misunderstandings. First, I’ve seen some people write-off the entire idea of social research using the tools of the digital age based on a few bad papers. If you are reading this book, you have probably already read a bunch of papers that uses social media data in ways that are banal or wrong (or both). I have too. However, it would be a serious mistake to conclude from these examples that all digital age social research is bad. In fact, you’ve probably also read a bunch of papers that use survey data in ways that are banal or wrong, but you don’t write-off all research using surveys. That’s because you know that there is great research done with survey data, and in this book, I’m going to show you that there is also great research done with the tools of the digital age.</p>

<p>The second common misunderstanding that I’ve seen from social scientists is to confuse the present with the future. When assessing social research in the digital age—the research that I’m going to describe in this book—it is important to ask two distinction questions:</p>

<p>How well does this style of research work now?
How well will this style of research work in the future as the data landscape changes and as researchers devote more attention to these problems?
Even though researchers are trained to answer the first question, for this book, I think the second question is more important. That is, even though social research in the digital age has not yet produced massive, paradigm-changing intellectual contributions, the rate of improvement of digital age research is incredibly rapid. It is this rate of change, more than the current level, that makes digital age research so exciting to me.</p>

<p>Even though that last paragraph seemed to offer you potential riches at some unspecified time in the future, my goal in this book is not to sell you on any particular type of research. I don’t personally own shares in Twitter, Facebook, Google, Microsoft, Apple or any other tech company (although, for the sake of full disclosure, I have worked at or received research funding from Microsoft, Google, and Facebook). If you are happy with the research that you are already doing: great, keep doing what you are doing. But, if you have a sense that the digital age means that new and different things are possible, then I’d like to show you those possibilities. Thus, throughout the book my goal is to remain a credible narrator, telling you about all the exciting new stuff that is possible, while guiding you away from a few pitfalls that I’ve seen others fall into. I hope that this will help improve your research and help you better evaluate the research of others.</p>

<p>As you might have noticed already, the tone of this book is a bit different from some other academic books. That’s intentional. This book emerged from a graduate seminar that I have taught at Princeton in the Department of Sociology, and I’d like this book to capture some of the the energy and excitement from that seminar. In particular, I want this book to have three characteristics: helpful, optimistic, and future-oriented.</p>

<p>Helpful: My goal is to write a book that is helpful for you. Therefore, I’m going to write in an open and informal style. That’s because the most important thing that I want to convey is a certain way of thinking about social research. And, my experience from teaching suggests that the best way to convey this way of thinking is informally and with lots of examples.</p>

<p>Optimistic: The two communities that this book engages—social scientists and data scientists—have very different styles. Data scientists are generally excited; they tend to see the glass as half full. Social scientists, on the other hand, are generally more critical; they tend to see the glass as half empty. In this book, I’m going to adopt the optimistic tone of a data scientist, even though my training is as a social scientist. So, when I present examples, I’m going to tell you what I love about these examples. And, when I do point out problems with the examples—and I will do this because no research is perfect—I’m going to try to point out these problems in a way is positive and optimistic. I’m not going to be critical for the sake of being critical. I’m going to be critical so that I can help you create more beautiful research.</p>

<p>Future-oriented: I hope that this book will help you do social research using the digital systems that exist today and the digital systems that will be created in the future. I started doing this kind of research in 2003, and since then I’ve seen a lot of changes. I remember that when I was in graduate school people were very excited about using MySpace for social research. And, when I taught my first class on what I then called “web-based social research,” people were very excited about virtual worlds such as SecondLife. I’m sure that in the future much of what people are talking about today will seem silly and outdated. The trick to staying relevant in the face of this rapid change is abstraction. Therefore, this is not going to be a book that teaches you exactly how to use the Twitter API; instead, it is going to be a book that teaches you how to learn from digital traces (Chapter 2). This is not going to be a book that gives you step-by-step instructions for running experiments on Amazon Mechanical Turk; instead, it is going to teach you how to design and interpret experiments that rely on digital age infrastructure (Chapter 4). Through the use of abstraction, I hope this will be a timeless book on a timely topic.</p>

<p>I think this is the most exciting time ever to be a social researcher, and I’m going to try to convey that excitement in a way that is precise. That is, it is time to move beyond vague generalities about the magical powers of new data. It is time to get specific.</p>

<h1 id="1-introduction">1 Introduction</h1>

<h2 id="1-1-an-ink-blot">1.1 An ink blot</h2>

<p>In the summer of 2009, mobile phones were ringing all across Rwanda. In addition to the millions of calls between family, friends, and business associates, about 1,000 Rwandans received a call from Joshua Blumenstock and his colleagues. The researchers were studying wealth and poverty by conducting a survey of people who had been randomly sampled from a database of 1.5 million customers from Rwanda’s largest mobile phone provider. Blumenstock and colleagues asked the participants if they wanted to participate in a survey, explained the nature of the research to them, and then asked a series of questions about their demographic, social, and economic characteristics.</p>

<p>Everything I have said up until now makes this sound like a traditional social science survey. But, what comes next is not traditional, at least not yet. They used the survey data to train a machine learning model to predict someone’s wealth from their call data, and then they used this model to estimate the wealth of all 1.5 million customers. Next, they estimated the place of residence of all 1.5 million customers by using the geographic information embedded in the call logs. Putting these two estimates together—the estimated wealth and the estimated place of residence—Blumenstock and colleagues were able to produce high-resolution estimates of the geographic distribution of wealth across Rwanda. In particular, they could produce an estimated wealth for each of Rwanda’s 2,148 cells, the smallest administrative unit in the country.</p>

<p>It was impossible to validate these estimates because no one had ever produced estimates for such small geographic areas in Rwanda. But, when Blumenstock and colleagues aggregated their estimates to Rwanda’s 30 districts, they found that their estimates were similar to estimates from the Demographic and Health Survey, the gold standard of surveys in developing countries. Although these two approaches produced similar estimates in this case, the approach of Blumenstock and colleagues was about 10 times faster and 50 times cheaper than the traditional Demographic and Health Surveys. These dramatically faster and lower cost estimates create new possibilities for researchers, governments, and companies (Blumenstock, Cadamuro, and On 2015).</p>

<p>In addition to developing a new methodology, this study is kind of like a Rorschach inkblot test; what people see depends on their background. Many social scientists see a new measurement tool that can be used to test theories about economic development. Many data scientists see a cool new machine learning problem. Many business people see a powerful approach for unlocking value in the digital trace data that they have already collected. Many privacy advocates see a scary reminder that we live in a time of mass surveillance. Many policy makers see a way that new technology can help create a better world. In fact, this study is all of those things, and that is why it is a window into the future of social research.</p>

<h2 id="1-2-welcome-to-the-digital-age">1.2 Welcome to the digital age</h2>

<blockquote>
<p>The digital age is everywhere, it’s growing, and it changes what is possible for researchers.</p>
</blockquote>

<p>The central premise of this book is that the digital age creates new opportunities for social research. Researchers can now observe behavior, ask questions, run experiments, and collaborate in ways that were simply impossible in the quite recent past. Along with these new opportunities also come new risks; researchers can now harm people in ways that were impossible in the quite recent past. The source of these opportunities and risks is the transition from the analog age to the digital age. This transition did not happen all at once—like a light-switch turning on—and, in fact, the transition is not yet complete. But, by this point we’ve seen enough to know that something big is happening.</p>

<p>One way to notice this transition is to look for changes in your daily life. Many things in your life that used to be analog are now digital. Maybe you used to use a camera with film and now you use a digital camera (which is probably part of your digital phone). Maybe you used to read a physical newspaper and now you read an online newspaper. Maybe you used to pay for things with physical cash and now you pay with a credit card. In each case, the transition from analog to digital means that more information is now being captured and stored digitally.</p>

<p>In fact, when looked at in aggregate, the effects of the transition are astonishing. The amount of information in the world is rapidly increasing and more of that information is stored digitally, which facilitates analysis, transmission, and merging (Figure 1.1) (Hilbert and López 2011). All of this digital information has come to be called “big data.” In addition to this explosion of digital data, there is a parallel growth in our access to computing power (Figure 1.1) (Hilbert and López 2011). These trends—increasing digital information and increasing computing—show no sign of slowing down.</p>

<p><img src="http://oaf2qt3yk.bkt.clouddn.com/34696d2461b0e919e3d278b9260a278d.png" alt="fig1.1" /></p>

<p>Figure 1.1: Information storage capacity and computing power are increasing dramatically. Further, information storage is now almost exclusively digital (Hilbert and López 2011). These changes create incredible opportunities for social researchers.</p>

<p>For the purposes of social research, I think the most important feature of the digital age is computers everywhere. Beginning as room-sized machines that were only available to governments and big companies, computers have been constantly shrinking in size and increasing in popularity. Each decade since the 1980s, we’ve seen a new kind of computing emerge: personal computers, laptops, smart phones, and now embedded processors (i.e., computers inside of devices such as cars, watches, and thermostats) (Waldrop 2016). Increasingly these ubiquotous computers do more than just calculate; they also sense, store, and transmit information.</p>

<p>For researchers, the implications of computers everywhere are easiest to see online, an environment that is fully measured and amenable to experimentation. For example, an online store can easily collect incredibly precise data about the shopping and purchasing patterns of millions of customers. Further, an online store can easily randomize some customers to receive one shopping experience and others to receive another. This ability to randomize on top of tracking means that online stores can constantly run randomized controlled experiments. In fact, if you’ve ever bought anything from an online store your behavior has been tracked and you’ve almost certainly been a participant in an experiment, whether you knew it or not.</p>

<p>This fully-measured-fully-randomizable world is not just happening online; it is increasingly happening everywhere. Physical stores already collect extremely detailed purchase data, and they are developing infrastructure to monitor customers shopping behavior and mix experimentation into routine business practice. In other words, when you think about the digital age you should not just think online, you should think everywhere. Digital age social research will involve people interacting in fully digital spaces and will involve people using digital devices in the physical world.</p>

<p>In addition to enabling the measurement of behavior and randomization of treatments, the digital age has also enabled new ways for people to communicate. These new forms of communication allow researchers to run innovative surveys and to create mass collaboration with their colleagues and the general public.</p>

<p>A skeptic might point out that none of these capabilities are really new. That is, in the past, there have been other major advances in peoples’ abilities to communicate (e.g., the telegraph (Gleick 2011)), and computers have been getting faster at roughly the same rate since the 1960s (Waldrop 2016). But, what this skeptic is missing is that at a certain point more of the same becomes something different (Halevy, Norvig, and Pereira 2009). Here’s an analogy that I like. If you can capture an image of a horse, then you have a photograph. And, if you can capture 24 images of a horse per second, then you have a movie. Of course, a movie is just a bunch of photos, but only a die hard skeptic would claim that photos and movies are the same.</p>

<p>Researchers are in the process of making a transition akin to the transition from photography to cinematography. This transition does not mean that everything we have learned in the past should be ignored. Just as the principles of photography inform the principles of cinematography, the principles of social research in the past will inform the social research of the future. But, the transition also means that we should not continue doing the same thing. Rather, we must combine the approaches of the past with the capabilities of the present and future. For example, the research of Blumenstock and colleagues was a mixture of traditional survey research with what some might call data science. Both of those ingredients were necessary: neither the survey responses nor the phone records by themselves were enough. More generally, I think that increasingly social researchers will need to combine social science with data science in order to take advantage of the opportunities of the digital age. To continue only taking pictures when we could also be making movies would be a mistake.</p>

<h2 id="1-3-research-design">1.3 Research design</h2>

<blockquote>
<p>Research design is about connecting questions and answers.</p>
</blockquote>

<p>This book is written for two audiences that have a lot to learn from each other. On the one hand, this book is for social scientists who have training and experience studying social behavior, but who are less familiar with the opportunities created by the digital age. On the other hand, this book is for another group of researchers who are very comfortable using the tools of the digital age, but who are new to studying social behavior. This second group resists an easy name, but I will call them data scientists. These data scientists—who often have training in fields such as computer science, information science, engineering, and physics—have been some of the earliest adopters of digital age social research, in part because they had the necessary data access and computational skills. Data scientists, however, have less training and experience studying social behavior. This book brings these two communities together to produce something richer and more interesting than either community could produce individually.</p>

<p>The best way to create this powerful hybrid is not to focus on abstract social theory or fancy machine learning. The best place to start is research design. If you think of social research as the process of asking and answering questions about human behavior, then research design is the connective tissue; research design links questions and answers. Getting this connection right is the key to producing convincing research.</p>

<h2 id="1-4-themes-of-this-book">1.4 Themes of this book</h2>

<h3 id="1-4-1-readymades-and-custommades">1.4.1 Readymades and Custommades</h3>

<blockquote>
<p>Social research in the digital age will involve Readymades, Custommades, and powerful hybrids.</p>
</blockquote>

<p>One of the most famous urinals ever was purchased in 1917, and that urinal is, in some ways, similar to lots of social research in the digital age. The urinal in question was purchased by the French artist Marcel Duchamp. After purchasing it, Duchamp scribbled “R. Mutt 1917” on it and then named his creation Fountain (Figure 1.2). Although its initial reception for was lukewarm, Fountain has come to considered one of the most important pieces of modern art because it fundamentally changed how people think about art (Higgins 2004). Fountain is an example of a Readymade, where an artist sees something that already exists in the world then repurposes it as art.</p>

<p><img src="http://oaf2qt3yk.bkt.clouddn.com/9782262673cccc279d9be93631b75f7c.png" alt="fig1.2" /></p>

<p>Figure 1.2: Fountain by Marcel Duchamp. Fountain is an example of a Readymade, where an artist sees something that already exists in the world then creatively repurposes it for art. So far, a lot of social research in the digital age has involved repurposing data that created for some purpose other than research. Photo by Alfred Stiglitz, 1917. Source: Wikimedia Commons.</p>

<p>Much of social research in the digital age has, so far, had a similar structure, although not quite with the same result. Researchers have realized that digital records created by governments and businesses for their own purposes—such as phone logs, digitized texts, and social media data—can be repurposed for social research (Lazer 2015). In other words, a lot of social research in the digital age has been a search for Data Readymades.</p>

<p>However, just as most artists don’t walk around looking for Readymades, most social researchers in the past have not walked around looking for data that can be repurposed. Instead, rather than being data-driven, most successful social research in the past has been question-driven. That is, a researcher had a question and then found or created the data needed to answer that question. An artist who illustrates this other style of work is Michelangelo. He wanted to make a statue of David so he spent 3 years laboring with a block of marble to create his masterpiece (Figure 1.3). David is not a Readymade; it is a Custommade.</p>

<p><img src="http://oaf2qt3yk.bkt.clouddn.com/0fdbd65a64e560d12bc60f9e01101018.png" alt="fig1.3" /></p>

<p>Figure 1.3: David by Michaelangelo. David is an example of art that was intentionally created; it is a Custommade. This style contrasts with Readymades such as Fountain (Figure 1.2). Social research in the digital age will involve both Readymades and Custommades. Photo by Jörg Bittner Unna, 2008. Source: Wikimedia Commons.</p>

<p>Social research in the digital age will involve both Duchamps and Michelangelos, both Readymades and Custommades. This book will explore these two approaches, and, more importantly, it will show how they can be combined into a powerful hybrid. For example, Joshua Blumenstock and colleagues were part Duchamp and part Michelangelo; they repurposed the mobile phone call data (a Readymade) and they created their own survey data (a Custommade). This blending of Readymades and Custommades is a pattern that you’ll see throughout this book.</p>

<h3 id="1-4-2-simplicity-over-complexity">1.4.2 Simplicity over complexity</h3>

<blockquote>
<p>Complex research can never convince anyone of something surprising. If you care about changing minds, then your research should be simple.</p>
</blockquote>

<p>Social research in the digital age is often associated with complexity, such as fancy algorithms and sophisticated computing. This is unfortunate because the most convincing social research is often the simplest. To be clear, simple research is not the same as easy research. In fact, it is often much harder to create simple research.</p>

<p>The most important reason to prefer simple research is that it is the only way to create believable, unexpected results. For example, imagine that you have just conducted some research using an incredibly complex methodology. If your results match your expectation, then you will probably accept them. But, if your results are different from what you expected, you have two options: accept the unexpected results or doubt the complex methodology. My guess is that you are much more likely to doubt the complex methodology. This makes perfect sense, but it means that the more complex the methodology, the less likely it is to produce unexpected results that you will actually believe. At some point, methods can become so complex that the only results that you can believe are those that match you expectations. At that point, the research has lost something very important: research should be able to change your mind.</p>

<p>The problem that I’ve just described is ever more severe once you start trying to change someone else’s mind. Imagine presenting an incredibly complex piece of research that has an unexpected result to someone else. That other person has not spent months writing your code and working through your data so when they are faced with the choice of accepting the unexpected result or doubting the complex methodology, they are almost certainly going to doubt the complex methodology. If you care about convincing someone else to change their mind, then your research needs to be simple.</p>

<p>Simple research comes from a natural fit between question and data; in other words, good research design. Poor research design, however, leads to the ugly complexity that come from stretching your data to a question for which they are not well suited. This book focuses on two approaches to create a natural fit between question and data. First, this book will help you ask realistic questions of your data. Second, this book will help you collect the right data to answer your question.</p>

<h3 id="1-4-3-ethics-everywhere">1.4.3 Ethics everywhere</h3>

<blockquote>
<p>In the future, researchers will struggle less with what can be done and more with what should be done.</p>
</blockquote>

<p>In the past, cost has been a dominant constraint on what researchers do. But, as you will see throughout this book, the cost of certain forms of research is plummeting. At essentially no cost, researchers can now secretly observe the behavior of millions of people, and can conduct massive experiments without the consent or even awareness of participants. In the future, therefore, researchers will struggle less with what can be done and more with what should be done. Chapter 6 will be entirely devoted to ethics, but I also integrate ethics into the other chapters as well. In the digital age, ethics will become an increasingly important consideration when researchers balance the trade-offs between various research approaches.</p>

<h2 id="1-5-outline-of-the-book">1.5 Outline of the book</h2>

<p>This book is organized around a progression through four broad research approaches: observing behavior, asking questions, running experiments, and creating mass collaboration. These four approaches were all used in some form 50 years ago, and I’m confident that they will all be used in some form 50 years from now. I’ve dedicated one chapter to each approach. Many of the chapters have a section devoted to further commentary, a technical or historical appendix, and activities that could be used in a class or for self-study. Because of these features, I’m going to keep the main text as simple as possible; you can refer to these other parts of the chapters if you would like more details and citations into the literature.</p>

<p>In chapter 2 (Observing behavior), I will describe what and how researchers can learn from observing people’s behavior. In particular, I’ll focus on digital trace data and administrative data where the researcher had no role in the creation of the data. I’ll describe common features of this kind of data, and I’ll explain some research strategies that can be used to successfully learn from observed behavior.</p>

<p>In chapter 3 (Asking questions), I will begin by showing what researchers can learn by moving beyond observing behavior and start interacting with people. In particular, I will argue that there is great value in doing survey research, even in a world awash with already existing digital data. I will review the traditional total survey error framework and use it to organize the developments that the digital age enables for survey research. In particular, I will show how the digital age can lead to big changes in sampling and interviewing. Finally, I’ll describe two strategies for combining survey data with digital trace data. Despite the pessimism that some survey researchers currently feel, I expect that the digital age will be the golden age of survey research.</p>

<p>In chapter 4 (Running experiments), I will begin by showing what researchers can learn when they move beyond observing behavior and asking survey questions. In particular, I will show how randomized controlled experiments—where the researcher intervenes in the world in a very specific way—enable researchers to learn about causal relationships. I will compare the kinds of experiments that we could do in the past with the kinds that we can do now. With that background, I’ll describe the trade-offs involved in the two main strategies for conducting digital experiments. Finally, I’ll conclude with some design advice about how you can take advantage of the real power of digital experiments and describe some of responsibility that comes with that power.</p>

<p>In chapter 5 (Creating mass collaboration), I will show how researchers can create mass collaborations—such as crowdsourcing and citizen science—in order to do social research. By describing successful mass collaboration projects and by providing a few key organizing principles, I hope to convince you of two things: first, that mass collaboration can be harnessed for social research, and second, that researchers who use mass collaboration will be able to solve problems that had previously seemed impossible. Although mass collaboration is often promoted as a way to save money, it is much more than that. Mass collaboration doesn’t just allow us to do research cheaper; it allows us to do research better.</p>

<p>In chapter 6 (Ethics), I will argue that researchers have rapidly increasing power over participants, and that these capabilities are changing faster than our norms, rules, and laws. This combination—increasing power and lack of agreement about how that power should be used—leaves well-meaning researchers in a difficult situation. To address this problem, I will argue that researchers should adopt a principles-based approach. That is, researchers should evaluate their research through existing rules—which I will take as given—and through more general ethical principles. I’ll propose four established principles and two ethical frameworks that can help guide your decisions. Finally, I’ll describe and analyze some specific ethical challenges that I expect will confront researchers in the future, and I’ll offer practical tips for working in an area with unsettled ethics.</p>

<p>Finally, in chapter 7 (The future), I will summarize three themes that recur across chapters and that will be particularly important in the future.</p>

<p>Social research in the digital age will combine what we have done in the past with the very different capabilities of the future. Thus, social research will be shaped by both social scientists and data scientists. Each group has something to contribute, and each group has something to learn.</p>

<h2 id="2-1-introduction">2.1 Introduction</h2>

<p>In the analog age, collecting data about behavior—who does what when—was expensive, and therefore, relatively rare. Now, in the digital age, the behaviors of billions of people are recorded, stored, and analyzable. For example, every time you click on a website, make a call on your cell phone, or pay for something with your credit card, a digital record of your behavior is created and stored by a business. Because these data are a by-product of people’s every day actions, they are often called digital traces. In addition to these traces held by businesses, governments also have incredibly rich data about both people and businesses, data which is often digitized and analyzable. Together these business and government records are often called big data.</p>

<p>The ever-rising flood of big data means that we have moved from a world where behavioral data was scarce to a world where behavioral data is plentiful. But, because these types data are relatively new, an unfortunate amount of research using them looks like scientists blindly chasing available data. This chapter, instead, offers a principled approach to understanding the different sources of data and how they can be used. This richer understanding should help you better match your research questions to appropriate sources of data. Or, if such existing sources are lacking, convince you to collect your own data using the ideas in future chapters.</p>

<p>A first step to learning from big data is to realize that it is part of a broader category of data that has been used for social research for many years: observational data. Roughly, observational data is any data that results from observing a social system without intervening in some way. A crude way to think about it is that observational data is everything that does not involve talking with people (e.g., surveys, the topic of Chapter 3) or changing people’s environments (e.g., experiments, the topic of Chapter 4). Thus, in addition to business and government records, observational data also includes things like the text of newspaper articles and satellite photos.</p>

<p>This chapter has three parts. First, in Section 2.2, I describe big data in more detail and clarify a fundamental difference between it and the data that have generally been used for social research in the past. Then, in Section 2.3, I describe ten common characteristics of big data sources. Understanding these characteristics enables us to quickly recognize the strengths and weaknesses of existing sources and will help us harness the new sources that will be created in the future. Finally, in Section 2.4, I describe three main research strategies that you can use to learn from observational data: counting things, forecasting things, and approximating an experiment.</p>

<h2 id="2-2-big-data">2.2 Big data</h2>

<blockquote>
<p>Big data are created and collected by governments for purposes other than research. Using this data for research, therefore, requires repurposing.</p>
</blockquote>

<p>An idealized view of social research imagines a scientist having an idea and then collecting data to test that idea. This style of research leads to a tight fit between research question and data, but it is limited because an individual researcher often do not have the resources needed to collect the data they need, such as large, rich, and nationally-representative data. Therefore, a lot of social research in the past has used large-scale social surveys, such as the General Social Survey (GSS), the American National Election Study (ANES), and Panel Study of Income Dynamics (PSID). These large-scale survey are generally run by a team of researchers and they are designed to create data that can be used by many researchers. Because of the goals of these large-scale surveys, great care is put into designing the data collection and preparing the resulting data for use by researchers. These data are by researchers and for researchers.</p>

<p>Most social research using digital age sources, however, is fundamentally different. Instead of using data collected by researchers and for researchers, it uses data sources that were created and collected by businesses and governments for their own purposes such as making a profit, providing a service, or administering a law. These business and government data sources have come to be called big data. Doing research with big data is different than doing research with data that was originally created for research. Compare, for example, a social media website, such as Twitter, with a traditional public opinion survey such as the General Social Survey (GSS). Twitter’s main goals are to provide a service to its users and to make a profit. In the process of achieving these goals, Twitter creates data that might be useful for studying certain aspects of public opinion. But, unlike the General Social Survey (GSS), Twitter is not primarily focused on social research.</p>

<p>The term big data is frustratingly vague, and it groups together many different things. For the purposes of social research, I think it is helpful to distinguish between two kinds of big data sources: government administrative records and business administrative records. Government administrative records are data that are created by governments as part of their routine activities. These kinds of records have been used by researchers in the past—such as demographers studying birth, marriage, and death records—but governments are increasingly collecting and releasing detailed records in analyzable forms. For example, the New York City government installed digital meters inside of every taxi in the city. These meters record all kinds of data about each taxi ride including the driver, the start time and location, the stop time and location, and the fare. In a study that I’ll tell later in this chapter, Henry Farber (2015) repurposed these data to address a fundamental debate in labor economics about the relationship between hourly wages and the number of hours worked.</p>

<p>The second main type of big data for social research is business administrative records. These are data that business create and collect as part of their routine activities. These business administrative records are often called digital traces, and include things like search engine query logs, social media posts, and call records from mobile phones. Critically, these business administrative records are not just about online behavior. For example, stores that use check-out scanners are creating real-time measures of worker productivity. In a study that I’ll tell you about later in this chapter, Alexandre Mas and Enrico Moretti (2009) repurposed this supermarket check-out data to study how a workers’ productivity is impacted by the productivity of their peers.</p>

<p>As both of these examples illustrate, the idea of repurposing is fundamental to learning from big data. In my experience, social scientists and data scientists approach to this repurposing very differently. Social scientists, who are accustomed to working with data designed for research, are quick to point out the problems with repurposed data while ignoring its strengths. On the other hand, data scientists are quick to point out the benefits of repurposed data while ignoring its weaknesses. Naturally, the best approach would be a hybrid. That is, researchers need to understand the characteristics of these new sources of data—both good and bad—and then figure out how to learn from them. And, that is the plan for the remainder of this chapter. Next, I will describe ten common characteristics of business and government administrative data</p>

<h2 id="2-3-common-characteristics-of-big-data">2.3 Common characteristics of big data</h2>

<blockquote>
<p>Big data sources tend to have ten characteristics; some are good for social research and some are bad.</p>
</blockquote>

<p>If researchers are going to learn from big data that they did not create or collect, then they must understand its general characteristics. Rather than taking a platform by platform approach (e.g., here’s what you need to know about Twitter, here’s what you need to know about Google search data, etc), I’m going to describe ten general characteristics of big data, characteristics that arise because the data was not created for the purpose of social research. By stepping back from the details of each particular system and looking at these general properties, researchers can quickly learn more about existing data sources and have a firm set of ideas to apply to future data sources.</p>

<p>I find it helpful to group the characteristics into two categories:</p>

<ul>
<li>generally good for research: big, always-on, non-reactive</li>
<li>generally bad for research: incomplete, inaccessible, non-representative, drifting, algorithmically confounded, inaccessible, dirty, and sensitive</li>
</ul>

<p>Broadly speaking, government administrative records are less non-representative, less algorithmically confounded, and less drifting. On the other hand, business administrative records tend to be larger and more always-on.</p>

<h3 id="2-3-1-characteristics-that-are-generally-good-for-research">2.3.1 Characteristics that are generally good for research</h3>

<h4 id="2-3-1-1-big">2.3.1.1 Big</h4>

<blockquote>
<p>Large datasets are a means to an end; they are not an end in themselves.</p>
</blockquote>

<p>The first of the three good characteristics of big data is the most discussed: these are big data. These data sources can be big in three different ways: many people, lots of information per person, or many observations over time. Having a big dataset enables some specific types of research—measuring heterogeneity, studying rare events, detecting small differences, and making causal estimates from observational data. It also seems to lead to a specific type of sloppiness.</p>

<p>The first thing for which size is particularly useful is moving beyond averages to make estimates for specific subgroups. For example, Gary King, Jennifer Pan, and Molly Roberts (2013) measured the probability that social media posts in China would be censored by the government. By itself this average probability of deletion is not very helpful for understanding why the government censors some posts but not others. But, because their dataset included 11 million posts, King and colleagues also produced estimates for the probability of censorship for posts on 85 separate categories (e.g., pornography, Tibet, and Traffic in Beijing). By comparing the probability of censorship for posts in different categories, they were able to understand more about how and why the government censors certain types of posts. With 11 thousand posts (rather than 11 million posts), they would not have been able to produce these category-specific estimates.</p>

<p>Second, size is particularly useful for is studying of rare events. For example, Goel and colleagues (2015) wanted to study the different ways that tweets can go viral. Because large cascades of re-tweets are extremely rare—about one in a 3,000—they needed to study more than a billion tweets in order to find enough large cascades for their analysis.</p>

<p>Third, large datasets enable researchers to detect small differences. In fact, much of the focus on big data in industry is about these small differences: reliably detecting the difference between 1% and 1.1% click-through rates on an ad can translate into millions of dollars in extra revenue. In some scientific settings, such small differences might not be particular important (even if they are statistically significant). But, in some policy settings, such small differences can become important when viewed in aggregate. For example, if there are two public health interventions and one is slightly more effective than the other, then switching to the more effective intervention could end up saving thousands of additional lives.</p>

<p>Finally, large data sets greatly increase our ability to make causal estimates from observational data. Although large datasets don’t fundamentally change the problems with making causal inference from observational data, matching and natural experiments—two techniques that researchers have developed for making causal claims from observational data—both greatly benefit from large datasets. I’ll explain and illustrate this claim in greater detail later in this chapter when I describe research strategies.</p>

<p>Although bigness is generally a good property when used correctly, I’ve noticed that bigness commonly leads to a conceptual error. For some reason, bigness seems to lead researchers to ignore how their data was generated. While bigness does reduce the need to worry about random error, it actually increases the need to worry about systematic errors, the kinds of errors that I’ll describe in more below that arise from biases in how data are created and collected. In a small dataset, both random error and systematic error can be important, but in a large dataset random error is can be averaged away and systematic error dominates. Researchers who don’t think about systematic error will end up using their large datasets to get a precise estimate of the wrong thing; they will be precisely inaccurate (McFarland and McFarland 2015).</p>

<h4 id="2-3-1-2-always-on">2.3.1.2 Always-on</h4>

<blockquote>
<p>Always-on big data enables the study of unexpected events and real-time measurement.</p>
</blockquote>

<p>Many big data systems are always-on; they are constantly collecting data. This always-on characteristic provides researchers with longitudinal data (i.e., data over time). Being always-on has two important implications for research.</p>

<p>First, always-on data collection enables researchers to study unexpected events in ways that were not possible previously. For example, researchers interested in studying the Occupy Gezi protests in Turkey in the summer of 2013 would typically focus on the behavior of protesters during the event. Ceren Budak and Duncan Watts (2015) were able to do more by using the always-on nature of Twitter to study Twitter-using protesters before, during, and after the event. And, they were able to create a comparison group of non-participants (or participants who did not tweet about the protest) before, during, and after the event (Figure 2.1). In total their ex-post panel included the tweets of 30,000 people over two years. By augmenting the commonly used data from the protests with this other information, Budak and Watts were able to learn much more: they were able to estimate what kinds of people were more likely to participate in the Gezi protests and to estimate the changes in attitudes of participants and non-participants, both in the short-term (comparing pre-Gezi to during Gezi) and in the long-term (comparing pre-Gezi to post-Gezi).</p>

<p>Figure 2.1: Design used by Budak and Watts (2015) to study the Occupy Gezi protests in Turkey in the summer of 2013. By using the always-on nature of Twitter, the researchers created what they called an ex-post panel that included about 30,000 people over two years. In contrast the typical study that focused on participants during the protests, the ex-post panel adds 1) data from participants before and after the event and 2) data from non-participants before, during, and after the event. This enriched data structure enabled Budak and Watts to estimate what kinds of people were more likely to participate in the Gezi protests and to estimate the changes in attitudes of participants and non-participants, both in the short-term (comparing pre-Gezi to during Gezi) and in the long-term (comparing pre-Gezi to post-Gezi).
Figure 2.1: Design used by Budak and Watts (2015) to study the Occupy Gezi protests in Turkey in the summer of 2013. By using the always-on nature of Twitter, the researchers created what they called an ex-post panel that included about 30,000 people over two years. In contrast the typical study that focused on participants during the protests, the ex-post panel adds 1) data from participants before and after the event and 2) data from non-participants before, during, and after the event. This enriched data structure enabled Budak and Watts to estimate what kinds of people were more likely to participate in the Gezi protests and to estimate the changes in attitudes of participants and non-participants, both in the short-term (comparing pre-Gezi to during Gezi) and in the long-term (comparing pre-Gezi to post-Gezi).</p>

<p>It is true that some of these estimates could have been made without always-on data collection sources (e.g., long-term estimates of attitude change), although such data collection for 30,000 people would have been quite expensive. And, even given an unlimited budget, I can’t think of any other method that essentially allows researchers to travel back in time and directly observe participants behavior in the past. The closest alternative would be to collect retrospective reports of behavior, but these reports would be of limited granularity and questionable accuracy. Table 2.1 provides other examples of studies that use an always-on data source to study an unexpected event.</p>

<p>Table 2.1: Studies of unexpected events using always-on big data sources.</p>

<table>
<thead>
<tr>
<th>Unexpected event</th>
<th align="center">Always-on data source</th>
<th align="right">Citation</th>
</tr>
</thead>

<tbody>
<tr>
<td>Occupy Gezi movement in Turkey</td>
<td align="center">Twitter</td>
<td align="right">Budak and Watts (2015)</td>
</tr>

<tr>
<td>Umbrella protests in Hong Kong</td>
<td align="center">Weibo</td>
<td align="right">Zhang (2016)</td>
</tr>

<tr>
<td>Shootings of police in New York City</td>
<td align="center">Stop-and-frisk</td>
<td align="right">reports Legewie (2016)</td>
</tr>

<tr>
<td>Person joining ISIS</td>
<td align="center">Twitter</td>
<td align="right">Magdy, Darwish, and Weber (2016)</td>
</tr>

<tr>
<td>September 11, 2001 attack</td>
<td align="center">livejournal.com</td>
<td align="right">Cohn, Mehl, and Pennebaker (2004)</td>
</tr>

<tr>
<td>September 11, 2001 attack</td>
<td align="center">pager messages</td>
<td align="right">Back, Küfner, and Egloff (2010), Pury (2011), Back, Küfner, and Egloff (2011)</td>
</tr>
</tbody>
</table>

<p>Second, always-on data collection enables researchers to produce real-time measurements, which can be important in settings where policy makers want to not just learn from existing behavior but also respond to it. For example, social media data can be used to guide responses to natural disasters (Castillo 2016).</p>

<p>In conclusion, always-on data systems enable researchers to study unexpected events and provide real-time information to policy makers. I did not, however, propose that that always-on data systems enable researchers to track changes over long periods of time. That is because many big data systems are constantly changing—a process called drift (Section 2.3.2.4).</p>

<h4 id="2-3-1-3-non-reactive">2.3.1.3 Non-reactive</h4>

<blockquote>
<p>Measurement is much less likely to change behavior in big data sources.</p>
</blockquote>

<p>One challenge of social research is that people can change their behavior when they know that they are being observed by researchers. Social scientists generally call this behavior change in response to researcher measurement reactivity (Webb et al. 1966). One aspect of big data that many researcher find promising is that participants are generally not aware that their data are being captured or they have become so accustomed to this data collection that it no longer changes their behavior. Because they are non-reactive, therefore, many sources of big data can be used to study behavior that has not been amendable to accurate measurement previously. For example, Stephens-Davidowitz (2014) used the prevalence of racist terms in search engine queries to measure racial animus in different regions of the United States. The non-reactive and big (see previous section) nature of search data enabled measurements that would be difficult using other methods, such as surveys.</p>

<p>Non-reactivity, however, does not ensure that these data are somehow a direct reflect of people’s behavior or attitudes. For example, as one respondent told Newman et al. (2011), “It’s not that I don’t have problems, I’m just not putting them on Facebook.” In other words, even though some big data sources are non-reactive, they are not always free of social desirability bias, the tendency for people to want to present themselves in the best possible way. Further, as I’ll describe more below, these data sources are sometimes impacted by the goals of platform owners, a problem called algorithmic confounding (described more below).</p>

<p>Although non-reactivity is advantageous for research, tracking people’s behavior without their consent and awareness raises ethical concerns discussed below and in detail in Chapter 6. A public backlash against increased digital surveillance could lead big data systems to become more reactive over time, and strong concern about digital surveillance could even lead some people to attempt to opt-out of big data systems completely, increasing concerns about non-representativity (described more below).</p>

<p>These three good properties of big data for social research—big, always-on, and non-reactive—generally arise because these data sources were not created by researchers for research. Now, I’ll turn to the seven properties of big data sources that are bad for research. These features also tend to arise because this data was not created by researchers for research.</p>

<h3 id="2-3-2-characteristics-that-are-generally-bad-for-research">2.3.2 Characteristics that are generally bad for research</h3>

<h4 id="2-3-2-1-incomplete">2.3.2.1 Incomplete</h4>

<blockquote>
<p>No matter how “big” your “big data” it probably doesn’t have the information you want.</p>
</blockquote>

<p>Most big data sources are incomplete, in the sense that they don’t have the information that you will want for your research. This is a common feature of data that were created for purposes other than research. Many social scientists have already had the experience of dealing with the incompleteness, such as an existing survey that didn’t ask the question you wanted. Unfortunately, the problems of incompleteness tend to be more extreme in big data. In my experience, big data tends to be missing three types of information useful for social research: demographics, behavior on other platforms, and data to operationalize theoretical constructs.</p>

<p>All three of these forms of incompleteness are illustrated in a study by Gueorgi Kossinets and Duncan Watts (2006) about the evolution of the social network at a university. Kossinets and Watts started with the email logs from the university, which had precise information about who sent emails to whom at what time (the researchers did not have access to the content of the emails). These email records sound like an amazing dataset, but, they are—despite their size and granularity—fundamentally incomplete. For example, the email logs do not include data about the demographic characteristics of the students, such as gender and age. Further, the email logs do not include information about communication through other media, such as phone calls, text message, or face-to-face conversations. Finally, the email logs do not directly include information about relationships, the theoretical constructs in many existing theories. Later in the chapter, when I talk about research strategies, you’ll see how Kossinets and Watts solved these problems.</p>

<p>Of three kinds of incompleteness, the problem of incomplete data to operationalize theoretical constructs is the hardest to solve, and in my experience, it is often accidentally overlooked by data scientists. Roughly, theoretical constructs are abstract ideas that social scientists study, but, unfortunately, these constructs cannot always be unambiguously defined and measured. For example, let’s imagine trying to empirically test the apparently simple claim that people who are more intelligent earn more money. In order to test this claim you would need to measure “intelligence.” But, what is intelligence? For example, Gardner (2011) argued that there are actually eight different forms of intelligence. And, are there procedures that could accurately measure any of these forms of intelligence? Despite enormous amounts of work by psychologists, these questions still don’t have unambiguous answers. Thus, even a relatively simple claim—people who are more intelligent earn more money—can be hard to assess empirically because it can be hard to operationalize theoretical constructs in data. Other examples of theoretical constructs that are important but hard to operationalize include “norms,” “social capital,” and “democracy.” Social scientists call the match between theoretical constructs and data construct validity (Cronbach and Meehl 1955). And, as this list of constructs suggests, construct validity is a problem that social scientists have struggled with for a very long time, even when they were working with data that was collected for the purpose of research. When working with data collected for purposes other than research, the problems of construct validity are even more challenging (Lazer 2015).</p>

<p>When you are reading a research paper, one quick and useful way to assess concerns about construct validity is to take the main claim in the paper, which is usually expressed in terms of constructs, and re-express it in terms of the data used. For example, consider two hypothetical studies that claim to show that more intelligent people earn more money:</p>

<ul>
<li>Study 1: people who score well on the Raven Progressive Matrices Test—a well studied test of analytic intelligence (Carpenter, Just, and Shell 1990)—have higher reported incomes on their tax returns</li>
<li>Study 2: people on Twitter who used longer words are more likely to mention luxury brands</li>
</ul>

<p>In both cases, researchers could assert that they have shown that more intelligent people earn more money. But, in the first study the theoretical constructs are well operationalized by the data, and in the second they are not. Further, as this example illustrates, more data does not automatically solve problems with construct validity. You should doubt the results of Study 2 whether it involved a million tweets, a billion tweets, or a trillion tweets. For researchers not familiar with the idea of construct validity, Table 2.2 provides some examples of studies that have operationalized theoretical constructs using digital trace data.</p>

<p>Table 2.2: Examples of digital traces that are used as measures of more abstract theoretical concepts. Social scientists call this match construct validity and it is a major challenge with using big data sources for social research (Lazer 2015).</p>

<table>
<thead>
<tr>
<th>Digital trace</th>
<th align="center">Theoretical construct</th>
<th align="right">Citation</th>
</tr>
</thead>

<tbody>
<tr>
<td>email logs from a university (meta-data only)</td>
<td align="center">Social relationships</td>
<td align="right">Kossinets and Watts (2006), Kossinets and Watts (2009), De Choudhury et al. (2010)</td>
</tr>

<tr>
<td>social media posts on Weibo</td>
<td align="center">Civic engagement</td>
<td align="right">Zhang (2016)</td>
</tr>

<tr>
<td>email logs from a firm (meta-data and complete text)</td>
<td align="center">Cultural fit in an organization</td>
<td align="right">Goldberg et al. (2015)</td>
</tr>
</tbody>
</table>

<p>Although the problem of incomplete data for operationalizing theoretical constructs is pretty hard to solve, there are three common solutions to the problem of incomplete demographic information and incomplete information on behavior on other platforms. The first is to actually collect the data you need; I’ll tell you about an example of that in Chapter 3 when I tell you about surveys. Unfortunately, this kind of data collection is not always possible. The second main solution is to do what data scientists call user-attribute inference and what social scientists call imputation. In this approach, researchers use the information that they have on some people to infer attributes of other people. The third possible solution—the one used by Kossinets and Watts—was to combine multiple data sources. This process is sometimes called merging or record linkage. My favorite metaphor for this process was proposed in the very first paragraph of the very first paper ever written on record linkage (Dunn 1946):</p>

<blockquote>
<p>“Each person in the world creates a Book of Life. This Book starts with birth and ends with death. Its pages are made up of records of the principle events in life. Record linkage is the name given to the process of assembling the pages of this book into a volume.”</p>
</blockquote>

<p>This passage was written in 1946, and at that time, people were thinking that the Book of Life could include major life events like birth, marriage, divorce, and death. However, now that so much information about people is recorded, the Book of Life could be an incredibly detailed portrait, if those different pages (i.e., our digital traces), can be bound together. This Book of Life could be a great resource for researchers. But, the Book of Life could also be called a database of ruin (Ohm 2010), which could be used for all kinds of unethical purposes, as described more below when I talk about the sensitive nature of the information collected by big data sources below and in Chapter 6 (Ethics).</p>

<h4 id="2-3-2-2-inaccessible">2.3.2.2 Inaccessible</h4>

<blockquote>
<p>Data held by businesses and governments are difficult for researchers to access.</p>
</blockquote>

<p>In May 2014, the US National Security Agenda opened a data center in rural Utah that has an awkward name, the Intelligence Community Comprehensive National Cybersecurity Initiative Data Center. However, this data center, which has come to be known as the Utah Data Center, is reported to have astounding capabilities. One report alleges that the Utah Data Center is able to store and process all forms of communication including “the complete contents of private emails, cell phone calls, and Google searches, as well as all sorts of personal data trails—parking receipts, travel itineraries, bookstore purchases, and other digital `pocket litter’” (Bamford 2012). In addition to the raising concerns about the sensitive nature of much of the information captured in big data, which will be described more below, the Utah Data Center is an extreme example of a rich data source that is inaccessible to researchers. More generally, many sources of big data that would be useful to researchers are controlled and restricted by governments (e.g., tax data and educational data) and companies (e.g., queries to search engines and phone call meta-data). Therefore, these data will not be immediately available to researchers at universities, and most will not even be available to researchers in the governments or companies.</p>

<p>In my experience, many researchers based at universities misunderstand the source of this inaccessibility. These data are not inaccessible because people at companies and governments are stupid, lazy, or uncaring. Rather, there are serious legal, technical, business, and ethical barriers that prevent data access. For example, some terms-of-service agreements for websites only allow data to be used by employees or to improve the service. So certain forms of data sharing could expose companies to legitimate lawsuits from customers. There are also substantial business risks to companies involved in sharing data. Try to imagine how the public would respond if personal search data accidentally leaked out from Google as part of a university research project. Such a data breach, if extreme, might even be an existential risk for the company. So Google—and most large companies—are very risk-averse about sharing data with researchers.</p>

<p>In fact, almost everyone who is in a position to provide access to large amounts of data knows the story of Abdur Chowdhury. In 2006, when he was the head of AOL research, he intentionally released what he thought were anonymized search queries from 650,000 AOL users to the research community. As far as I can tell, Chowdhury and the researchers at AOL had good intentions and they thought that they had anonymized the data. But, they were wrong. It was quickly discovered that the data were not as anonymous as the researchers thought, and reporters from the New York Times were able to identify people in the dataset with ease (Barbaro and Zeller Jr 2006). Once these problems were discovered, Chowdhury removed the data from AOL’s website, but it was too late. The data had been reposted on other websites, and it will probably still be available when you are reading this book. Because of his attempt to share data with the research community, Chowdhury was fired, and AOL’s chief technology officer resigned (Hafner 2006). As this example shows, the benefits for specific individuals inside of companies to facilitate data access are pretty small and the worst-case scenario is terrible.</p>

<p>Research can, however, gain access to data that is inaccessible to the general public. Governments have procedures that researchers can follow to apply for access, and as the examples later in this chapter show, researchers can occasionally gain access to corporate data. For example, Einav et al. (2015) partnered with a researcher at eBay to study the digital traces from online auctions. I’ll talk more about the research that came from this collaboration later in the chapter (Section 2.4.3.2), but I mention it now because it had all four of the ingredients that I see in successful partnerships: researcher interest, researcher capability, company interest, and company capability. In other words, Einav and colleagues were interested in and capable of studying online auctions. And, eBay was also. However, I’ve seen many possible collaboration fail because either the researcher or company lacked one of these ingredients.</p>

<p>Even if you are able to develop a partnership with a business, however, there are some downsides for you. First, the questions that you can ask with the data with likely be limited; companies are unlikely to allow research that could make them look bad. Second, you will probably not be able to share your data with other researchers, which means that other researchers will not be able to verify and extend your results. Further, these partnerships can create at least the appearance of a conflict of interest, where people might think that your results were influenced by your partnerships. All of these downsides can be addressed, but it is important to be clear that working with data that is not accessible to everyone had both upsides and downsides.</p>

<p>In summary, lots of big data is inaccessible to researchers. There are serious legal, technical, business, and ethical barriers that prevent data access, and these barriers will not go away. National governments generally have established procedures for enabling data access, but the process can be more ad hoc at the state and local levels. Also, in some cases, researchers can partner with companies to obtain data access, but this can create a variety of problems for researchers.</p>

<h4 id="2-3-2-3-non-representative">2.3.2.3 Non-representative</h4>

<blockquote>
<p>Two sources of non-representativeness are different populations and different usage patterns.</p>
</blockquote>

<p>Big data tend to be systematically biased in two main ways. This need not cause a problem for all kind of analysis, but for some analysis it can be a critical flaw.</p>

<p>A first source of systematic bias is that the people captured are typically neither a complete universe of all people or a random sample from any specific population. For example, Americans on Twitter are not a random sample of Americans (Hargittai 2015). A second source of systematic bias is that many big data systems capture actions, and some people contribute many more actions than others. For example, some people on Twitter contribute hundreds of times more tweets than others. Therefore, the events on a specific platform can be ever more heavily reflective of certain subgroups than the platform itself.</p>

<p>Normally researchers want to know a lot about the data that they have. But, given the non-representative nature of big data, it is helpful to also flip your thinking. You also need to know a lot about the data that you don’t have. This is especially true when the data that you don’t have are systematically different from the data that you do have. For example, if you have the call records from a mobile phone company in a developing countries, you should think not just about the people in your dataset, but also about the people who might be too poor to own a mobile phone. Further, in Chapter 3, we’ll learn about how weighting can enable researchers to make better estimates from non-representative data.</p>

<h4 id="2-3-2-4-drifting">2.3.2.4 Drifting</h4>

<blockquote>
<p>Population drift, usage drift, and system drift make it hard to use big data source to study long-term trends.</p>
</blockquote>

<p>One of the great advantages of many big data sources are that they collect data over time. Social scientists call this kind of over-time data, longitudinal data. And, naturally, longitudinal data are very important for studying change. In order to reliably measure change, however, the measurement system itself must be stable. In the words of sociologist Otis Dudley Duncan, “if you want to measure change, don’t change the measure” (Fischer 2011).</p>

<p>Unfortunately, many big data systems—especially business system that create and capture digital traces—are changing all the time, a process that I’ll call drift. In particular, these systems change in three main ways: population drift (change in who is using them), behavioral drift (change in how people are using them), and system drift (change in the system itself). The three sources of drift mean that any pattern in digital trace data could be caused by an important change in the world, or it could be caused by some form of drift.</p>

<p>The first source of drift—population drift—is who is using the system, and this changes on long-time scales and short-time scales. For example, from 2008 to present the average age of people on social media has increased. In addition to these long-term trends, the people using a system at any moment varies. For example, during the US Presidential election of 2012 the proportion of tweets about politics that were written by women fluctuated from day to day (Diaz et al. 2016). Thus, what might appear to be a change in the mood of the Twitter-verse might actually just be changes in who is talking at any moment.</p>

<p>In addition to changes in who is using a system, there are also changes in how the system is used. For example, during the Occupy Gezi Park protests in Istanbul, Turkey in 2013 protesters changed their use of hashtags as the protest evolved. Here’s how Zeynep Tufekci (2014) described the drift, which she was able to detect because she was observing behavior on Twitter and on the ground:</p>

<blockquote>
<p>“What had happened was that as soon as the protest became the dominant story, large numbers of people &hellip; stopped using the hashtags except to draw attention to a new phenomenon &hellip;. While the protests continued, and even intensified, the hashtags died down. Interviews revealed two reasons for this. First, once everyone knew the topic, the hashtag was at once superfluous and wasteful on the character-limited Twitter platform. Second, hashtags were seen only as useful for attracting attention to a particular topic, not for talking about it.”
Thus, researchers who were studying the protests by analyzing tweets with protest-related hashtags would have a distorted sense of what was happening because of this behavioral drift. For example, they might believe that the discussion of the protest decreased long before it actually decreased.</p>
</blockquote>

<p>The third kind of drift is system drift. In this case, it is not the people changing or their behavior changing, but the system itself changing. For example, over time Facebook has increased the limit on the length of status updates. Thus, any longitudinal study of status updates will be vulnerable to artifacts caused by this change. System drift is closely related to problem called algorithmic confounding to which we now turn.</p>

<h4 id="2-3-2-5-algorithmically-confounded">2.3.2.5 Algorithmically confounded</h4>

<blockquote>
<p>Behavior in found data is not natural, it is driven by the engineering goals of the systems.</p>
</blockquote>

<p>Although many found data sources are non-reactive because people are not aware their data are being recorded (Section 2.3.1.3), researchers should not consider behavior in these online systems to be “naturally occurring” or “pure.” In reality, the digital systems that record behavior are highly engineered to induce specific behaviors such as clicking on ads or posting content. The ways that the goals of system designers can introduce patterns into data is called algorithmic confounding. Algorithmic confounding is relatively unknown to social scientists, but it is a major concern among careful data scientists. And, unlike some of the other problems with digital traces, algorithmic confounding is largely invisible.</p>

<p>A relatively simple example of algorithmic confounding is the fact that on Facebook there are an anomalously high number of users with approximately 20 friends (Ugander et al. 2011). Scientists analyzing with this data without any understanding of how Facebook works could doubtlessly generate many stories about how 20 is some kind of magical social number. However, Ugander and his colleagues had a substantial understanding of the process that generated the data, and they knew that Facebook encouraged people with few connections on Facebook to make more friends until they reached 20 friends. Although Ugander and colleagues don’t say this in the paper, this policy was presumably created by Facebook in order to encourage new users to become more active. Without knowing about the existence of this policy, however, it is easy to draw the wrong conclusion from the data. In other words, the surprisingly high number of people with about 20 friends tells us more about Facebook than human behavior.</p>

<p>More pernicious than this previous example where algorithmic confounding produced a quirky result that a careful researchers might investigate further, there is an even trickier version of algorithmic confounding that occurs when designers of online systems are aware of social theories and then bake these theories into the working of their systems. Social scientists call this performativity: when theories change the world in such a way that they bring the world more into line with the theory. In the cases of performative algorithmic confounding, the confounded nature of the data is likely invisible.</p>

<p>One example of a pattern created by performativity is transitivity in online social networks. In the 1970s and 1980s, researchers repeatedly found that if you are friends with Alice and you are friends with Bob, then Bob and Alice are more likely to be friends with each other than two randomly chosen people. And, this very same pattern was found in the social graph on Facebook (Ugander et al. 2011). Thus, one might conclude that patterns of friendship on Facebook replicate patterns of offline friendships, at least in terms of transitivity. However, the magnitude of transitivity in the Facebook social graph is partially driven by algorithmic confounding. That is, data scientists at Facebook knew of the empirical and theoretical research about transitivity and then baked it into how Facebook works. Facebook has a “People You May Know” feature that suggests new friends, and one way that Facebook decides who to suggest to you is transitivity. That is, Facebook is more likely to suggest that you become friends with the friends of your friends. This feature thus has the effect of increasing transitivity in the Facebook social graph; in other words, the theory of transitivity brings the world into line with the predictions of the theory (Healy 2015). Thus, when big data sources appears to reproduce predictions of social theory, we must be sure that the theory itself was not baked into how the system worked.</p>

<p>Rather than thinking of big data sources as observing people in a natural setting, a more apt metaphor is observing people in a casino. Casinos are highly engineered environments designed to induce certain behaviors, and a researchers would never expect that behavior in a casino would provide an unfettered window into human behavior. Of course, we could learn something about human behavior studying people in casinos—in fact a casino might be an ideal setting for studying the relationship between alcohol consumption and risk preferences—but if we ignored that the data was being created in a casino we might draw some bad conclusions.</p>

<p>Unfortunately, dealing with algorithmic confounding is particularly difficult because many features of online systems are proprietary, poorly documented, and constantly changing. For example, as I’ll explain later in this chapter, algorithmic confounding was one possible explanation for the gradual break-down of Google Flu Trends (Section 2.4.2), but this claim was hard to assess because the inner workings of Google’s search algorithm are proprietary. The dynamic nature of algorithmic confounding is one form of system drift. Algorithmic confounding means that we should be cautious about any claim for human behavior that comes from a single digital system, no matter how big.</p>

<h4 id="2-3-2-6-dirty">2.3.2.6 Dirty</h4>

<blockquote>
<p>Big data sources can be loaded with junk and spam.</p>
</blockquote>

<p>Some researchers believe that big data sources, especially those from online sources, are pristine because they are collected automatically. In fact, people who have worked with big data sources know that they are frequently dirty. That is, they frequently include data that do not reflect real actions of interest to researchers. Many social scientists are already familiar with the process of cleaning large-scale social survey data, but cleaning big data sources is more difficult for two reasons: 1) they were not created by researchers for researchers and 2) researchers generally have less understanding of how they were created.</p>

<p>The dangers of dirty digital trace data are illustrated by Back and colleagues’ (2010) study of the emotional response to the attacks of September 11, 2001. Researchers typically study the response to tragic events using retrospective data collected over months or even years. But, Back and colleagues found an always-on source of digital traces—the timestamped, automatically recorded messages from 85,000 American pagers—and this enabled the researchers to study emotional response on a much finer timescale. Back and colleagues created a minute-by-minute emotional timeline of September 11th by coding the emotional content of the pager messages by the percentage of words related to (1) sadness (e.g., crying, grief), (2) anxiety (e.g., worried, fearful), and (3) anger (e.g., hate, critical). They found that sadness and anxiety fluctuated throughout the day without a strong pattern, but that there was a striking increase in anger throughout the day. This research seems to be a wonderful illustration of the power of always-on data sources: using standard methods it would be impossible to have such a high-resolution timeline of the immediate response to an unexpected event.</p>

<p>Just one year later, however, Cynthia Pury (2011) looked at the data more carefully. She discovered that a large number of the supposedly angry messages were generated by a single pager and they were all identical. Here’s what those supposedly angry messages said:</p>

<p>“Reboot NT machine [name] in cabinet [name] at [location]:CRITICAL:[date and time]”
These messages were labeled angry because they included the word “CRITICAL”, which may generally indicate anger but does not in this case. Removing the messages generated by this single automated pager completely eliminates the apparent increase in anger over the course of the day (Figure 2.2). In other words, the main result in Back, Küfner, and Egloff (2010) was an artifact of one pager. As this example illustrates, relatively simple analysis of relatively complex and messy data has the potential to go seriously wrong.</p>

<p><img src="http://oaf2qt3yk.bkt.clouddn.com/548d989d34644c5cd7daf88a9257a729.png" alt="fig2.2" /></p>

<p>Figure 2.2: Estimated trends in anger over the course of September 11, 2001 based on 85,000 American pagers (Back, Küfner, and Egloff 2010; Pury 2011; Back, Küfner, and Egloff 2011). Originally, Back, Küfner, and Egloff (2010) reported a pattern of increasing anger throughout the day. However, most of these apparent angry messages were generated by a single pager that repeatedly sent out the following message: Reboot NT machine [name] in cabinet [name] at [location]:CRITICAL:[date and time]. With this message removed, the apparent increase in anger disappears (Pury 2011; Back, Küfner, and Egloff 2011). This figure is a reproduction of Fig 1B in Pury (2011).</p>

<p>While dirty data that is created unintentionally—such as from one noisy pager—can be detected by a reasonably careful researcher, there are also some online systems that attract intentional spammers. These spammers actively generate fake data, and—often motivated by profit—work very hard to keep their spamming concealed. For example, political activity on Twitter seems to include at least some reasonably sophisticated spam, whereby some political causes are intentionally made to look more popular than they actual are (Ratkiewicz et al. 2011). Researchers working with data that may contain intentional spam face the challenge of convincing their audience that they have detected and removed relevant spam.</p>

<p>Finally, what is considered dirty data can depend in subtle ways on your research questions. For example, many edits to Wikipedia are created by automated bots (Geiger 2014). If you are interested in the ecology of Wikipedia, then these bots are important. But, if you are interested in how humans contribute to Wikipedia, these edits made by these bots should be excluded.</p>

<p>The best ways to avoid being fooled by dirty data are to understand how your data were created to perform simple exploratory analysis, such as making simple scatter plots.</p>

<h4 id="2-3-2-7-sensitive">2.3.2.7 Sensitive</h4>

<blockquote>
<p>Some of the information that companies and governments have is sensitive.</p>
</blockquote>

<p>Health insurance companies have detailed information about the medical care received by their customers. This information could be used for important research about health, but if it became public it could potentially lead to emotional harm (e.g., embarrassment) and economic harm (e.g., loss of employment). Far from distinctive, many big data sources have information that is sensitive. The sensitive nature of this information is part of the reason that big data sources are often inaccessible (described above).</p>

<p>One way that researchers attempt to deal with this situation is to de-identify datasets that have sensitive information. But, as I will show in detail in Chapter 6 (Ethics) this approach seriously limited in ways that are not widely appreciated by both social scientists and data scientists.</p>

<p>In conclusion, the big data sources of today (and tomorrow) generally have ten characteristics. Many of the good properties—big, always-on, and nonreactive—come from the fact in the digital age companies and governments are able to collect data at a scale that was not possible previously. And, many of the bad properties—incomplete, inaccessible, non-representative, drifting, algorithmically confounded, inaccessible, dirty, and sensitive—come from the fact that the data is not collected by researchers for researchers. Understanding these characteristics are a necessary first step to learning from big data. And, now we turn to research strategies we can use with this data.</p>

<h2 id="2-4-research-strategies">2.4 Research strategies</h2>

<p>Given these ten characteristics of big data sources and the inherent limitations of even perfectly observed data, what kind of research strategies are useful? That is, how can we learn when we don’t ask questions and don’t run experiments? It might seem that just watching people could not lead to interesting research, but that’s not the case.</p>

<p>I see three main strategies for learning from observational data: counting things, forecasting things, and approximating experiments. I’ll describe each of these approaches—which could be called “research strategies” or “research recipes”—and I’ll illustrate them with examples. These strategies are neither mutually exclusive or exhaustive, but they do capture a lot of research with observational data.</p>

<p>To foreshadow the claims that follow, counting things is most important when we are empirically adjudicating between predictions from different theories. Forecasting, and especially nowcasting, can be useful for policy makers. Finally, big data increases our ability to make causal estimates from observational data.</p>

<h4 id="2-4-1-counting-things">2.4.1 Counting things</h4>

<blockquote>
<p>Simple counting can be interesting if you combine a good question with good data.</p>
</blockquote>

<p>Although it is couched in sophisticated sounding language, lots of social research is really just counting things. In the age of big data, researchers can count more than ever, but that does not automatically mean that research should be focused on counting more and more stuff. Instead, if we are going to do good research with big data, we need to ask: what things are worth counting? This may seem like an entirely subjective matter, but there are some general patterns.</p>

<p>Often students motivate their counting research by saying: I’m going to count something that no one has ever counted before. For example, a student might say, many people have studied migrants and many people have studied twins, but nobody has studied migrant twins. Motivation by absence does not usually lead to good research. Of course, there might be good reasons to study migrant twins, but the fact that they have not been studied before does not mean that they should be studied now. No one has ever counted the number of threads on the carpet in my office, but that does not automatically imply that this would be a good research project. Motivation by absence is kind of like saying: look, there’s a hole over there, and I’m going to work very hard to fill it up. But, not every hole needs to be filled.</p>

<p>Instead of motivating by absence, I think that counting leads to good research in two situations, when the research is interesting or important (or ideally both). For example, measuring the rate of unemployment is important because it is an indicator of the economy that drives policy decisions. Generally, people have a pretty good sense of what is important. So, in the rest of this section, I’m going to provide three examples where counting is interesting. In each case, the researchers were not counting haphazardly, rather they were counting in very particular settings that revealed important insights into more general ideas about how social systems work. In other words, a lot of what makes these particular counting exercises interesting is not in the data itself, it comes from these more general ideas.</p>

<p>Below I’ll present three examples on: 1) the working behavior of taxi drivers in New York (Section 2.4.1.1), 2) friendship formation by students (Section 2.4.1.2) and 3) social media censorship behavior of the Chinese government (Section 2.4.1.3). What these examples share is that they all show that counting big data can be used to test theoretical predictions. In some cases, big data sources enable you to do this counting relatively directly (as in the case of New York Taxis). In other cases, researchers will need to deal with incompleteness by merging data together and operationalizing theoretical constructs (as in the case of friendship formation); and in some cases researchers will need to collect their own observational data (as in the case of social media censorship). As I hope these examples show, for researchers who are able to ask interesting questions, big data holds great promise.</p>

<h4 id="2-4-1-1-taxis-in-new-york-city">2.4.1.1 Taxis in New York City</h4>

<blockquote>
<p>A researcher used big data from taxi meters to study decision-making of taxi drivers in New York. These data was well suited for this research.</p>
</blockquote>

<p>One example of the simple power of counting the right thing comes from Henry Farber’s (2015) study of the behavior of New York City taxi drivers. Although this group might not sound inherently interesting it is a strategic research site for testing two competing theories in labor economics. For the purposes of Farber’s research, there are two important features about the work environment of taxi drivers: 1) their hourly wage fluctuates from day-to-day, based in part on factors like the weather and 2) the number of hours they work can fluctuate each day based on the driver’s decisions. These features lead to an interesting question about the relationship between hourly wages and hours worked. Neoclassical models in economics predict that taxi drivers would work more on days where they have higher hourly wages. Alternatively, models from behavioral economics predict exactly the opposite. If drivers set a particular income target—say \$100 per day—and work until that target is met, then drivers would end up working fewer hours on days that they are earning more. For example, if you were a target earner, you might end up working 4 hours on a good day (\$25 per hour) and 5 hours on a bad day (\$20 per hour). So, do drivers work more hours on days with higher hourly wages (as predicted by the neoclassical models) or more hours on days with lower hourly wages (as predicted by behavioral economic models)?</p>

<p>To answer this question Farber obtained data on every taxi trip taken by New York City cabs from 2009 - 2013, data that are now publicly available. This data—which was collected by electronic meters that the city requires taxis to use—includes several pieces of information for each trip: start time, start location, end time, end location, fare, and tip (if the tip was paid with a credit card). In total, Farber’s data contained information on approximately 900 million trips taken during approximately 40 million shifts (a shift is roughly one day’s work for one driver). In fact, there was so much data, that Farber only used a random sample of it for his analysis. Using this taxi meter data, Farber found that most drivers work more on days when wages are higher, consistent with the neoclassical theory. In addition to this main finding, Farber was able to leverage the size of the data for a better understanding of heterogeneity and dynamics. Farber found that over time newer drivers gradually learn to work more hours on high wage days (e.g., they learn to behave as the neoclassical models predicts). And, new drivers who behave more like target earners are more likely to quit being a taxi driver. Both of these more subtle findings, which help explain the observed behavior of current drivers, were only possible because of the size of the dataset. They would have been impossible to detect in earlier studies that used paper trip sheets from a small number of taxi drivers over a short period of time (e.g., Camerer et al. (1997)).</p>

<p>Farber’s study was close to a best-case for a study using big data. First, the data were not non-representative because the city required drivers to use digital meters. And, the data were not incomplete because the data that was collected by the city was pretty close to the data that Farber would have collected if he had the choice (one difference is that Farber would have wanted data on total wages—fares plus tips—but the city data only included tips paid by credit card). The key to Farber’s research was combining a good question with good data. The data alone are not enough.</p>

<h4 id="2-4-1-2-friendship-formation-among-students">2.4.1.2 Friendship formation among students</h4>

<blockquote>
<p>Researchers used email logs and administrative records to understand friendship formation. This research requires dealing with the incompleteness of big data.</p>
</blockquote>

<p>In many situations, researchers are not lucky enough to have everything that they want automatically collected in one place. Two common problems are incomplete information about the people and a mismatch between theoretical constructs and data. Both of these problems were addressed by Kossinets and Watts (2009) as part of their efforts to understand how social networks evolve.</p>

<p>Roughly speaking, researchers think that social network evolution is driven by three features: 1) the structure of existing relationships 2) shared activities (e.g., dorms, classes) and 3) demographics. Understanding the interrelationships between these three factors requires longitudinal network data combined with information about individuals’ demographics and activities. Earlier studies had some of these features, but none had all three.</p>

<p>Kossinets and Watts started their research by acquiring the email logs from a large university. However, these email logs alone were incomplete, they don’t include everything needed to understand the various factors driving network evolution. Therefore, Kossinets and Watts merged these email logs, with two other sources of information: demographic information collected by the university and information about shared activities (e.g., student residence information and a complete list of enrollment in courses). Once these three sources of information, each of which was incomplete, were merged together Kossinets and Watts had a powerful data structure for understanding network evolution.</p>

<p>But, there was one final challenge that they had to overcome. Kossinets and Watts wanted to study how the social network in this university evolved so they needed a way to use the email logs into an estimate of who was connected to who at which time. As discussed in previously (Section 2.3.2.1), this kind of operationalization of theoretical constructs is a big challenge when using digital traces for social research. In the end, Kossinets and Watts decided that two people were considered connected at time  t if and only if they had exchanged emails (i emailed j and j emailed i) in the previous 60 days. These choices were not arbitrary; they were based on careful consideration of this empirical setting, and Kossinets and Watts checked that their results were robust to these choices. In general, if your operationalization involves choosing some specific cutoffs—say 60 days instead of 30 days or 90 days—it is a good idea to make sure that your results are not sensitive to this choice.</p>

<p>Once Kossinets and Watts addressed the problem caused by incompleteness (e.g., missing demographic information, missing information about shared activity, and missing theoretical constructs), they had data that enabled them to understand the three main forces that can drive network evolution: 1) the structure of existing relationships 2) shared activities (e.g., dorms, classes) and 3) demographics. Consistent with earlier research, they found that people with similar demographics are more likely to form relationships. However, unlike earlier studies, they found that this pattern was strongly mitigated by the existing network structure and shared activities. In other words, the pattern that earlier researchers had seen was partially explained by data that earlier researchers did not have. Thus, by successfully dealing with the incompleteness of their data, Kossinets and Watts were able clarify the interaction of a variety of different factors that drive social network evolutions.</p>

<h4 id="2-4-1-3-censorship-of-social-media-by-the-chinese-government">2.4.1.3 Censorship of social media by the Chinese government</h4>

<blockquote>
<p>Researchers scraped Chinese social media sites to study censorship. They dealt with incompleteness with latent-trait inference.</p>
</blockquote>

<p>In addition to the big data used in the two previous examples, researchers can also collect their own observational data, as was wonderfully illustrated by Gary King, Jennifer Pan, and Molly Roberts’ (2013) research on censorship by the Chinese government.</p>

<p>Social media posts in China are censored by an enormous state apparatus that is thought to include tens of thousands of people. Researchers and citizens, however, have little sense of how these censors decide what content should be deleted from the social media. Scholars of China actually have conflicting expectations about which kinds of posts are most likely to get deleted. Some think that censors focus on posts that are critical of the state while others think they focus on posts that encourage collective behavior, such as protests. Figuring out which of these expectations is correct has implications for how researchers understand China and other authoritarian governments that engage in censorship. Therefore, King and colleagues wanted to compare posts that were published and subsequently deleted to posts that were published and never deleted.</p>

<p>Collecting these posts involved the amazing engineering feat of crawling more than 1,000 Chinese social media websites—each with different page layouts—finding relevant posts, and then revisiting these posts to see which were subsequently deleted. In addition to the normal engineering problems associated with large scale web-crawling, this project had the added challenge that it needed to be extremely fast because many censored posts are taken down in less than 24 hours. In other words, a slow crawler would miss lots of posts that were censored. Further, the crawlers had to do all this data collection while evading detection lest the social media websites block access or otherwise change their policies in response to the study.</p>

<p>Once this massive engineering task was completed, King and colleagues had obtained about 11 million posts on 85 different topics that were pre-specified based on their expected level of sensitivity. For example, a topic of high sensitivity is Ai Weiwei, the dissident artist; a topic of middle sensitivity is appreciation and devaluation of the Chinese currency, and a topic of low sensitivity is the World Cup. Of these 11 million posts about 2 million had been censored, but posts on highly sensitive topics were censored only slightly more often than posts on middle and low sensitivity topics. In other words, Chinese censors are about as likely to censor a post that mentions Ai Weiwei as a post that mentions the World Cup. These findings did not match the simplistic idea that the government censors all posts on sensitive topics.</p>

<p>This simple calculation of censorship rate by topic could be misleading, however. For example, the government might censor posts that are supportive of Ai Weiwei, but leave posts that are critical of him. In order to distinguish between posts more carefully, the researchers need to measure the sentiment of each post. Thus, one way to think about it is that the sentiment of each post in an important latent feature of each post. Unfortunately, despite much work, fully automated methods of sentiment detection using pre-existing dictionaries are still not very good in many situations (think back to the problems creating an emotional timeline of September 11, 2001 from Section 2.3.2.6). Therefore, King and colleagues needed a way to label their 11 million social media posts as to whether they were 1) critical of the state, 2) supportive of the state, or 3) irrelevant or factual reports about the events. This sounds like a massive job, but they solved it using a powerful trick; one that is common in data science but currently relatively rare in social science.</p>

<p>First, in a step typically called pre-processing, the researchers converted the social media posts into a document-term matrix, where there was one row for each document and one column that recorded whether the post contained a specific word (e.g., protest, traffic, etc.). Next, a group of research assistants hand-labeled the sentiment of a sample of post. Then, King and colleagues used this hand-labeled data to estimate a machine learning model that could infer the sentiment of a post based on its characteristics. Finally, they used this machine learning model to estimate the sentiment of all 11 million posts. Thus, rather than manually reading and labeling 11 million posts (which would be logistically impossible), they manually labeled a small number of posts and then used what data scientists would call supervised learning to estimate the categories of all the posts. After completing this analysis, King and colleagues were able to conclude that, somewhat surprisingly, the probability of a post being deleted was unrelated to whether it was critical of the state or supportive of the state.</p>

<p><img src="http://oaf2qt3yk.bkt.clouddn.com/e5af8b87bb9c85f434d8e1335cc4f954.png" alt="fig2.3" /></p>

<p>Figure 2.3: Simplified schematic for the procedure used in King, Pan, and Roberts (2013) to estimating the sentiment of 11 million Chinese social media posts. First, in a step typically called pre-processing, the researchers converted the social media posts into a document-term matrix (see Grimmer and Stewart (2013) for more information). Second, the researchers hand-coded the sentiment of a small sample of posts. Third, the researchers trained a supervised learning model to classify the sentiment of posts. Fourth, the researchers used the supervised learning model to estimate the sentiment of all the posts. See King, Pan, and Roberts (2013), Appendix B for a more detailed description.</p>

<p>In the end, King and colleagues discovered that only three types of posts were regularly censored: pornography, criticism of censors, and those that had collective action potential (i.e., the possibility of leading to large-scale protests). By observing a huge number of posts that were deleted and posts that were not deleted, King and colleagues were able to learn how the censors work just by watching and counting. In subsequent research, they actually directly intervened into the Chinese social media ecosystem by creating posts with systematically different content and measuring which get censored (King, Pan, and Roberts 2014). We will learn more about experimental approaches in Chapter 4. Further, foreshadowing a theme that will occur throughout the book, these latent-attribute inference problems—which can sometimes be solved with supervised learning—turn out to be very common in social research in the digital age. You will see pictures very similar to Figure 2.3 in Chapters 3 (Asking questions) and 5 (Creating mass collaboration); it is one of the few ideas that appears in multiple chapters.</p>

<p>All three of these examples—the working behavior of taxi drivers in New York, friendship formation by students, and social media censorship behavior of the Chinese government—show that relatively simple counting of observational data can enable researchers to test theoretical predictions. In some cases, big data enables you to do this counting relatively directly (as in the case of New York Taxis). In other cases, researchers will need to collect their own observational data (as in the case of Chinese censorship); deal with incompleteness by merging data together (as in the case of network evolution); or performing some form of latent-trait inference (as in the case of Chinese censorship). As I hope these examples show, for researchers who are able to ask interesting questions, big holds great promise.</p>

<h4 id="2-4-2-forecasting-and-nowcasting">2.4.2 Forecasting and nowcasting</h4>

<blockquote>
<p>Predicting the future is hard, but predicting the present is easier.</p>
</blockquote>

<p>The second main strategy used by researchers with observational data is forecasting. Predicting the future is notoriously difficult, but it can be incredibly important for decision makers, whether they work in companies or governments.</p>

<p>Kleinberg et al. (2015) offers two stories that clarify the importance of forecasting for certain policy problems. Imagine one policy maker, I’ll call her Anna, who is facing a drought and must decide whether to hire a shaman to do a rain dance to increase the chance of rain. Another policy maker, I’ll call him Bob, must decide whether to take an umbrella to work to avoid getting wet on the way home. Both Anna and Bob can make a better decision if they understand weather, but they need to know different things. Anna needs to understand whether the rain dance causes rain. Bob, on the other hand, does not need to understanding anything about causality; he just needs an accurate forecast. Social researchers often focus on what Kleinberg et al. (2015) call “rain dance–like” policy problems—those that focus on causality—and ignore “umbrella-like” policy problems that are focused on forecasting.</p>

<p>I’d like to focus, however, on a special kind of forecasting called nowcasting—a term derived from combining “now” and “forecasting.” Rather than predicting the future, nowcasting attempts to predict the present (Choi and Varian 2012). In other words, nowcasting uses forecasting methods for problems of measurement. As such, it should be especially useful to governments who require timely and accurate measures about their countries. Nowcasting can be illustrated most clearly with the example of Google Flu Trends.</p>

<p>Imagine that you are feeling a bit under the weather so you type “flu remedies” into a search engine, receive a page of links in response, and then follow one of them to a helpful webpage. Now imagine this activity being played out from the perspective of the search engine. Every moment, millions of queries are arriving from around the world, and this stream of queries—what Battelle (2006) has called the “database of intentions”— provides a constantly updated window into the collective global consciousness. However, turning this stream of information into a measurement of the prevalence of the flu is difficult. Simply counting up the number of queries for “flu remedies” might not work well. Not everyone who has the flu searches for flu remedies and not everyone who searchers for flu remedies has the flu.</p>

<p>The important and clever trick behind Google Flu Trends was to turn a measurement problem into a forecasting problem. The U.S. Centers for Disease Control and Prevention (CDC) has an influenza monitoring system that collects information from doctors around the country. However, one problem with this CDC system is there is a two week reporting lag; the time it takes for the data arriving from doctors to be cleaned, processed, and published. But, when handling an emerging epidemic, public health offices don’t want to know how much influenza there was two weeks ago; they want to know how much influenza there is right now. In fact, in many other traditional sources of social data, there are gaps between waves of data collection and reporting lags. Most big data sources, on the other hand, are always-on (Section 2.3.1.2).</p>

<p>Therefore, Jeremy Ginsberg and colleagues (2009) tried to predict the CDC flu data from the Google search data. This is an example of “predicting the present” because the researchers were trying to measure how much flu there is now by predicting future data from the CDC, future data that is measuring the present. Using machine learning, they searched through 50 million different search terms to see which are most predictive of the CDC flu data. Ultimately, they found a set of 45 different queries that seemed to be most predictive, and the results were quite good: they could use the search data to predict the CDC data. Based in part on this paper, which was published in Nature, Google Flu Trends became an often repeated success story about the power of big data.</p>

<p>There are two important caveats to this apparent success, however, and understanding these caveats will help you evaluate and do forecasting and nowcasting. First, the performance of Google Flu Trends was actually not much better than a simple model that estimates the amount of flu based on a linear extrapolation from the two most recent measurements of flu prevalence (Goel et al. 2010). And, over some time periods Google Flu Trends was actually worse than this simple approach (Lazer et al. 2014). In other words, Google Flu Trends with all its data, machine learning, and powerful computing did not dramatically outperform a simple and easier to understand heuristic. This suggests that when evaluating any forecast or nowcast it is important to compare against a baseline.</p>

<p>The second important caveat about Google Flu Trends is that its ability to predict the CDC flu data was prone to short-term failure and long-term decay because of drift and algorithmic confounding. For example, during the 2009 Swine Flu outbreak Google Flu Trends dramatically over-estimated the amount of influenza, probably because people tend to change their search behavior in response to widespread fear of a global pandemic (Cook et al. 2011; Olson et al. 2013). In addition to these short-term problems, the performance gradually decayed over time. Diagnosing the reasons for this long term decay are difficult because the Google search algorithms are proprietary, but it appears that in 2011 Google made changes that would suggest related search terms when people search for symptoms like “fever” and “cough” (it also seem that this feature is no longer active). Adding this feature is a totally reasonable thing to do if you are running a search engine business, and it had the effect of generating more health related searches. This was probably a success for the business, but it caused Google Flu Trends to over-estimate flu prevalence (Lazer et al. 2014).</p>

<p>Fortunately, these problems with Google Flu Trends are fixable. In fact, using more careful methods, Lazer et al. (2014) and Yang, Santillana, and Kou (2015) were able to get better results. Going forward, I expect that nowcasting studies that combine big data with researcher collected data—that combine Duchamp-style Readymades with Michaelangelo-style Custommades—will enable policy makers to produce faster and more accurate measurements of the present and predictions of the future.</p>

<h3 id="2-4-3-approximating-experiments">2.4.3 Approximating experiments</h3>

<blockquote>
<p>We can approximate experiments that we can’t do. Two approaches that especially benefit from the digital age are matching and natural experiments.</p>
</blockquote>

<p>Many important scientific and policy questions are causal. Let’s consider, for example, the following question: what is the effect of a job training program on wages? One way to answer this question would be with a randomized controlled experiment where workers were randomly assigned to either receive training or not receive training. Then, researchers could estimate the effect of training for these participants by simply comparing the wages of people who received the training to those that did not receive it.</p>

<p>The simple comparison is valid because of something that happens before the data was even collected: the randomization. Without randomization, the problem is much trickier. A researcher could compare the wages of people who voluntarily signed up for training to those who didn’t sign-up. That comparison would probably show that people who received training earned more, but how much of this is because of training and how much of this is because people that sign-up for training are different from those that don’t sign-up for training? In other words, is it fair to compare the wages of these two groups of people?</p>

<p>This concern about fair comparisons leads some researchers to believe that it is impossible to make causal estimates without running an experiment. This claim goes too far. While it is true that experiments provide the strongest evidence for causal effects, there are other strategies that can provide valuable causal estimates. Instead of thinking that causal estimates are either easy (in the case of experiments) or impossible (in the case of passively observed data), it is better to think of the strategies for making causal estimates lying along a continuum from strongest to weakest (Figure 2.4). At the strongest end of the continuum are randomized controlled experiments. But, these are often difficult to do in social research because many treatments require unrealistic amounts of cooperation from governments or companies; quite simply there are many experiments that we cannot do. I will devote all of Chapter 4 to both the strengths and weaknesses of randomized controlled experiments, and I’ll argue that in some cases, there are strong ethical reasons to prefer observational to experimental methods.</p>

<p><img src="http://oaf2qt3yk.bkt.clouddn.com/6bd9485a59999ed235e72a8b3ea38e85.png" alt="fig2.4" /></p>

<p>Figure 2.4: Continuum of research strategies for estimated causal effects.</p>

<p>Moving along the continuum, there are situations where researchers have not explicitly randomized. That is, researchers are attempting to learn experiment-like knowledge without actually doing an experiment; naturally, this is going to be tricky, but big data greatly improves our ability to make causal estimates in these situations.</p>

<p>Sometimes there are settings where randomness in the world happens to create something like an experiment for researchers. These designs are called natural experiments, and they will be considered in detail in Section 2.4.3.1. Two features of big data sources—their always-on nature and their size—greatly enhances our ability to learn from natural experiments when they occur.</p>

<p>Moving further away from randomized controlled experiments, sometimes there is not even an event in nature that we can use to approximate a natural experiment. In these settings, we can carefully construct comparisons within non-experimental data in an attempt to approximate an experiment. These designs are called matching, and they will be considered in detail in Section 2.4.3.2. Like natural experiments, matching is a design that also benefits from big data sources. In particular, the massive size—both in terms of number of cases and type of information per case—greatly facilitates matching. The key difference between natural experiments and matching is that in natural experiments the researcher knows the process through which treatment was assigned and believes it to be random.</p>

<p>The concept of fair comparisons that motivated the desires to do experiments also underlies the two alternative approaches: natural experiments and matching. These approaches will enable you to estimate causal effects from passively observed data by discovering fair comparisons sitting inside of the data that you already have.</p>

<h4 id="2-4-3-1-natural-experiments">2.4.3.1 Natural experiments</h4>

<blockquote>
<p>Natural experiments take advantage of random events in the world. random event + always-on data system = natural experiment</p>
</blockquote>

<p>The key to randomized controlled experiments enabling fair comparison is the randomization. However, occasionally something happens in the world that essentially assigns people randomly or nearly randomly to different treatments. One of clearest examples of the strategy of using natural experiments comes from the research of Angrist (1990) that measures the effect of military services on earnings.</p>

<p>During the war in Vietnam, the United States increased the size of its armed forces through a draft. In order to decide which citizens would be called into service, the US government held a lottery. Every birthdate was represented on a piece of paper, and these papers were placed in a large glass jar. As shown in Figure 2.5, these slips of paper were drawn from the jar one at a time to determine the order that young men would be called to serve (young women were not subject to the draft). Based on the results, men born on September 14 were called first, men born on April 24 were called second, and so on. Ultimately, in this lottery, men born on 195 different days were called to service while men born on 171 days were not called.</p>

<p><img src="http://oaf2qt3yk.bkt.clouddn.com/c55e0c91fe69f3958341962913e0cbe5.png" alt="fig2.5" /></p>

<p>Figure 2.5: Congressman Alexander Pirnie (R-NY) drawing the first capsule for the Selective Service draft on December 1, 1969. Joshua Angrist (1990) combined the draft lottery with earnings data from the Social Security Administration to estimate the effect of military service on earnings. This is an example of research using a natural experiment. Source: Wikimedia Commons</p>

<p>Although it might not be immediately apparent, a draft lottery has a critical similarity to a randomized controlled experiment: in both situations participants are randomly assigned to receive a treatment. In the case of the draft lottery, if we are interested in learning about the effects of draft-eligibility and military service on subsequent labor market earnings, we can compare outcomes for people whose birthdates were below the lottery cutoff (e.g., September 14, April 24, etc.) with the outcomes for people whose birthdays were after the cutoff (e.g., February 20, December 2, etc.).</p>

<p>Given that this treatment of being drafted has been randomly assigned, we can then measure the effect of this treatment for any outcome that has been measured. For example, Angrist (1990) combined the information about who was randomly selected in the draft with earnings data that was collected by the Social Security Administration to conclude that the earnings of white veterans were about 15% less than the earnings of comparable non-veterans. Other researchers have used a similar trick as well. For example, Conley and Heerwig (2011) combined the information about who was randomly selected in the draft with household data collected from the 2000 Census and 2005 American Community Survey and found that so long after the draft, there was little long-term effect of military service on variety of outcomes such as housing tenure (owning versus renting) and residential stability (likelihood of having moved in previous five years).</p>

<p>As this example illustrates, sometimes social, political, or natural forces create experiments or near-experiments that can be leveraged by researchers. Often natural experiments are the best way to estimate cause-and-effect relationships in settings where it is not ethical or practical to run randomized controlled experiments. They are an important strategy for discovering fair comparisons in non-experimental data. This research strategy can be summarized by this equation:</p>

<p>random (or as if random) event+always-on data stream=natural experiment(2.1)</p>

<p>However, the analysis of natural experiments can be quite tricky. For example, in the case of the Vietnam draft, not everyone who was draft-eligible ended up serving (there were a variety of exemptions). And, at the same time, some people who were not draft-eligible volunteered for service. It was as if in a clinical trial of a new drug, some people in the treatment group did not take their medicine and some of the people in the control group somehow received the drug. This problem, called two-sided noncompliance, as well as many other problems are described in greater detail in some of the recommended readings at the end of this chapter.</p>

<p>The strategy of taking advantage of naturally occurring random assignment precedes the digital age, but the prevalence of big data makes this strategy much easier to use. Once you realize some treatment has been assigned randomly, big data sources can provide the outcome data that you need in order to compare the results for people in the treatment and control conditions. For example, in his study of the effects of the draft and military service, Angrist made use of earnings records from the Social Security Administration; without this outcome data, his study would not have been possible. In this case, the Social Security Administration is the always-on big data source. As more and more automatically collected data sources exist, we will have more outcome data that can measure the effects of changes created by exogenous variation.</p>

<p>To illustrate this strategy in the digital age, let’s consider Mas and Moretti’s (2009) elegant research on the effect of peers on productivity. Although on the surface it might look different than Angrist’s study about the effects of the Vietnam Draft, in structure they both follow the pattern in eq. 2.1.</p>

<p>Mas and Moretti measured how peers affect the productivity of workers. On the one hand, having a hard working peer might lead workers to increase their productivity because of peer pressure. Or, on the other hand, a hard working peer might lead other workers to slack off even more. The clearest way to study peer effects on productivity would be a randomized controlled experiment where workers are randomly assigned to shifts with workers of different productivity levels and then resulting productivity is measured for everyone. Researchers, however, do not control the schedule of workers in any real business, and so Mas and Moretti had to rely on a natural experiment which took place in a supermarket.</p>

<p>Just like eq. 2.1, their study had two parts. First, they used the logs from the supermarket checkout system to have a precise, individual, and always-on measure of productivity: the number of items scanned per second. And, second, because of the way that scheduling was done at this supermarket, they have near random composition of peers. In other words, even though the scheduling of cashiers is not determined by a lottery, it was essentially random. In practice, the confidence we have in natural experiments frequently hinges on the plausibility of this “as-if” random claim. Taking advantage of this random variation, Mas and Moretti found that working with higher productivity peers increases productivity. Further, Mas and Moretti used the size and richness of their dataset to move beyond the estimation of cause-and-effect to explore two more important and subtle issues: heterogeneity of this effect (for which kinds of workers is the effect larger) and mechanism behind the effect (why does having high productivity peers lead to higher productivity). We will return to these two important issues—heterogeneity of treatment effects and mechanisms—in Chapter 5 when we discuss experiments in more detail.</p>

<p>Generalizing from the studies on the effect of the Vietnam Draft on earnings and the study of the effect of peers on productivity, Table 2.3 summarizes other studies that have this exact same structure: using an always-on data source to measure the impact of some event. As Table 2.3 makes clear, natural experiments are everywhere if you just know how to look for them.</p>

<p>Table 2.3: Examples of natural experiments using big data sources. All these studies follow the same basic recipe: random (or as if random) event + always-on data system. See Dunning (2012) for more examples.</p>

<table>
<thead>
<tr>
<th>Substantive focus</th>
<th align="center">Source of natural experiment</th>
<th align="right">Always-on data source</th>
<th align="right">Citation</th>
</tr>
</thead>

<tbody>
<tr>
<td>Peer effects on productivity</td>
<td align="center">scheduling process</td>
<td align="right">checkout data</td>
<td align="right">Mas and Moretti (2009)</td>
</tr>

<tr>
<td>Friendship formation</td>
<td align="center">hurricanes</td>
<td align="right">Facebook</td>
<td align="right">Phan and Airoldi (2015)</td>
</tr>

<tr>
<td>Spread of emotions</td>
<td align="center">rain</td>
<td align="right">Facebook</td>
<td align="right">Coviello et al. (2014)</td>
</tr>

<tr>
<td>Peer to peer economic transfers</td>
<td align="center">earthquake</td>
<td align="right">mobile money data</td>
<td align="right">Blumenstock, Fafchamps, and Eagle (2011)</td>
</tr>

<tr>
<td>Personal consumption behavior</td>
<td align="center">2013 US government shutdown</td>
<td align="right">personal finance data</td>
<td align="right">Baker and Yannelis (2015)</td>
</tr>

<tr>
<td>Economic impact of recommender systems</td>
<td align="center">various</td>
<td align="right">browsing data at Amazon</td>
<td align="right">Sharma, Hofman, and Watts (2015)</td>
</tr>

<tr>
<td>Effect of stress on unborn babies</td>
<td align="center">2006 Israel–Hezbollah war</td>
<td align="right">Birth records</td>
<td align="right">Torche and Shwed (2015)</td>
</tr>

<tr>
<td>Reading behavior on Wikipedia</td>
<td align="center">Snowden revelations</td>
<td align="right">Wikipedia logs</td>
<td align="right">Penney (2016)</td>
</tr>
</tbody>
</table>

<p>In practice, researchers use two different strategies for finding natural experiments, both of which can be fruitful. Some researchers start with the always-on data source and look for random events in the world; others start with random events in the world and look for data sources that capture their impact. Finally, notice that the strength of natural experiments comes not from the sophistication of the statistical analysis, but from the care in discovering a fair comparison created by a fortunate accident of history.</p>

<h4 id="2-4-3-2-matching">2.4.3.2 Matching</h4>

<blockquote>
<p>Matching create fair comparisons by pruning away cases.</p>
</blockquote>

<p>Fair comparisons can come from either randomized controlled experiments or natural experiments. But, there are many situations where you can’t run the ideal experiment and nature has not provided a natural experiment. In these settings, the best way to create a fair comparison is matching. In matching, the researcher looks through non-experimental data to create pairs of people that are similar except that one has received the treatment and one has not. In the process of matching, researchers are actually also pruning; that is, discarding cases where there are no obvious comparison. Thus, this method would be more accurately called matching-and-pruning, but I’ll stick with the traditional term: matching.</p>

<p>A beautiful example of the power of matching strategies with massive non-experimental data sources come from the research on consumer behavior by Liran Einav and colleagues (2015). Einav and colleagues were interested in auctions taking place on eBay, and in describing their work, I’ll focus on one particular aspect: the effect of auction starting price on auction outcomes, such as the sale price or the probability of a sale.</p>

<p>The most naive way to answer the question about the effect of starting price on sale price would be to simply calculate the final price for auctions with different starting prices. This approach would be fine if you simply want to predict the sale price of a given item that had been put on eBay with a given starting price. But, if your question is what is the effect of starting price on market outcomes this approach will not work because it is not based on fair comparisons; the auctions with lower starting prices might be quite different from auctions with higher starting prices (e.g., they might be for different types of goods or include different types of sellers).</p>

<p>If you are already concerned about making fair comparisons, you might skip the naive approach and consider running a field experiment where you would sell a specific item—say, a golf club—with a fixed set of auction parameters—say, free shipping, auction open for two weeks, etc.—but with randomly set starting prices. By comparing the resulting market outcomes, this field experiment would offer a very clear measurement of the effect of starting price on sale price. But, this measurement would only apply to one particular product and set of auction parameters. The results might be different, for example, for different types of products. Without strong theory, it is difficult to extrapolate from this single experiment the full range of possible experiments that could have been run. Further, field experiments are sufficiently expensive that it would be infeasible to run enough of them up to cover the whole parameter space of products and auction types.</p>

<p>In contrast to the naive approach and the experimental approach, Einav and colleagues take a third approach: matching. The main trick of their strategy is to discover things similar to field experiments that have already happened on eBay. For example, Figure 2.6 shows some of the 31 listings for exactly the same golf club—a Taylormade Burner 09 Driver—being sold by exactly the same seller—“budgetgolfer”. However, these listings have slightly different characteristics. Eleven of them offer the driver for a fixed price of $124.99, while the other 20 are auctions with different end dates. Also, the listings have different shipping fees, either $7.99 or $9.99. In other words, it is as if “budgetgolfer” is running experiments for the researchers.</p>

<p>The listings of the Taylormade Burner 09 Driver being sold by “budgetgolfer” are one example of a matched set of listings, where the exact same item is being sold by the exact same seller but each time with slightly different characteristics. Within the massive logs of eBay there are literally hundreds of thousands of matched sets involving millions of listings. Thus, rather than comparing the final price for all auctions within a given starting price, Einav and colleagues make comparisons within matched sets. In order to combine results from the comparisons within these hundreds of thousands of matched sets, Einav and colleagues re-express the starting price and final price in terms of the reference value of each item (e.g., its average sale price). For example, if the Taylormade Burner 09 Driver has a reference value of $100 (based on its sales), then a starting price of $10 would be expressed as 0.1 and final price of $120 would be expressed as 1.2.</p>

<p><img src="http://oaf2qt3yk.bkt.clouddn.com/72131fe0e2d1f1466aa0b8ae599c143c.png" alt="fig2.6" /></p>

<p>Figure 2.6: An example of a matched set. This is the exact same golf club (a Taylormade Burner 09 Driver) being sold by the exact same person (budgetgolfer), but some of these sales were performed different conditions (e.g., different starting price). Figure taken from Einav et al. (2015).</p>

<p>Recall that Einav and colleagues were interested in the effect of start price on auction outcomes. First, using linear regression they estimated that higher starting prices decrease the probability of a sale, and that higher starting prices increase the final sale price, conditional on a sale occurring. By themselves, these estimates—which are averaged over all products and assume a linear relationship between starting price and final outcomes—are not all that interesting. But, Einav and colleagues also use the massive size of their data to estimate a variety of more subtle findings. First, Einav and colleagues made these estimates separately for items of different prices and without using linear regression. They found that while the relationship between start price and probability of a sale is linear, the relationship between starting price and sale price is clearly non-linear (Figure 2.7). In particular, for starting prices between 0.05 and 0.85, the starting price has very little impact on sale price, a finding that was completed missed in the analysis that had assumed a linear relationship.</p>

<p><img src="http://oaf2qt3yk.bkt.clouddn.com/6c4b3eaa480e9f5488e837434afa152e.png" alt="fig2.7" /></p>

<p>Figure 2.7: Relationship between auction start price and probability of a sale (left panel) and sale price (right panel). There is roughly a linear relationship between start price and probability of sale, but there is a non-linear relationship between start price and sale price; for starting prices between 0.05 and 0.85, the starting price has very little impact on sale price. In both cases, the relationships are basically independent of item value. These graphs reproduce Fig 4a and 4b Einav et al. (2015).</p>

<p>Second, rather than averaging over all items, Einav and colleagues also use the massive scale of their data to estimate the impact of starting price for 23 different categories of items (e.g, pet supplies, electronics, and sports memorabilia) (Figure 2.8). These estimates show that for more distinctive items—such as memorabilia—start price has a smaller effect on the probability of a sale and a larger effect on the final sale price. Further, for more commodified items—such as DVDs and video—the start price has almost no impact on the final price. In other words, an average that combines results from 23 different categories of items hides important information about the differences between these items.</p>

<p><img src="http://oaf2qt3yk.bkt.clouddn.com/9d89fbd54de2f8d18e8d9f638bad3412.png" alt="fig2.8" /></p>

<p>Figure 2.8: Results showed estimates from each category individually; the solid dot in the estimate for all categories pooled together, Table 11(Einav et al. 2015, Table 11). These estimates show that for more distinctive items—such as memorabilia—the start price has a smaller effect on the probability of a sale (x-axis) and a larger effect on the final sale price (y-axis).</p>

<p>Even if you are not particularly interested in auctions on eBay, you have to admire the way that Figure 2.7 and Figure 2.8 offer a richer understanding of eBay than simple linear regression estimates that assume linear relationships and combine many different categories of items. These more subtle estimates illustrate the power of matching in massive data; these estimates would have been impossible without an enormous number of field experiments, which would have been prohibitively expensive.</p>

<p>Of course, we should have less confidence in the results of any particular matching study than we would in the results of a comparable experiment. When assessing the results from any matching study, there are two important concerns. First, we have to remember that we can only ensure fair comparisons on things that were used for matching. In their main results, Einav and colleagues did exact matching on four characteristics: seller ID number, item category, item title, and subtitle. If the items were different in ways that were not used for matching, that could create an unfair comparison. For example, if “budgetgolfer” lowered prices for Taylormade Burner 09 Driver in the winter (when golf clubs are less popular), then it could appear that lower starting prices lead to lower final prices, when in fact this would be an artifact of seasonal variation in demand. In general, the best approach to this problem seems to be trying many different kinds of matching. For example, Einav and colleagues repeat their analysis where matched sets include items on sale within one year, within one month, and contemporaneously. Making the time window tighter decreases the number of matched sets, but reduces concerns about seasonal variation. Fortunately, they find that results are unchanged by these changes in matching criteria. In the matching literature, this type of concern is usually expressed in terms of observables and unobservables, but the key idea is really that researchers are only creating fair comparisons on the features used in matching.</p>

<p>The second major concern when interpreting matching results is that they only apply to matched data; they do not apply to the cases that could not be matched. For example, by limiting their research to items that had multiple listings Einav and colleagues are focusing on professional and semi-professional sellers. Thus, when interpreting these comparisons we must remember that they only apply to this subset of eBay.</p>

<p>Matching is a powerful strategy for finding fair comparisons in large datasets. To many social scientists, matching feels like second-best to experiments, but that is a belief that should be revised, slightly. Matching in massive data might be better than a small number of field experiments when: 1) heterogeneity in effects is important and 2) there are good observables for matching. Table 2.4 provides some other examples of how matching can be used with big data sources.</p>

<p>Table 2.4: Examples of studies that use matching to find fair comparisons within digital traces.</p>

<table>
<thead>
<tr>
<th>Substantive focus</th>
<th align="center">Big data source</th>
<th align="right">Citation</th>
</tr>
</thead>

<tbody>
<tr>
<td>Effect of shootings on police</td>
<td align="center">violence    Stop-and-frisk records</td>
<td align="right">Legewie (2016)</td>
</tr>

<tr>
<td>Effect of September 11, 2001 on families and neighbors</td>
<td align="center">voting records and donation records</td>
<td align="right">Hersh (2013)</td>
</tr>

<tr>
<td>Social contagion</td>
<td align="center">Communication and product adoption data</td>
<td align="right">Aral, Muchnik, and Sundararajan (2009)</td>
</tr>
</tbody>
</table>

<p>In conclusion, naive approaches to estimating causal effects from non-experimental data are dangerous. However, strategies for making causal estimates lying along a continuum from strongest to weakest, and researchers can discover fair comparisons within non-experimental data. The growth of always-on, big data systems increases our ability to effectively use two existing methods: natural experiments and matching.</p>

<h2 id="2-5-conclusion">2.5 Conclusion</h2>

<p>Big data is everywhere, but using it and other forms of observational data for social research is difficult. In my experience there is something like a no free lunch property for research: if you don’t put in a lot of work collecting data, then you are probably going to have to put in a lot of work analyzing your data or in thinking about what is in an interesting question to ask of the data. Based on the ideas in this chapter, I think that there are three main ways that big data sources will be most valuable for social research:</p>

<ul>
<li>empirically adjudicating between competing theoretical predictions. Examples of this kind of work include Farber (2015) (New York Taxi drivers) and King, Pan, and Roberts (2013) (Censorship in China)</li>
<li>improved social measurement for policy through nowcasting. An example of this kind of work is Ginsberg et al. (2009) (Google Flu Trends).</li>
<li>estimating causal effects with natural experiments and matching. Examples of this kind of work. Mas and Moretti (2009) (peer effects on productivity) and Einav et al. (2015) (effect of starting price on auctions at eBay).</li>
</ul>

<p>Many important questions in social research could be expressed as one of these three. However, these approaches generally require researchers to bring a lot to the data. What makes Farber (2015) interesting is the theoretical motivation for the measurement. This theoretical motivation comes from outside the data. Thus, for those who are good at asking certain types of research questions, big data sources can be very fruitful.</p>

<p>Finally, rather than theory-driven empirical research (which has been the focus on this chapter), we can flip the script and create empirically-driven theorizing. That is, through the careful accumulation of empirical facts, patterns, and puzzles, we can build new theories.</p>

<p>This alternative, data-first approach to theory is not new, and it was most forcefully articulated by Glaser and Strauss (1967) with their call for grounded theory. This data-first approach, however, does not imply “the end of theory,” as has been claimed by much of the journalism around research in the digital age (Anderson 2008). Rather, as the data environment changes, we must expect a re-balancing in the relationship between theory and data. In a world where data collection was expensive, it makes sense to only collect the data that theories suggest will be the most useful. But, in a world where enormous amounts of data are already available for free, it makes sense to also try a data-first approach (Goldberg 2015).</p>

<p>As I have shown in this chapter, researchers can learn a lot by watching people. In the next three chapters, I’ll describe how we can learn more and different things if we tailor our data collection and interact with people more directly by asking them questions (Chapter 3), running experiments (Chapter 4), and even involving them in the research process directly (Chapter 5).</p>

<h2 id="technical-appendix">Technical appendix</h2>

<p>In the main text, I discussed making causal claims from non-experimental data using natural experiments and matching. In this appendix, I will introduce the potential outcomes model, and define more precisely the conditions that are required for causal inference from observational data. This chapter will draw on Morgan and Winship (2014) and Imbens and Rubin (2015).</p>

<h2 id="further-commentary">Further commentary</h2>

<blockquote>
<p>This section is designed to be used as a reference, rather than to be read as a narrative.</p>
</blockquote>

<p><strong>Introduction (Section 2.1)</strong></p>

<p>One kind of observing that is not included in this chapter is ethnography. For more on ethnography in digital spaces see Boellstorff et al. (2012), and for more on ethnography in mixed digital and physical spaces see Lane (2016).</p>

<p><strong>Big data (Section 2.2)</strong></p>

<p>When you are repurposing data, there are two mental tricks that can help you understand the possible problems that you might encounter. First, you can try to imagine the ideal dataset for your problem and the compare that to the dataset that you are using. How are they similar and how are they different? If you didn’t collect your data yourself, there are likely to be difference between what you want and what you have. But, you have to decide if these differences are minor or major.</p>

<p>Second, remember that someone created and collected your data for some reason. You should try to understand their reasoning. This kind of reverse-engineering can help you identify possible problems and biases in your repurposed data.</p>

<p>There is no single consensus definition of “big data”, but many definitions seem to focus on the 3 Vs: volume, variety, and velocity (e.g., Japec et al. (2015)). Rather than focusing on the characteristics of the data, my definition focuses more on why the data was created.</p>

<p>My inclusion of government administrative data inside the category of big data is a bit unusually. Others who have made this case, include Legewie (2015), Connelly et al. (2016), and Einav and Levin (2014). For more about the value of government administrative data for research, see Card et al. (2010), Taskforce (2012), and Grusky, Smeeding, and Snipp (2015).</p>

<p>For a view of administrative research from inside the government statistical system, particularly the US Census Bureau, see Jarmin and O’Hara (2016). For a book length treatment of the administrative records research at Statistics Sweden, see Wallgren and Wallgren (2007).</p>

<p>In the chapter, I briefly compared a traditional survey such as the General Social Survey (GSS) to a social media data source such as Twitter. For a thorough and careful comparison between traditional surveys and social media data, see Schober et al. (2016).</p>

<p>Common characteristics of big data (Section 2.3)
These 10 characteristics of big data have been described in a variety of different ways by a variety of different authors. Writing that influenced my thinking on these issues include: Lazer et al. (2009), Groves (2011), Howison, Wiggins, and Crowston (2011), boyd and Crawford (2012), Taylor (2013), Mayer-Schönberger and Cukier (2013), Golder and Macy (2014), Ruths and Pfeffer (2014), Tufekci (2014), Sampson and Small (2015), Lewis (2015), Lazer (2015), Horton and Tambe (2015), Japec et al. (2015), and Goldstone and Lupyan (2016).</p>

<p>Throughout this chapter, I’ve used the term digital traces, which I think is relatively neutral. Another popular term for digital traces is digital footprints (Golder and Macy 2014), but as Hal Abelson, Ken Ledeen, and Harry Lewis (2008) point out, a more appropriate term is probably digital fingerprints. When you create footprints, you are aware of what is happening and your footprints cannot generally be traced to you personally. The same is not true for your digital traces. In fact, you are leaving traces all the time about which you have very little knowledge. And, although these traces don’t have your name on them, they can often be linked back to you. In other words, they are more like fingerprints: invisible and personally identifying.</p>

<p><strong>Big</strong></p>

<p>For more on why large datasets, render statistical tests problematic, see Lin, Lucas, and Shmueli (2013) and McFarland and McFarland (2015). These issues should lead researchers to focus on practical significance rather than statistical significance.</p>

<p><strong>Always-on</strong></p>

<p>When considering always-on data, it is important to consider whether you are comparing the exact same people over time or whether you are comparing some changing group of people; see for example, Diaz et al. (2016).</p>

<p><strong>Non-reactive</strong></p>

<p>A classic book on non-reactive measures is Webb et al. (1966). The examples in the book pre-date the digital age, but they are still illuminating. For examples of people changing their behavior because of the presence of mass surveillance, see Penney (2016) and Brayne (2014).</p>

<p><strong>Incomplete</strong></p>

<p>For more on record linkage, see Dunn (1946) and Fellegi and Sunter (1969) (historical) and Larsen and Winkler (2014) (modern). Similar approached have also been developed in computer science under the names such as data deduplication, instance identification, name matching, duplicate detection, and duplicate record detection (Elmagarmid, Ipeirotis, and Verykios 2007). There are also privacy preserving approaches to record linkage which do not require the transmission of personally identifying information (Schnell 2013). Facebook also has developed a proceed to link their records to voting behavior; this was done to evaluate an experiment that I’ll tell you about in Chapter 4 (Bond et al. 2012; Jones et al. 2013).</p>

<p>For more on construct validity, see Shadish, Cook, and Campbell (2001), Chapter 3.</p>

<p><strong>Inaccessible</strong></p>

<p>For more on the AOL search log debacle, see Ohm (2010). I offer advice about partnering with companies and governments in Chapter 4 when I describe experiments. A number of authors have expressed concerns about research that relies on inaccessible data, see Huberman (2012) and boyd and Crawford (2012).</p>

<p>One good way for university researchers to acquire data access is to work at a company as an intern or visiting researcher. In addition to enabling data access, this process will also help the researcher learn more about how the data was created, which is important for analysis.</p>

<p><strong>Non-representative</strong></p>

<p>Non-representativeness is a major problem for researchers and governments who wish to make statements about an entire population. This is less of concern for companies that are typically focused on their users. For more on how Statistics Netherlands considers the issue of non-representativeness of business big data, see Buelens et al. (2014).</p>

<p>In Chapter 3, I’ll describe sampling and estimation in much greater detail. Even if data are non-representative, under certain conditions, they can be weighted to produce good estimates.</p>

<p><strong>Drifting</strong></p>

<p>System drift is very hard to see from the outside. However, the MovieLens project (discussed more in Chapter 4) has been run for more than 15 years by an academic research group. Therefore, they have documented and shared information about the way that the system has evolved over time and how this might impact analysis (Harper and Konstan 2015).</p>

<p>A number of scholars have focused on drift in Twitter: Liu, Kliman-Silver, and Mislove (2014) and Tufekci (2014).</p>

<p><strong>Algorithmically confounded</strong></p>

<p>I first heard the term “algorithmically confounded” used by Jon Kleinberg in a talk. The main idea behind performativity is that some social science theories are “engines not cameras” (Mackenzie 2008). That is, they actually shape the world rather than just capture it.</p>

<p><strong>Dirty</strong></p>

<p>Governmental statistical agencies call data cleaning, statistical data editing. De Waal, Puts, and Daas (2014) describe statistical data editing techniques developed for survey data and examine to which extent they are applicable to big data sources, and Puts, Daas, and Waal (2015) presents some of the same ideas for a more general audience.</p>

<p>For some examples of studies focused on spam in Twitter, Clark et al. (2016) and Chu et al. (2012). Finally, Subrahmanian et al. (2016) describes the results of the DARPA Twitter Bot Challenge.</p>

<p><strong>Sensitive</strong></p>

<p>Ohm (2015) reviews earlier research on the idea of sensitive information and offers a multi-factor test. The four factors he proposes are: the probability of harm; probability of harm; presence of a confidential relationship; and whether the risk reflect majoritarian concerns.</p>

<p><strong>Counting things (Section 2.4.1)</strong></p>

<p>Farber’s study of taxis in New York was based on an earlier study by Camerer et al. (1997) that used three different convenience samples of paper trip sheets—paper forms used by drivers to record trip start time, end time, and fare. This earlier study found that drivers seemed to be target earners: they worked less on days where their wages were higher.</p>

<p>Kossinets and Watts (2009) was focused on the origins of homophily in social networks. See Wimmer and Lewis (2010) for a different approach to the same problem which uses data from Facebook.</p>

<p>In subsequent work, King and colleagues have further explored online censorship in China (King, Pan, and Roberts 2014; King, Pan, and Roberts 2016). For a related approach to measuring online censorship in China, see Bamman, O’Connor, and Smith (2012). For more on statistical methods like the one used in King, Pan, and Roberts (2013) to estimate the sentiment of the 11 million posts, see Hopkins and King (2010). For more on supervised learning, see James et al. (2013) (less technical) and Hastie, Tibshirani, and Friedman (2009) (more technical).</p>

<p><strong>Forecasting (Section 2.4.2)</strong></p>

<p>Forecasting is a big part of industrial data science (Mayer-Schönberger and Cukier 2013; Provost and Fawcett 2013). One type of forecasting that are commonly done by social researchers are demographic forecasting, for example Raftery et al. (2012).</p>

<p>Google Flu Trends was not the first project to use search data to nowcast influenza prevalence. In fact, researchers in the United States (Polgreen et al. 2008; Ginsberg et al. 2009) and Sweden (Hulth, Rydevik, and Linde 2009) have found that certain search terms (e.g., “flu”) predicted national public health surveillance data before it was released. Subsequently many, many other projects have tried to use digital trace data for disease surveillance detection, see Althouse et al. (2015) for a review.</p>

<p>In addition to using digital trace data to predict health outcomes, there has also been a huge amount of work using Twitter data to predict election outcomes; for reviews see Gayo-Avello (2011), Gayo-Avello (2013), Jungherr (2015) (Ch. 7), and Huberty (2015).</p>

<p>Using search data to predicting influenza prevalence and using Twitter data to predict elections are both examples of using some kind of digital trace to predict some kind of event in the world. There an enormous number of studies that have this general structure. Table 2.5 includes a few other examples.</p>

<p>Table 2.5: Partial list of studies use some digital trace to predict some event.</p>

<table>
<thead>
<tr>
<th>Digital trace</th>
<th align="right">Outcome</th>
<th align="right">Citation</th>
</tr>
</thead>

<tbody>
<tr>
<td>Twitter</td>
<td align="right">Box office revenue of movies in the US</td>
<td align="right">Asur and Huberman (2010)</td>
</tr>

<tr>
<td>Search logs</td>
<td align="right">Sales of movies, music, books, and video games in the US</td>
<td align="right">Goel et al. (2010)</td>
</tr>

<tr>
<td>Twitter</td>
<td align="right">Dow Jones Industrial Average (US stock market)</td>
<td align="right">Bollen, Mao, and Zeng (2011)</td>
</tr>
</tbody>
</table>

<p><strong>Approximating experiments (Section 2.4.3)</strong></p>

<p>The journal P.S. Political Science had a symposium on big data, causal inference, and formal theory, and Clark and Golder (2015) summarizes each contribution. The journal Proceedings of the National Academy of Sciences of the United States of America had a symposium on causal inference and big data, and Shiffrin (2016) summarizes each contribution.</p>

<p>In terms of natural experiments, Dunning (2012) provides an excellent book length treatment. For more on using the Vietnam draft lottery as a natural experiment, see Berinsky and Chatfield (2015). For machine learning approaches that attempt to automatically discover natural experiments inside of big data sources, see Jensen et al. (2008) and Sharma, Hofman, and Watts (2015).</p>

<p>In terms of matching, for an optimistic review, see Stuart (2010), and for a pessimistic review see Sekhon (2009). For more on matching as a kind of pruning, see Ho et al. (2007). For books that provide excellent treatments of matching, see Rosenbaum (2002), Rosenbaum (2009), Morgan and Winship (2014), and Imbens and Rubin (2015).</p>

<h2 id="activities">Activities</h2>

<p>Key:</p>

<ul>
<li>degree of difficulty: easy easy, medium medium, hard hard, very hard very hard</li>
<li>requires math (requires math)</li>
<li>requires coding (requires coding)</li>
<li>data collection (data collection)</li>
<li>my favorites (my favorite)</li>
</ul>

<ol>
<li><p>[medium, my favorite] Algorithmic confounding was a problem with Google Flu Trends. Read the paper by Lazer et al. (2014), and write a short, clear email to an engineer at Google explaining the problem and offering an idea of how to fix the problem.</p></li>

<li><p>[medium] Bollen, Mao, and Zeng (2011) claims that data from Twitter can be used to predict the stock market. This finding led to the creation of a hedge fund—Derwent Capital Markets—to invest in the stock market based on data collected from Twitter (Jordan 2010). What evidence would you want to see before putting your money in that fund?</p></li>

<li><p>[easy] While some public health advocates hail e-cigarettes as an effective aid for smoking cessation, others warn about the potential risks, such as the high-levels of nicotine. Imagine that a researcher decides to study public opinion toward e-cigarettes by collecting e-cigarettes-related Twitter posts and conducting sentiment analysis.</p>

<ul>
<li>What are the three possible biases that you are most worried about in this study?</li>
<li>Clark et al. (2016) ran just such a study. First, they collected 850,000 tweets that used e-cigarette-related keywords from January 2012 through December 2014. Upon closer inspection, they realized that many of these tweets were automated (i.e., not produced by humans) and many of these automated tweets were essentially commercials. They developed a Human Detection Algorithm to separate automated tweets from organic tweets. Using this Human Detect Algorithm they found that 80% of tweets were automated. Does this finding change your answer to part (a)?</li>
<li>When they compared the sentiment in organic and automated tweets they found that the automated tweets are more positive than organic tweets (6.17 versus 5.84). Does this finding change your answer to (b)?</li>
</ul></li>

<li><p>[easy] In November 2009, Twitter changed the question in the tweet box from “What are you doing?” to “What’s happening?” (<a href="https://blog.twitter.com/2009/whats-happening" target="_blank">https://blog.twitter.com/2009/whats-happening</a>).</p>

<ul>
<li>How do you think the change of prompts will affect who tweet and/or what they tweet?</li>
<li>Name one research project for which you would prefer the prompt “What are you doing?” Explain why.</li>
<li>Name one research project for which you would prefer the prompt “What’s happening?” Explain why.</li>
</ul></li>

<li><p>[medium] Kwak et al. (2010) analyzed 41.7 million user profiles, 1.47 billion social relations, 4262 trending topics, and 106 million tweets between June 6th and June 31st, 2009. Based on this analysis they concluded that Twitter serves more as a new medium of information sharing than a social network.</p>

<ul>
<li>Considering Kwak et al’s finding, what type of research would you do with Twitter data? What type of research would you not do with Twitter data? Why?</li>
<li>In 2010, Twitter added a Who To Follow service making tailored suggestion to users. Three recommendations are shown at a time on the main page. Recommendations are often drawn from one’s “friends-of-friends,” and mutual contacts are also displayed in the recommendation. Users can refresh to see a new set of recommendations or visit a page with a longer list of recommendations. Do you think this new feature would change your answer to part a)? Why or why not?</li>
<li>Su, Sharma, and Goel (2016) evaluated the effect of Who To Follow service and found that while users across the popularity spectrum benefited from the recommendations, the most popular users profited substantially more than average. Does this finding change your answer to part b)? Why or why not?</li>
</ul></li>

<li><p>[easy] “Retweets” are often used to measure influence and spread of influence on Twitter. Initially, users had to copy and paste the tweet they liked, tag the original author with his/her handle, and manually type “RT” before the tweet to indicate that it’s a retweet. Then, in 2009 Twitter added a “retweet” button. In June 2016, Twitter made it possible for users to retweet their own tweets (<a href="https://twitter.com/twitter/status/742749353689780224" target="_blank">https://twitter.com/twitter/status/742749353689780224</a>). Do you think these changes should affect how you use “retweets” in your research? Why or why not?</p></li>

<li><p>[medium, data collection, requires coding] Michel et al. (2011) constructed a corpus emerging from Google’s effort to digitize books. Using the first version of the corpus, which was published in 2009 and contained over 5 million digitized books, the authors analyzed word usage frequency to investigate linguistic changes and cultural trends. Soon the Google Books Corpus became a popular data source for researchers, and a 2nd version of the database was released in 2012.</p>

<p>However, Pechenick, Danforth, and Dodds (2015) warned that researchers need to fully characterize the sampling process of the corpus before using it for drawing broad conclusions. The main issue is that the corpus is library-like, containing one of each book. As a result, an individual, prolific author is able to noticeably insert new phrases into the Google Books lexicon. Moreover, scientific texts constitute an increasingly substantive portion of the corpus throughout the 1900s. In addition, by comparing two versions of the English Fiction datasets, Pechenick et al. found evidence that insufficient filtering was used in producing the first version. All of the data needed for activity is available here: <a href="http://storage.googleapis.com/books/ngrams/books/datasetsv2.html" target="_blank">http://storage.googleapis.com/books/ngrams/books/datasetsv2.html</a></p>

<p>In Michel et al.’s original paper (2011), they used the 1st version of the English data set, plotted the frequency of usage of the years “1880”, “1912” and “1973”, and concluded that “we are forgetting our past faster with each passing year” (Fig. 3A, Michel et al.). Replicate the same plot using 1) 1st version of the corpus, English dataset (same as Fig. 3A, Michel et al.)</p>

<p>Now replicate the same plot with the 1st version, English fiction dataset.</p>

<p>Now replicate the same plot with the 2nd version of the corpus, English dataset.</p>

<p>Finally, replicate the same plot with the 2nd version, English fiction dataset.</p>

<p>Describe the differences and similarities between these four plots. Do you agree with Michel et al.’s original interpretation of the observed trend? (Hint: c) and d) should be the same as Figure 16 in Pechenick et al.)</p>

<p>Now that you have replicated this one finding using different Google Books corpora, choose another linguistic change or cultural phenomena presented in Michel et al.’s original paper. Do you agree with their interpretation in light of the limitations presented in Pechenick et al.? To make your argument stronger, try replicate the same graph using different versions of data set as above.</p></li>

<li><p>[very hard, data collection, requires coding, my favorite] Penney (2016) explores whether the widespread publicity about NSA/PRISM surveillance (i.e., the Snowden revelations) in June 2013 is associated with a sharp and sudden decrease in traffic to Wikipedia articles on topics that raise privacy concerns. If so, this change in behavior would be consistent with a chilling effect resulting from mass surveillance. The approach of Penney (2016) is sometimes called an interrupted time series design and is related to the approaches in the chapter about approximating experiments from observational data (Section 2.4.3).</p>

<p>To choose the topic keywords, Penney referred to the list used by U.S. Department of Homeland Security for tracking and monitoring social media. The DHS list categorizes certain search terms into a range of issues, i.e. “Health Concern,” “Infrastructure Security,” and “Terrorism.” For the study group, Penney used the forty-eight keywords related to “Terrorism” (see Table 8 Appendix). He then aggregated Wikipedia article view counts on a monthly basis for the corresponding forty-eight Wikipedia articles over a thirty-two month period, from the beginning of January 2012 to the end of August 2014. To strengthen his argument, he also created several comparison groups by tracking article views on other topics.</p>

<p>Now, you are going to replicate and extend Penney (2016). All the raw data that you will need for this activity is available from Wikipedia (<a href="https://dumps.wikimedia.org/other/pagecounts-raw/" target="_blank">https://dumps.wikimedia.org/other/pagecounts-raw/</a>). Or you can get it from the R package wikipediatrend (Meissner and Team 2016). When you write-up your responses, please note which data source you used. (Note: This same activity also appears in Chapter 6)</p>

<p>Read Penney (2016) and replicate Figure 2 which shows the page views for “Terrorism”-related pages before and after the Snowden revelation. Interpret the findings.</p>

<p>Next, replicate Fig 4A, which compares the study group (“Terrorism”-related articles) with a comparator group using keywords categorized under “DHS &amp; Other Agencies” from the DHS list (see Appendix Table 10). Interpret the findings.</p>

<p>In part b) you compared the study group to one comparator group. Penney also compared to two other comparator groups: “Infrastructure Security”-related articles (Appendix Table 11) and popular Wikipedia pages (Appendix Table 12). Come up with an alternative comparator group, and test if the findings from part b) is sensitive to your choice of comparator group. Which choice of comparator group makes most sense? Why?</p>

<p>The author stated that keywords relating to “Terrorism” were used to select the Wikipedia articles because the U.S. government cited terrorism as a key justification for its online surveillance practices. As a check of these 48 “Terrorism”-related keywords, Penney (2016) also conducted a survey on MTurk asking respondents to rate each of keywords in terms of Government Trouble, Privacy-Sensitive, and Avoidance (Appendix Table 7 and 8). Replicate the survey on MTurk and compare your results.</p>

<p>Based on the results in part d) and your reading of the article, do you agree with the author’s choice of topic keywords in the study group? Why or why not? If not, what would you suggest instead?</p></li>

<li><p>[easy] Efrati (2016) reports, based on confidential information, that “total sharing” on Facebook had declined by about 5.5% year over year while “original broadcast sharing” was down 21% year over year. This decline was particularly acute with Facebook users under 30 years of age. The report attributed the decline to two factors. One is the growth in the number of “friends” people have on Facebook. The other is that some sharing activity has shifted to messaging and to competitors such as SnapChat. The report also revealed the several tactics Facebook had tried to boost sharing, including News Feed algorithm tweaks that make original posts more prominent, as well as periodical reminders of the original posts users “On This Day” several years ago. What implications, if any, does these findings have for researchers who want to use Facebook as a data source?</p></li>

<li><p>[medium] Tumasjan et al. (2010) reported that proportion of tweets mentioning a political party matched the proportion of votes that party received in the German parliamentary election in 2009 (Figure 2.9). In other words, it appeared that you could use Twitter to predict the election. At the time this study was published it was considered extremely exciting because it seemed to suggest a valuable use for a common source of big data.</p>

<p>Given the bad features of big data, however, you should immediately be skeptical of this result. Germans on Twitter in 2009 were quite a non-representative group, and supporters of one party might tweet about politics more often. Thus, it seems surprising that all the possible biases that you could imagine would somehow cancel out. In fact, the results in Tumasjan et al. (2010) turned out to be too good to be true. In their paper, Tumasjan et al. (2010) considered six political parties: Christian Democrats (CDU), Christian Social Democrats (CSU), SPD, Liberals (FDP), The Left (Die Linke), and the Green Party (Grüne). However, the most mentioned German political party on Twitter at that time was the Pirate Party (Piraten), a party that fights government regulation of the Internet. When the Pirate Party was included in the analysis, Twitter mentions becomes a terrible predictor of election results (Figure 2.9) (Jungherr, Jürgens, and Schoen 2012).</p></li>
</ol>

<p><img src="http://oaf2qt3yk.bkt.clouddn.com/e5b24f95166fc15fa7aad386ff75fd2b.png" alt="fig2.9" /></p>

<pre><code>Figure 2.9: Twitter mentions appear to predict the results of the 2009 German election (Tumasjan et al. 2010), but this result turns out to depend on some arbitrary and unjustified choices (Jungherr, Jürgens, and Schoen 2012).
</code></pre>

<p>Subsequently, other researchers around the world have used fancier methods—such as using sentiment analysis to distinguish between positive and negative mentions of the parties—in order to improve the ability of Twitter data to predict a variety of different types of elections (Gayo-Avello 2013; Jungherr 2015, Ch. 7.). Here’s how Huberty (2015) summarized the results of these attempts to predict elections:</p>

<p>“All known forecasting methods based on social media have failed when subjected to the demands of true forward-looking electoral forecasting. These failures appear to be due to fundamental properties of social media, rather than to methodological or algorithmic difficulties. In short, social media do not, and probably never will, offer a stable, unbiased, representative picture of the electorate; and convenience samples of social media lack sufficient data to fix these problems post hoc.”</p>

<p>Read some of the research that lead Huberty (2015) to that conclusion, and write a one page memo to a political candidate describing if and how Twitter should be used to forecast elections.</p>

<ol>
<li><p>[medium] What is the difference between a sociologist and a historian? According to Goldthorpe (1991), the main difference between a sociologist and a historian is control over data collection. Historians are forced to use relics whereas sociologists can tailor their data collection to specific purposes. Read Goldthorpe (1991). How is the difference between sociology and history related to the idea of Custommades and Readymades?</p></li>

<li><p>[hard] Building on the previous question, Goldthorpe (1991) drew a number of critical responses, including one from Nicky Hart (1994) that challenged Goldthorpe’s devotion to tailor made data. To clarify the potential limitations of tailor-made data, Hart described the Affluent Worker Project, a large survey to measure the relationship between social class and voting that was conducted by Goldthorpe and colleagues in the mid-1960s. As one might expect from a scholar who favored designed data over found data, the Affluent Worker Project collected data that was tailored to address a recently proposed theory about the future of social class in an era of increasing living standards. But, Goldthorpe and colleagues somehow “forgot” to collect information about the voting behavior of women. Here’s how Nicky Hart (1994) summaries the whole episode:</p>

<p>“&hellip; it [is] difficult to avoid the conclusion that women were omitted because this ‘tailor made’ dataset was confined by a paradigmatic logic which excluded female experience. Driven by a theoretical vision of class consciousness and action as male preoccupations &hellip; , Goldthorpe and his colleagues constructed a set of empirical proofs which fed and nurtured their own theoretical assumptions instead of exposing them to a valid test of adequacy.”</p>

<p>Hart continued:</p>

<p>“The empirical findings of the Affluent Worker Project tell us more about the masculinist values of mid-century sociology than they inform the processes of stratification, politics and material life.”</p>

<p>Can you think of other examples where tailor-made data collection has the biases of the data collector built into it? How does this compare to algorithmic confounding? What implications might this have for when researchers should use Readymades and when they should use Custommades?</p></li>

<li><p>[medium] In this chapter, I contrasted data collected by researchers for researchers with administrative records created by companies and governments. Some people call these administrative records “found data,” which they contrast with “designed data.” It is true that administrative records are found by researchers, but they are also highly designed. For example, modern tech companies spend enormous amounts of time and resources to collect and curate their data. Thus, these administrative records are both found and designed, it just depends on your perspective (Figure 2.10).</p></li>
</ol>

<p><img src="http://oaf2qt3yk.bkt.clouddn.com/ea3b7813b00b8b90a27151f5f102f13a.png" alt="fig2.10" /></p>

<p>Figure 2.10: The picture is both a duck and a rabbit; what you see depends on your perspective. Government and business administrative records are both found and designed; what you see depends on your perspective. For example, the call data records collected by a cell phone company are found data from the perspective of a researcher. But, these exact same records are designed data perspective of someone working in the billing department of the phone company. Source: Wikimedia Commons</p>

<p>Provide an example of data source where seeing it both as found and designed is helpful when using that data source for research.</p>

<ol>
<li>[easy] In a thoughtful essay, Christian Sandvig and Eszter Hargittai (2015) describe two kinds of digital research, where the digital system is “instrument” or “object of study.” An example of the first kind of study is where Bengtsson and colleagues (2011) used mobile phone data to track migration after the earthquake in Haiti in 2010. An example of the second kind is where Jensen (2007) studies how the introduction of mobile phones throughout Kerala, India impacted the functioning of the market for fish. I find this helpful because it clarifies that studies using digital data sources can have quite different goals even if they are using the same kind of data source. In order to further clarify this distinction, describe four studies that you’ve seen: two that use a digital system as an instrument and two that use a digital system as an object of study. You can use examples from this chapter if you want.</li>
</ol>

<h1 id="3-asking-questions">3 Asking questions</h1>

<p><a href="http://www.bitbybitbook.com/en/asking-questions/" target="_blank">http://www.bitbybitbook.com/en/asking-questions/</a></p>

<h2 id="3-1-introduction">3.1 Introduction</h2>

<p>Researchers who study dolphins can’t ask them questions. So, dolphin researchers are forced to study behavior.Researchers who study humans, on the other hand, should take advantage of the fact that our participants can talk. Asking people questions has been an important part of social research for a long time, and the digital age both enables and requires certain changes in survey research. Despite the pessimism that some survey researchers currently feel, I expect that the digital age is going to be a golden age of survey research.</p>

<p>The history of survey research can be roughly divided into three overlapping eras, separated by two contested transitions (Groves 2011; Converse 1987). Right now we are in a period of transition between the second and third eras, but the first and the second eras—as well as the transition between them—provide insight into the future of survey research.</p>

<p>During the first era of survey research, roughly 1930 - 1960, developments in scientific sampling and questionnaire design gradually resulted in a modern understanding of survey research. The first era of survey research was characterized by area probability sampling and face-to-face interviews.</p>

<p>Then, a technological development—the widespread diffusion of landline phones in wealthy countries—eventually led to the second era of survey research. This second era, roughly from 1960 - 2000, was characterized by random digit dialing (RDD) probability sampling and telephone interviews. The change from the first era to the second era resulted in major increases in efficiency and decreases in cost. Many researchers regard this second era as the golden age of survey research.</p>

<p>Now, another technological development—the digital age—will eventually bring us to a third era of survey research. This transition is being driven by both push and pull factors. In part, researchers are being forced to change because approaches from the second era are breaking down in the digital age (Meyer, Mok, and Sullivan 2015). For example, more and more homes do not have landline telephones and non-response rates—respondents who are sampled but do not participate in surveys—have been increasing (Council 2013). Simultaneous with this breakdown of second-era approaches to sampling and interviewing, there is increasing availability of big data sources (see Chapter 2) that appears to threaten to replace surveys. In addition to these push factors, there are also pull factors: the third era approaches offer incredible opportunities, as I’ll show in this chapter. Although things are not totally settled yet, I expect that the third era of survey research will be characterized by non-probability sampling and computer-administered interviews. Further, although the earlier eras were characterized by their approaches to sampling and interviewing, I expect that the third era of survey research will also be characterized by the linkage of surveys with big data sources (Table 3.1).</p>

<p>Table 3.1: Three eras of survey research. This chapter will focus on the third era of survey research: non-probability sampling, computer-administered interviews, and surveys linked to other data.</p>

<p>|Time   |Sampling   |Interviewing|  Data environment|
|&ndash;|:&ndash;|:&ndash;|:&ndash;|
|First era| 1930 - 1960|    Area probability sampling|  Face-to-face    |Stand-alone surveys|
|Second era |1960 - 2000    |Random digit dialing (RDD) probability sampling|   Telephone   |Stand-alone surveys|
|Third era  |2000 - present|    Non-probability sampling    Computer-administered   |Surveys linked to other data|</p>

<p>The transition between the second and third eras of survey research has not been completely smooth, and there have been fierce debates about how researchers should proceed. Looking back on the transition between the first and second eras, I think there is one key insight for us now: the beginning is not the end. That is, initially many second-era methods were ad-hoc and did not work very well. But, through hard work, researchers solved these problems, and second-era approaches eventually were better than first-era approaches. For example, researchers had been doing telephone random digit dialing for many years before Mitofsky and Waksberg developed a random digit dialing sampling method that had good practical and theoretical properties (Waksberg 1978; Brick and Tucker 2007). Thus, we should not confuse the current state of third-era approaches with their ultimate outcomes. The history of survey research makes clear that the field evolves, driven by changes in technology and society. There is no way to stop that evolution. Rather, we should embrace it, while continuing to draw wisdom from earlier eras. In fact, I believe that the digital age will be the most exciting age yet for asking people questions.</p>

<p>The remainder of the chapter begins by arguing that big data sources will not replace surveys and that the abundance of data increases—not decreases—the value of surveys (Section 3.2). Given that motivation, I’ll summarize the total survey error framework (Section 3.3) that was developed during the first two eras of survey research. This framework enables us to understand new approaches to representation—in particular, non-probability samples (Section 3.4)—and new approaches to measurement—in particular, new ways of asking questions to respondents (Section 3.5). Finally, I’ll describe two research templates for linking survey data to big data sources (Section 3.6).</p>

<h2 id="3-2-asking-vs-observing">3.2 Asking vs. observing</h2>

<blockquote>
<p>We are always going to need to ask people questions.</p>
</blockquote>

<p>Given that more and more of our behavior is capture in government and business administrative data, some people might think that asking questions is a thing of the past. But, its not that simple. It is certainly true that researchers will ask less about behavior in the future, but, as I discussed in Chapter 2, there are real problems with the accuracy, completeness, and accessibility of many big data sources. Therefore, I expect that the problems with these data sources mean that researchers will continue to ask respondents about their behavior for the foreseeable future.</p>

<p>In addition to these practical reasons, there is also a more fundamental reason to ask: behavior data—even perfect behavior data—is limited. Some of the most important social outcomes and predictors are internal states, such as emotions, knowledge, expectations, and opinions. Internal states exist only inside people’s heads, and sometimes the best way to learn about internal states is to ask.</p>

<p>The practical and fundamental limitations of big data sources, and how they can be overcome with surveys, are illustrated by Moira Burke and Robert Kraut’s (2014) research on how the strength of friendships was impacted by interaction on Facebook. At the time, Burke was working at Facebook so she had complete access to one of the most massive and detailed records of human behavior ever created. But, even still, Burke and Kraut had to use surveys in order to answer their research question. Their outcome of interest—how close the respondent feels to specific friends—is an internal state that only exists inside the respondent’s head. Further, in addition to using a survey to collect their outcome of interest, Burke and Kraut also had to use a survey to learn about other potentially confounding factors. In particular, they wanted to separate the impact of communicating on Facebook from communication through other channels (e.g., email, phone, face-to-face). Even though interactions through email and phone are automatically recorded, these traces were not available to Burke and Kraut. Combining their survey data about friendship strength and non-Facebook interaction with the Facebook log data, Burke and Kraut concluded that <strong>communication via Facebook did in fact lead to increased feelings of closeness</strong>.</p>

<p>As the work of Burke and Kraut illustrates, big data sources will not eliminate the need to ask people questions. In fact, I would draw the opposite lesson from this study: big data actually increases the value of asking questions, as I will show throughout this chapter. Therefore, the best way to think about the relationship between asking and observing is that they are complements rather than substitutes; they are like peanut butter and jelly. When there is more peanut butter, people want more jelly; when there is more big data, people want more surveys.</p>

<h3 id="3-3-the-total-survey-error-framework">3.3 The total survey error framework</h3>

<blockquote>
<p>Total survey error = representation errors + measurement errors.</p>
</blockquote>

<p>There are many kinds of errors that can creep into estimates from surveys, and since the 1940s researchers have worked to systematically organize, understand, and reduce these errors. An important result from all of that effort is the total survey error framework (Groves et al. 2009; Weisberg 2005). The main insight from the total survey error framework is that problems can be grouped into two main buckets: problems related to who you talk to (representation) and problems related to what you learn from those conversations (measurement). For example, you might be interested in estimating attitudes about online privacy among adults living in France. Making these estimates requires two quite different types of inference. First, from the answers that respondents give, you have to infer their attitudes about online privacy. Second, from the inferred attitudes among respondents, you must infer the attitudes in the population as a whole. The first type of inference is the domain of psychology and cognitive science; and the second type of inference is the domain of statistics. A perfect sampling scheme with bad survey questions will produce bad estimates, and a bad sampling scheme with perfect survey questions will also produce bad estimates. Good estimates requires sound approaches to measurement and representation. Given that background, next, I’ll review how survey researchers have thought about representation and measurement in the past. I expect that much of this material will be review to social scienitsts, but it may be new to some data scientists. Then, I’ll show you how those ideas guide digital age survey research.</p>

<h3 id="3-3-1-representation">3.3.1 Representation</h3>

<blockquote>
<p>Representation is about making inferences from your respondents to your target population.</p>
</blockquote>

<p>In order to understand the kind of errors that can happen when inferring from respondents to the larger population, let’s consider the Literary Digest straw poll that tried to predict the outcome of the 1936 US Presidential election. Although it was more than 75 years ago, this debacle still has an important lesson to teach researchers today.</p>

<p>Literary Digest was a popular general-interest magazine, and starting in 1920 they began running straw polls to predict the outcomes of Presidential Elections. To make these predictions they would send ballots to lots of people, and then simply tally up the ballots that were returned; Literary Digest proudly reported that the ballots they received were neither “weighted, adjusted, nor interpreted.” This procedure correctly predicted the winner of the elections in 1920, 1924, 1928 and 1932. In 1936, in the midst of the Great Depression, Literary Digest sent out ballots to 10 million people, whose names predominately came from telephone directories and automobile registration records. Here’s how they described their methodology:</p>

<p>“THE DIGEST’s smooth-running machine moves with the swift precision of thirty years’ experience to reduce guesswork to hard facts &hellip; .This week 500 pens scratched out more than a quarter of a million addresses a day. Every day, in a great room high above motor-ribboned Fourth Avenue, in New York, 400 workers deftly slide a million pieces of printed matter—enough to pave forty city blocks—into the addressed envelops [sic]. Every hour, in THE DIGEST’S own Post Office Substation, three chattering postage metering machines sealed and stamped the white oblongs; skilled postal employees flipped them into bulging mailsacks; fleet DIGEST trucks sped them to express mail-trains &hellip; Next week, the first answers from these ten million will begin the incoming tide of marked ballots, to be triple-checked, verified, five-times cross-classified and totaled. When the last figure has been totted and checked, if past experience is a criterion, the country will know to within a fraction of 1 percent the actual popular vote of forty million [voters].” (August 22, 1936)
The Digest’s fetishization of size is instantly recognizable to any “big data” researcher today. Of the 10 million ballots distributed, an amazing 2.4 million ballots were returned—that’s roughly 1,000 times larger than modern political polls. From these 2.4 million respondents the verdict was clear: Literary Digest predicted that the challenger Alf Landon was going to defeat the incumbent Franklin Roosevelt. But, in fact, the exact opposite happened. Roosevelt defeated Landon in a landslide. How could Literary Digest go wrong with so much data? Our modern understanding of sampling makes Literary Digest’s errors clear and helps us avoid making similar errors in the future.</p>

<p>Thinking clearly about sampling requires us to consider four different groups of people (Figure 3.1). The first group of people is the target population; this is the group that the research defines as the population of interest. In the case of Literary Digest the target population was voters in the 1936 Presidential Election. After deciding on a target population, a researcher next needs to develop a list of people that can be used for sampling. This list is called a sampling frame and the population on the sampling frame is called the frame population. In the case of Literary Digest the frame population was the 10 million people whose names came predominately from telephone directories and automobile registration records. Ideally the target population and the frame population would be exactly the same, but in practice this is often not the case. Differences between the target population and frame population are called coverage error. Coverage error does not, by itself guarantee problems. But, if the people in the frame population are systematically different from people not in the frame population there will be coverage bias. Coverage error was the first of the major flaws with the Literary Digest poll. They wanted to learn about voters—that was their target population—but they constructed a sampling frame predominately from telephone directories and automobile registries, sources that over-represented wealthier Americans who were more likely to support Alf Landon (recall that both of these technologies, which are common today, were relatively new at the time and that the US was in the midst of the Great Depression).</p>

<p>Figure 3.1: Representation errors.</p>

<p><img src="http://oaf2qt3yk.bkt.clouddn.com/21069033ac1b35779e0a1b3b7cc7000e.png" alt="fig3.1" /></p>

<p>After defining the frame population, the next step is for a researcher to select the sample population; these are the people that the researcher will attempt to interview. If the sample has different characteristics than the frame population, then we can introduce sampling error. This is the kind of error quantified in the margin of error that usually accompanies estimates. In the case of the Literary Digest fiasco, there actually was no sample; they attempted to contact everyone in the frame population. Even though there was no sampling error, there was obviously still error. This clarifies that the margins of errors that are typically reported with estimates from surveys are usually misleadingly small; they don’t include all sources of error.</p>

<p>Finally, a researcher attempts to interview everyone in the sample population. Those people that are successfully interviewed are called respondents. Ideally, the sample population and the respondents would be exactly the same, but in practice there is non-response. That is, people who are selected in the sample refuse to participate. If the people who respond are different from those who don’t respond, then there can be non-response bias. Non-response bias was the second main problem with the Literary Digest poll. Only 24% of the people who received a ballot responded, and it turned out that people who supported Landon were more likely to respond.</p>

<p>Beyond just being an example to introduce the ideas of representation, the Literary Digest poll is an oft-repeated parable, cautioning researchers about the dangers of haphazard sampling. Unfortunately, I think that the lesson that many people draw from this story is the wrong one. The most common moral of the story is that researchers can’t learn anything from non-probability samples (i.e., samples without strict probability-based rules for selecting participants). But, as I’ll show later in this chapter, that’s not quite right. Instead, I think there are really two morals to this story; morals that are as true today as they were in 1936. First, a large amount of haphazardly collected data will not guarantee a good estimate. Second, researchers need to account for how their data was collected when they are making estimates from it. In other words, because the data collection process in the Literary Digest poll was systematically skewed toward some respondents, researchers need to use a more complex estimation process that weights some respondents more than others. Later in this chapter, I’ll show you one such weighting procedure—post-stratification—that can enable you to make better estimates with non-probability samples.</p>

<h3 id="3-3-2-measurement">3.3.2 Measurement</h3>

<blockquote>
<p>Measurement is about making inferences from what your respondents say to what your respondents think and do.</p>
</blockquote>

<p>The second category of the total survey error framework is measurement; it deals with how we can make inferences from the answers that respondents give to our questions. It turns out that the answers we receive, and therefore the inferences we make, can depend critically—and in sometimes surprising ways—on exactly how we ask. Perhaps nothing illustrates this important point better than a joke in the wonderful book Asking Questions by Norman Bradburn, Seymour Sudman, and Brian Wansink (2004):</p>

<p>Two priests, a Dominican and a Jesuit, are discussing whether it is a sin to smoke and pray at the same time. After failing to reach a conclusion, each goes off to consult his respective superior. The Dominican says, “What did your superior say?”
  The Jesuit responds, “He said it was alright.”
  “That’s funny” the Dominican replies, “My supervisor said it was a sin.”
  The Jesuit said, “What did you ask him?” The Dominican replies, “I asked him if it was alright to smoke while praying.” “Oh” said the Jesuit, “I asked if it was OK to pray while smoking.”
  There are many examples of anomalies like the one experienced by the two priests. In fact, the very issue at the root of this joke has a name in the survey research community: question form effects (Kalton and Schuman 1982). To see how question form effects might impact real surveys, consider these two very similar looking survey questions:</p>

<ul>
<li>“How much do you agree with the following statement: Individuals are more to blame than social conditions for crime and lawlessness in this country.”</li>
<li>“How much do you agree with the following statement: Social conditions are more to blame than individuals for crime and lawlessness in this country.”</li>
</ul>

<p>Although both questions appear to measure the same thing, they produced different results in a real survey experiment (Schuman and Presser 1996). When asked one way, about 60% of respondents reported that individuals were more to blame for crime, but when asked the other way about 60% reported that social conditions were more to blame (Figure 3.2). In other words, the small difference between the two questions could lead researchers to a different conclusion.</p>

<p><img src="http://oaf2qt3yk.bkt.clouddn.com/db7d00bba31f46cb720768d8e699ff0e.png" alt="fig3.2" /></p>

<p>Figure 3.2: Results from a survey experiment , Table 8.1(Schuman and Presser 1996, Table 8.1). Researchers can get different answers depending on exactly how they asked the question. This is an example of a question form effect (Kalton and Schuman 1982).</p>

<p>In addition to the structure of the question, respondents also can give different answers based on the specific words used. For example, in order to measure opinions about governmental priorities, respondents were read the following prompt:</p>

<blockquote>
<p>“We are faced with many problems in this country, none of which can be solved easily or inexpensively. I’m going to name some of these problems, and for each one I’d like you to tell me whether you think we’re spending too much money on it, too little money, or about the right amount.”
Next, half of the respondents were asked about “welfare” and half were asked about “aid for the poor.” While these might seem like two different phrases for the same thing, they elicited very different results (Figure 3.3); Americans report being much more supportive of “aid to the poor” than “welfare” (Smith 1987; Rasinski 1989; Huber and Paris 2013). While survey researchers consider these wording effects to be anomalies, they could also consider them research findings. That is, we have learned something about public opinion from this result.</p>
</blockquote>

<p><img src="http://oaf2qt3yk.bkt.clouddn.com/b6a97a16e7a5c9f34d19e59601ebef3c.png" alt="fig3.3" /></p>

<p>Figure 3.3: Results from Huber and Paris (2013). Respondents are much more supportive of aid to the poor than welfare. This is an example of a question wording effect whereby the answers that researchers receive depend on exactly which words they use in their questions.</p>

<p>As these examples about question form effects and wording effects show, the answers that researchers receive can be influenced in subtle ways based on how they ask their questions. This does not mean that surveys should not be used; often there is no choice. Rather, the examples illustrate that we should construct our questions carefully and we should not accept responses uncritically.</p>

<p>Most concretely, this means that if you are analyzing survey data collected by someone else, make sure that you have read the actual questionnaire. And, if you are creating your own questionnaire, I have three suggestions. First, I suggest you read more about questionnaire design (e.g., Bradburn, Sudman, and Wansink (2004)); there is more to it than I’ve been able to describe here. Second, I suggest that you copy—word for word—questions from high-quality surveys. Although this sounds like plagiarism, copying questions is encouraged in survey research (as long as you cite the original survey). If you copy questions from high-quality surveys, you can be sure that they have been tested and you can compare the responses to your survey to responses from some other survey. Finally, I suggest you pre-test your questions with some people from your frame population (Presser et al. 2004); my experience is that pre-testing always reveals surprising issues.</p>

<h3 id="3-3-3-cost">3.3.3 Cost</h3>

<blockquote>
<p>Surveys are not free, and this is a real constraint.</p>
</blockquote>

<p>So far, I’ve briefly reviewed the total survey error framework, which itself is the subject of book length treatments (Weisberg 2005; Groves et al. 2009). Although this framework is comprehensive, it generally causes researchers to leave out an important factor: cost. Although cost—which can be measured by either time or money—is rarely explicitly discussed by academic researchers, it is a real constraint that we ignore at our peril. In fact, the reason researchers interview samples of people rather than entire population is to save money. Thus, cost is fundamental to the process of survey research (Groves 2004). A single-minded devotion to minimizing error while completely ignoring costs is not always in our best interest.</p>

<p>The limitations of an obsession with reducing error are illustrated by the landmark study of Scott Keeter and colleagues (2000) on the effects of expensive field operations in order to reduce non-response in telephone surveys . Keeter and colleagues ran two simultaneous surveys, one using “Standard” procedures and one using “Rigorous” procedures. Although the “Rigorous” procedures did produce a lower rate of non-response, estimates from both samples were basically the same. However, the “Rigorous” procedures cost roughly twice as much and took 8 times as long. Are we better off with 2 reasonable surveys or 1 pristine survey? What about 10 reasonable surveys or 1 pristine survey? What about 100 reasonable surveys or 1 pristine survey? At some point cost advantages must outweigh vague, non-specific concerns about quality.</p>

<p>Many of the opportunities created by the digital age are not about creating estimates that obviously have lower error. Rather, these opportunities are about creating estimates cheaper and faster, but perhaps with errors that are currently higher or harder to measure. As many of the examples in this chapter will show, researchers who insist on a single-minded obsession with minimizing error at the expense of other dimensions of quality are going to miss out on exciting opportunities. Given this background about the total survey error framework, we will now turn to three main areas of the third era of survey research: new approaches to representation (Section 3.4), new approaches to measurement (Section 3.5), and new strategies for combining surveys with digital traces (Section 3.6).</p>

<h2 id="3-4-who-to-ask">3.4 Who to ask</h2>

<blockquote>
<p>Probability samples and non-probability samples are not that different in practice; in both cases, it’s all about the weights.</p>
</blockquote>

<p>Sampling is fundamental to survey research. Researchers almost never ask their questions to everyone in their target population. In this regard, surveys are not unique. Most research, in one way or another, involves sampling. Sometimes this sampling is done explicitly by the researcher; other times it happens implicitly. For example, a researcher that runs a laboratory experiment on undergraduate students in her university has also taken a sample. Thus, sampling is a problem that comes up throughout this book. In fact, one of the most common concerns that I hear about digital age sources of data is “they are not representative.” As we will see in this Section, this concern is both less serious and more subtle than many skeptics realize. In fact, I will argue that the whole concept of “representativeness” is not helpful for thinking about probability and non-probability samples. Instead, the key is to think about how the data was collected and how any biases in that data collection can be undone when making estimates.</p>

<p>Currently, the dominant theoretical approach to representation is probability sampling. When data are collected with a probability sampling method that has been perfectly executed, researchers are able to weight their data based on the way that they were collected to make unbiased estimates about the target population. However, perfect probability sampling basically never happens in the real world. There are typically two main problems 1) differences between the target population and the frame population and 2) non-response (these are exactly the problems that wrecked the Literary Digest poll). Thus, rather than thinking of probability sampling as a realistic model of what actually happens in the world, it is better to think of probability sampling as a helpful, abstract model, much like the way physicists think about a frictionless ball rolling down an infinitely long ramp.</p>

<p>The alternative to probability sampling is non-probability sampling. The main difference between probability and non-probability sampling is that with probability sampling everyone in the population has a known probability of inclusion. There are, in fact, many varieties of non-probability sampling, and these methods of data collection are becoming increasingly common in the digital age. But, non-probability sampling has a terrible reputation among social scientists and statisticians. In fact, non-probability sampling is associated with some of the most dramatic failures of survey researchers, such as the Literary Digest fiasco (discussed earlier) and the incorrect prediction about the US presidential elections of 1948 (“Dewey Defeats Truman”) (Mosteller 1949; Bean 1950; Freedman, Pisani, and Purves 2007).</p>

<p>However, the time is right to reconsider non-probability sampling for two reasons. First, as probability samples have become increasingly difficult to do in practice, the line between probability samples and non-probability samples is blurring. When there are high rates of non-response (as there are in real surveys now), the actual probability of inclusions for respondents are not known, and thus, probability samples and non-probability samples are not as different as many researchers believe. In fact, as we will see below, both approaches basically rely on the same estimation method: post-stratification. Second, there have been many developments in the collection and analysis of non-probability samples. These methods are different enough from the methods that caused problems in the past that I think it makes sense to think of them as “non-probability sampling 2.0.” We should not have an irrational aversion to non-probability methods because of errors that happened a long time ago.</p>

<p>Next, in order to make this argument more concrete, I’ll review standard probability sampling and weighting (Section 3.4.1). The key idea is that how you collected your data should impact how you make estimates. In particular, if everyone does not have the same probability of inclusion, then everyone should not have the same weight. In other words, if your sampling is not democratic, then your estimations should not be democratic. After reviewing weighting, I’ll describe two approaches to non-probability sampling: one that focuses on weighting to deal with the problem of haphazardly collected data (Section 3.4.2), and one that tries to place more control over how the data is collected (Section 3.4.3). The arguments in the main text will be explained below with words and pictures; readers who would like a more mathematical treatment should also see the technical appendix.</p>

<h3 id="3-4-1-probability-sampling-data-collection-and-data-analysis">3.4.1 Probability sampling: data collection and data analysis</h3>

<blockquote>
<p>Weights can undo distortions intentionally caused by the sampling process.</p>
</blockquote>

<p>Probability samples are those where all people have a known, non-zero probability of inclusion, and the simplest probability sampling design is simple random sampling where each person has equal probability of inclusion. When respondents are selected via simple random sampling with perfect execution (e.g., no coverage error and no non-response), then estimation is straightforward because the sample will—on average—be a miniature version of the population.</p>

<p>Simple random sampling is rarely used in practice, however. Rather, researchers intentionally select people with unequal probabilities of inclusion in order to reduce cost and increase accuracy. When researchers intentionally select people with different probabilities of inclusion, then adjustments are needed to undo the distortions caused by the sampling process. In other words, how we generalize from a sample depends on how the sample was selected.</p>

<p>For example, the Current Population Survey (CPS) is used by the US government to estimate the unemployment rate. Each month about 100,000 people are interviewed, either face-to-face or over the telephone, and the results are used to produce the estimated unemployment rate. Because the government wishes to estimate the unemployment rate in each state, it cannot do a simple random sample of adults because that would yield too few respondents in states with small populations (e.g., Rhode Island) and too many from states with large populations (e.g., California). Instead, the CPS samples people in different states at different rates, a process called stratified sampling with unequal probability of selection. For example, if the CPS wanted 2,000 respondents per state, then adults in Rhode Island would have about 30 times higher probability of inclusion than adults in California (Rhode Island: 2,000 respondents per 800,000 adults vs California: 2,000 respondents per 30,000,000 adults). As we will see later, this kind of sampling with unequal probability happens with online sources of data too, but unlike the CPS, the sampling mechanism is usually not known or controlled by the researcher.</p>

<p>Given its sampling design, the CPS is not directly representative of the US; it includes too many people from Rhode Island and too few from California. Therefore, it would be unwise to estimate the unemployment rate in the country with the unemployment rate in the sample. Instead of the sample mean, it is better to take a weighted mean, where the weights account for the fact that people from Rhode Island were more likely to be included than people from California. For example, each person from California would be upweighted— they would count more in the estimate—and each person from Rhode Island would be downweighted—they would count less in the estimate. In essence, you are given more voice to people that you are less likely to learn about.</p>

<p>This toy example illustrates an important but commonly misunderstood point: a sample does not need to be a miniature version of the population in order to produce good estimates. If enough is known about how the data was collected, then that information can be used when making estimates from the sample. The approach I’ve just described—and that I describe mathematically in the technical appendix—falls squarely within the classical probability sampling framework. Now, I’ll show how that same idea can be applied to non-probability samples.</p>

<h3 id="3-4-2-non-probability-samples-weighting">3.4.2 Non-probability samples: weighting</h3>

<blockquote>
<p>With non-probability samples, weights can undo distortions caused by the assumed sampling process.</p>
</blockquote>

<p>In the same way that researchers weight responses from probability samples, they can also weight responses from non-probability samples. For example, as an alternative to the CPS, imagine that you placed banner ads on thousands of websites to recruit participants for a survey to estimate the unemployment rate. Naturally, you would be skeptical that the simple mean of your sample would be a good estimate of the unemployment rate. Your skepticism is probably because you think that some people are more likely to complete your survey than others. For example, people who don’t spend a lot of time on the web are less likely to complete your survey.</p>

<p>As we saw in the last section, however, if we know how the sample was selected—as we do with probability samples—then we can undo distortions caused by the sampling process. Unfortunately, when working with non-probability samples, we don’t know how the sample was selected. But, we can make assumptions about the sampling process and then apply weighting in the same way. If these assumptions are correct, then the weighting will undo the distortions caused by the sampling process.</p>

<p>For example, imagine that in response to your banner ads, you recruited 100,000 respondents. However, you don’t believe that these 100,000 respondents are a simple random sample of American adults. In fact, when you compare your respondents to the US population, you find that people from some states (e.g., New York) are over-represented and that people from some states (e.g., Alaska) are under-represented. Thus, the unemployment rate of your sample is likely to be a bad estimate of the unemployment rate in the target population.</p>

<p>One way to undo the distortion that happened in the sampling process is to assign weights to each person; lower weights to people from states that are over-represented in the sample (e.g., New York) and higher weights to people from states that are under-represented in the sample (e.g., Alaska). More specifically, the weight for each respondent is related to their prevalence in your sample relative to their prevalence in the US population. This weighting procedure is called post-stratification, and the idea of weighing should remind you of the example in Section 3.4.1 where respondents from Rhode Island were given less weight than respondents from California. Post-stratification requires that you know enough to put your respondents into groups and to know the proportion of the target population in each group.</p>

<p>Although the weighting of the probability sample and of the non-probability sample are the same mathematically (see technical appendix), they work well in different situations. If the researcher has a perfect probability sample (i.e., no coverage error and no non-response), then weighting will produce unbiased estimates for all traits in all cases. This strong theoretical guarantee is why advocates of probability samples find them so attractive. On the other hand, weighting non-probability samples will only produce unbiased estimates for all traits if the response propensities are the same for everyone in each group. In other words, thinking back to our example, using post-stratification will produce unbiased estimates if everyone in New York has the same probability of participating and everyone in Alaska has the same probability of participating and so on. This assumption is called the homogeneous-response-propensities-within-groups assumption, and it plays a key role in knowing if post-stratification will work well with non-probability samples.</p>

<p>Unfortunately, in our example, the homogeneous-response-propensities-within-groups assumption seems unlikely to be true. That is, it seems unlikely that everyone in Alaska has the same probability of being in your survey. But, there are three important points to keep in mind about post-stratification, all of which make it seem more promising.</p>

<p>First, homogeneous-response-propensities-within-groups assumption becomes more plausible as the number of groups increases. And, researchers are not limited to groups just based on a single geographic dimension. For example, we could create groups based on state, age, sex, and level of education. It seems more plausible that there is homogeneous response propensities within the group of 18-29, female, college graduates living in Alaska than within the group of all people living in Alaska. Thus, as the number of groups used for post-stratification increases, the assumptions needed to support it become more reasonable. Given this fact, it seems like a researchers would want to create a huge number of groups for post-stratification. But, as the number of groups increases, researchers run into a different problem: data sparsity. If there are only a small number of people in each group, then the estimates will be more uncertain, and in the extreme case where there is a group that has no respondents, then post-stratification completely breaks down. There are two ways out of this inherent tension between the plausibility of homogeneous- response-propensity-within-groups assumption and the demand for reasonable sample sizes in each group. One approach is to move to a more sophisticated statistical model for calculating weights and the other is to collect a larger, more diverse sample, which helps ensure reasonable sample sizes in each group. And, sometimes researchers do both, as I’ll describe in more detail below.</p>

<p>A second consideration when working with post-stratification from non-probability samples is that the homogeneous-response-propensity-within-groups assumption is already frequently made when analyzing probability samples. The reason that this assumption is needed for probability samples in practice is that probability samples have non-response, and the most common method for adjusting for non-response is post-stratification as described above. Of course, just because many researchers make a certain assumption doesn’t mean that you should do it too. But, it does mean that when comparing non-probability samples to probability samples in practice, we must keep in mind that both depend on assumptions and auxiliary information in order to produce estimates. In most realistic settings, there is simply no assumption-free approach to inference.</p>

<p>Finally, if you care about one estimate in particular—in our example unemployment rate—then you need a condition weaker than homogeneous-response-propensity-within-groups assumption. Specifically, you don’t need to assume that everyone has the same response propensity, you only need to assume that there is no correlation between response propensity and unemployment rate within each group. Of course, even this weaker condition will not hold in some situations. For example, imagine estimating the proportion of Americans that do volunteer work. If people who do volunteer work are more likely to agree to be in a survey, then researchers will systematically over-estimate the amount of volunteering, even if they do post-stratification adjustments, a result that has been demonstrated empirically by Abraham, Helms, and Presser (2009).</p>

<p>As I said earlier, non-probability samples are viewed with great skepticism by social scientists, in part because of their role in some of the most embarrassing failures in the early days of survey research. A clear example of how far we have come with non-probability samples is the research of Wei Wang, David Rothschild, Sharad Goel, and Andrew Gelman that correctly recovered the outcome of the 2012 US election using a non-probability sample of American Xbox users—a decidedly non-random sample of Americans (Wang et al. 2015). The researchers recruited respondents from the XBox gaming system, and as you might expect, the Xbox sample skewed male and skewed young: 18 - 29 year olds make up 19% of the electorate but 65% of the Xbox sample and men make up 47% of the electorate and 93% of the Xbox sample (Figure 3.4). Because of these strong demographic biases, the raw Xbox data was a poor indicator of election returns. It predicted a strong victory for Mitt Romney over Barack Obama. Again, this is another example of the dangers of raw, unadjusted non-probability samples and is reminiscent of the Literary Digest fiasco.</p>

<p><img src="http://oaf2qt3yk.bkt.clouddn.com/e5c8d32a576e139c045eb6604302f8e1.png" alt="fig3.4" /></p>

<p>Figure 3.4: Demographics of respondents in Wang et al. (2015). Because respondents were recruited from XBox, they were more likely to be young and more likely to be male, relative to voters in the 2012 election.</p>

<p>However, Wang and colleagues were aware of these problems and attempted to weight the respondents to correct for the sampling process. In particular, they used a more sophisticated form of the post-stratification I told you about. It is worth learning a bit more about their approach because it builds intuition about post-stratification, and the particular version Wang and colleagues used is one of the most exciting approaches to weighting non-probability samples.</p>

<p>In our simple example about estimating unemployment in Section 3.4.1, we divided the population into groups based on state of residence. In contrast, Wang and colleagues divided the population into into 176,256 groups defined by: gender (2 categories), race (4 categories), age (4 categories), education (4 categories), state (51 categories), party ID (3 categories), ideology (3 categories) and 2008 vote (3 categories). With more groups, the researchers hoped that it would be increasingly likely that within each group, response propensity was uncorrelated with support for Obama. Next, rather than constructing individual-level weights, as we did in our example, Wang and colleagues used a complex model to estimate the proportion of people in each group that would vote for Obama. Finally, they combined these group estimates of support with the known size of each group to produce an estimated overall level of support. In other words, they chopped up the population into different groups, estimated the support for Obama in each group, and then took a weighted average of the group estimates to produce an overall estimate.</p>

<p>Thus, the big challenge in their approach is to estimate the support for Obama in each of these 176,256 groups. Although their panel included 345,858 unique participants, a huge number by the standards of election polling, there were many, many groups for which Wang and colleagues had almost no respondents. Therefore, to estimate the support in each group they used a technique called multilevel regression with post-stratification, which researchers affectionately call Mr. P. Essentially, to estimate the support for Obama within a specific group, Mr. P. pools information from many closely related groups. For example, consider the challenge of estimating the support for Obama among female, Hispanics, between 18-29 years old, who are college graduates, who are registered Democrats, who self-identify as moderates, and who voted for Obama in 2008. This is a very, very specific group, and it is possible that there is nobody in the sample with these characteristics. Therefore, to make estimates about this group, Mr. P. pools together estimates from people in very similar groups.</p>

<p>Using this analysis strategy, Wang and colleagues were able to use the XBox non-probability sample to very closely estimate the overall support that Obama received in the 2012 election (Figure 3.5). In fact their estimates were more accurate than an aggregate of public opinion polls. Thus, in this case, weighting—specifically Mr. P.—seems to do a good job correcting the biases in non-probability data; biases that are visible when you look at the estimates from the unadjusted Xbox data.</p>

<p><img src="http://oaf2qt3yk.bkt.clouddn.com/1511f22180cef2f8ac3f7e68de4e1ec1.png" alt="fig3.5" /></p>

<p>Figure 3.5: Estimates from Wang et al. (2015). Unadjusted XBox sample produced inaccurate estimates. But, the weighted XBox sample produced estimates that were more accurate than an average of probability-based telephone surveys.</p>

<p>There are two main lessons from the study of Wang and colleagues. First, unadjusted non-probability samples can lead to bad estimates; this is a lesson that many researchers have heard before. However, the second lesson is that non-probability samples, when weighted properly, can actually produce quite good estimates. In fact, their estimates were more accurate than the estimates from pollster.com, an aggregation of more traditional election polls.</p>

<p>Finally, there are important limitations to what we can learn from this one specific study. Just because post-stratification worked well in this particular case, there is no guarantee that it will work well in other cases. In fact, elections are perhaps one of the easiest settings because pollsters have been studying elections for almost 100 years, there is regular feedback (we can see who wins the elections), and party identification and demographic characteristics are relatively predictive of voting. At this point, we lack solid theory and empirical experience to know when weighting adjustments to non-probability samples will produce sufficiently accurate estimates. One thing that is clear, however, is if you are forced to work with non-probability samples, then there is strong reason to believe that adjusted estimates will be better than non-adjusted estimates.</p>

<h3 id="3-4-3-non-probability-samples-sample-matching">3.4.3 Non-probability samples: sample matching</h3>

<blockquote>
<p>Not all non-probability samples are the same. We can add more control on the front end.</p>
</blockquote>

<p>The approach Wang and colleagues used to estimate the outcome of the 2012 US presidential election depended entirely on improvements in data analysis. That is, they collected as many responses as they could and then attempted to re-weight them. A complementary strategy for working with non-probability sampling is to have more control over the data collection process.</p>

<p>The simplest example of a partially controlled non-probability sampling process is quota sampling, a technique that goes back to the early days of survey research. In quota sampling, researchers divide the population into different groups (e.g., young men, young women, etc) and then set quotas for the number of people to be selected in each group. Respondents are selected in a haphazard manner until the researcher has met their quota in each group. Because of the quotas, the resulting sample looks more like the target population than would be true otherwise, but because the probabilities of inclusion are unknown many researchers are skeptical of quota sampling. In fact, quota sampling was a cause of the “Dewey Defeats Truman” error in the 1948 US Presidential polls. Because it provides some control over the sampling process, however, one can see how quota sampling might have some advantages over a completely uncontrolled data collection.</p>

<p>Moving beyond quota sampling, more modern approaches to controlling the non-probability sampling process are now possible. One such approach is called sample matching, and it is used by some commercial online panel providers. In its simplest form, sample matching requires two data sources: 1) a complete register of the population and 2) a large panel of volunteers. It is important that the volunteers do not need to be a probability sample from any population; to emphasize that there are no requirements for selection into the panel, I’ll call it a dirty panel. Also, both the population register and the dirty panel must include some auxiliary information about each person, in this example, I’ll consider age and sex, but in realistic situations this auxiliary information could be much more detailed. The trick of sample matching is to select samples from a dirty panel in a way that produces samples that look like probability samples.</p>

<p>Sample matching begins when a simulated probability sample is taken from the population register; this simulated sample becomes a target sample. Then, based on the auxiliary information, cases in the target sample are matched to people in the dirty panel to form a matched sample. For example, if there is a 25 year old female in the target sample, then the researcher finds a 25 year old female from the dirty panel to be in the matched sample. Finally, members of the matched sample are interviewed to produce the final set of respondents.</p>

<p>Even though the matched sample looks like the target sample, it is important to remember that the matched sample is not a probability sample. Matched samples can only match the target sample on the known auxiliary information (e.g., age and sex), but not on unmeasured characteristics. For example, if people on the dirty panel tend to be poorer—after all, one reason to join a survey panel is to earn money—then even if the matched sample looks like the target sample in terms of age and sex it will still have a bias toward poor people. The magic of true probability sampling is to rule out problems on both measured and unmeasured characteristics (a point that is consistent with our discussion of matching for causal inference from observational studies in Chapter 2).</p>

<p>In practice, sample matching depends on having a large and diverse panel eager to complete surveys, and thus it is mainly done by companies that can afford to develop and maintain such a panel. Also, in practice, there can be problems with matching (sometimes a good match for someone in the target sample does not exist on the panel) and non-response (sometimes people in the matched sample refuse to participate in the survey). Therefore, in practice, researchers doing sample matching also perform some kind of post-stratification adjustment to make estimates.</p>

<p>It is hard to provide useful theoretical guarantees about sample matching, but in practice it can perform well. For example, Stephen Ansolabehere and Brian Schaffner (2014) compared three parallel surveys of about 1,000 people conducted in 2010 using three different sampling and interviewing methods: mail, telephone, and an Internet panel using sample matching and post-stratification adjustment. The estimates from the three approaches were quite similar to estimates from high-quality benchmarks such as the Current Population Survey (CPS) and the National Health Interview Survey (NHIS). More specifically, both the Internet and mail surveys were off by an average of 3 percentage points and the phone survey was off by 4 percentage points. Errors this large are approximately what one would expect from samples of about 1,000 people. Although, none of these modes produced substantially better data, both the Internet and phone survey (which took days or weeks) were substantially faster to field than the mail survey (which took eight months), and the Internet survey, which used sample matching, was cheaper than the other two modes.</p>

<p>In conclusion, social scientists and statisticians are incredibly skeptical of inferences from these non-probability samples, in part because they are associated with some embarrassing failures of survey research such as the Literary Digest poll. In part, I agree with this skepticism: unadjusted non-probability samples are likely to produce bad estimates. However, if researchers can adjust for the biases in the sampling process (e.g., post-stratification) or control the sampling process somewhat (e.g., sample matching), they can produce better estimates, and even estimates of sufficient quality for most purposes. Of course, it would be better to do perfectly executed probability sampling, but that no longer appears to be a realistic option.</p>

<p>Both non-probability samples and probability samples vary in their quality, and currently it is likely the case that most estimates from probability samples are more trustworthy than estimates from non-probability samples. But, even now, estimates from well-conducted non-probability samples are probably better than estimates from poorly-conducted probability samples. Further, non-probability samples are substantially cheaper. Thus, it appears that probability vs non-probability sampling offers a cost-quality trade-off (Figure 3.6). Looking forward, I expect that estimates from well-done non-probability samples will become cheaper and better. Further, because of the breakdown in landline telephone surveys and increasing rates of non-response, I expect that probability samples will become more expensive and of lower quality. Because of these long-term trends, I think that non-probability sampling will become increasingly important in the third era of survey research.</p>

<p><img src="http://oaf2qt3yk.bkt.clouddn.com/a7a8e0fba488e6d71493de7a492d1bfe.png" alt="fig3.6" /></p>

<p>Figure 3.6: Probability sampling in practice and non-probability sampling are both large, heterogeneous categories. In general, there is a cost-error trade-off with non-probability sampling being lower cost but higher error. However, well-done non-probability sampling can produce better estimates than poorly-done probability sampling. In the future, I expect that non-probability sampling will get better and cheaper while probability sampling will get worse and more expensive.</p>

<h2 id="3-5-new-ways-of-asking-questions">3.5 New ways of asking questions</h2>

<blockquote>
<p>Traditional surveys are closed, boring, and removed from life. Now we can ask questions more embedded in life, more open, and more fun.</p>
</blockquote>

<p>The total survey error framework encourages researchers to think about survey research as a two part process: recruiting respondents and asking them questions. In the previous section I discussed how the digital age changes how we recruit respondents, and now I’ll discuss how the digital age enables new ways to ask questions. These new approaches can be used with either probability samples or non-probability samples.</p>

<p>A survey mode is the environment in which the questions are asked, and it can have important impacts on measurement (Couper 2011). In the first era of survey research the most common mode was face-to-face, and in the second era the most common mode was telephone. Many researchers view the third era of survey research as just an expansion of survey modes to include computers and mobile phones. However, the digital age is more than just a change in the pipes through which questions and answers flow. Instead, the transition from analogue to digital enables—and will likely require—researchers to change how we ask.</p>

<p>A study by Michael Schober and colleagues illustrates the advantages of adjusting our asking to the capabilities and social norms around new technologies (Schober et al. 2015). In the study, Schober and colleagues compared different approaches for asking people questions via a mobile phone. They compared voice conversations, which would have been a natural translation of second era approaches, to collecting data via many text messages, an approach with no obvious precedent. Schober and colleagues found that texting led to higher quality data than voice interviews. In other words, simply transferring old approaches onto new technologies was not the best approach. Rather, researchers will have to customize our ways of asking to these new platforms.</p>

<p>There are many dimensions along which researchers can categorize survey modes, but the most critical feature of digital age survey modes is that they are computer-administered, rather than interviewer-administered (as in telephone and face-to-face surveys). Taking human interviewers out of the data collection process offers enormous benefits and introduces some drawbacks. In terms of benefits, removing interviewers dramatically reduces costs—interviews are one of the biggest expenses in survey research—and increases flexibility; respondents can participate when they want, not only when an interviewer is available. However, removing the interviewer also limits surveys in some ways. In particular, interviewers are critical to encouraging respondents to participate and keeping them engaged while slogging through long and occasionally tedious surveys.</p>

<p>Next, I’ll describe two approaches showing how researchers can take advantage of the tools of the digital age to ask questions differently: measuring internal states at a more appropriate time and place through ecological momentary assessment (Section 3.5.1) and combining the strengths of open-ended and close-ended survey questions through wiki surveys (Section 3.5.2). However, the move toward computer-administered, ubiquitous asking will also mean that we need to design ways of asking that are more enjoyable for participants, a process called gamification (Section 3.5.3).</p>

<h3 id="3-5-1-ecological-momentary-assessments">3.5.1 Ecological momentary assessments</h3>

<p>Researchers can chop up big surveys and sprinkle them into peoples’ lives.
Ecological momentary assessments (EMA) involves taking traditional surveys, chopping them up into pieces, and sprinkling them into the lives of participants. Thus, survey questions can be asked at an appropriate time and place, rather than in a long interview weeks after the events have occurred.</p>

<p>EMA is characterized by four features: (1) collection of data in real-world environments; (2) assessments that focus on individuals’ current or very recent states or behaviors; (3) assessments that may be event-based, time-based, or randomly prompted (depending on the research question); and (4) completion of multiple assessments over time (Stone and Shiffman 1994). EMA is an approach to asking that is greatly facilitated by smart phones that people interact with frequently throughout the day. Further, because smart phones are packed with sensors—such as GPS and accelerometers—it is increasingly possible to trigger measurements based on activity. For example, a smart phone could be programmed to trigger a survey question if a respondent goes into a particular neighborhood.</p>

<p>The promise of EMA is nicely illustrated by the dissertation research of Naomi Sugie. Since the 1970s the United States has dramatically increased the number of people that it imprisons. As of 2005, about 500 in every 100,000 Americans were in prison, a rate of incarceration higher than anywhere else in the world (Wakefield and Uggen 2010). The surge in the number of people entering prison has also produced a surge in the number of people leaving prison; about 700,000 people leave prison each year (Wakefield and Uggen 2010). These ex-offenders face severe challenges upon leaving prison, and unfortunately many end up back in prison. In order to understand and reduce recidivism, social scientists and policy makers need to understand the experience of ex-offenders as they re-enter society. However, these data are hard to collect with standard survey methods because ex-offenders tend to be difficult to study and their lives are extremely unstable. Measurement approaches that deploy surveys every few months miss enormous amounts of the dynamics in their lives (Sugie 2016).</p>

<p>In order to study the re-entry process of ex-offenders with much greater precision, Sugie took a standard probability sample of 131 people from the complete list of individuals leaving prison in Newark, New Jersey. She provided each participant with a smart phone that became a rich data collection platform. Sugie used the phones to administer two kinds of surveys. First, she sent an “experience sampling survey” at a randomly selected time between 9am and 6pm asking participants about their current activities and feelings. Second, at 7pm, she sent a “daily survey” asking about all the activities of that day. Together these two surveys provide detailed, longitudinal data about the lives of these ex-offenders.</p>

<p>In addition to these surveys, the phones recorded their geographic location at regular intervals and kept encrypted records of call and text meta-data. All of this data collection, particularly the passive data collection, raises some ethical questions, but Sugie’s design handled them well. Sugie received meaningful informed consent from each participant for this data collection, used appropriate security protections, and enabled participants to turn off the geographic tracking. Further, in order to minimize the risk of forced disclosure of data (e.g., a subpoena from the police), Sugie obtained a Certificate of Confidentiality from the federal government before any data was collected (Beskow, Dame, and Costello 2008; Wolf et al. 2012). Sugie’s procedures were reviewed by a third-party (her university’s Institutional Review Board), and they went far beyond what is required by existing regulations. As such, I think her work provides a valuable model for other researchers facing these same challenges; see Sugie (2014) and Sugie (2016) for a more detailed discussion.</p>

<p>The ability to secure and hold a stable job is important for a successful reentry process. However, Sugie found that her participants’ work experiences were informal, temporary, and sporadic. Further, within her participant pool, there were four distinct patterns: “early exit” (those who start searching for work but then drop out of the labor market), “persistent search” (those who spend much of the period searching for work), “recurring work” (those who spend much of the period working), and “low response” (those who do not respond to the surveys regularly). Further, Sugie wanted to understand more about the people who stop searching for jobs. One possibility is that these searchers become discouraged and depressed and eventually drop out of the labor market. Aware of this possibility, Sugie used her surveys to collect data about the emotional state of participants, and she found that the “early exit” group did not report higher levels of stress or unhappiness. Rather, the opposite was the case: those who continued to search for work reported more feelings of emotional distress. All of this fine-grained, longitudinal detail about the behavior and emotional state of the ex-offenders is important for understanding the barriers they face and easing their transition back into society. Further, all of this fine-grained detail would have been missed in a standard survey.</p>

<p>There are three general lessons from Sugie’s work. First, new approaches to asking are completely compatible with traditional methods of sampling; recall, that Sugie took a standard probability sample from a well-defined frame population. Second, high-frequency, longitudinal measurements can be particularly valuable for studying social experiences that are irregular and dynamic. Third, when survey data collection is combined with digital traces, additional ethical issues can arise. I’ll treat research ethics in more detail in Chapter 6, but Sugie’s work shows that these issues are addressable by conscientious and thoughtful researchers.</p>

<h3 id="3-5-2-wiki-surveys">3.5.2 Wiki surveys</h3>

<blockquote>
<p>Wiki surveys enable new hybrids of closed and open questions.</p>
</blockquote>

<p>In addition to asking questions at more natural times and in more natural contexts, new technology also allows us to change the form of the questions. Most survey questions are closed, where respondents choose from a set choices written by researchers. It’s a process that one prominent survey researcher calls “putting words in people’s mouths.” For example, here’s a closed survey question:</p>

<blockquote>
<p>“This next question is on the subject of work. Would you please look at this card and tell me which thing on this list you would most prefer in a job?”</p>
</blockquote>

<ol>
<li>High income;</li>
<li>No danger of being fired;</li>
<li>Working hours are short, lots of free time;</li>
<li>Chances for advancement;</li>
<li>The work is important, and gives a feeling of accomplishment.</li>
</ol>

<p>Now here’s the same question asked in an open form:</p>

<blockquote>
<p>“This next question is on the subject of work. People look for different things in a job. What would you most prefer in a job?”</p>
</blockquote>

<p>Although these two questions appear quite similar, a survey experiment by Howard Schuman and Stanley Presser (1979) revealed that they can produce very different results: nearly 60% of the open results fall outside of the categories in the closed responses (Figure 3.7).</p>

<p><img src="http://oaf2qt3yk.bkt.clouddn.com/3aa8645aa30003c130d9506ffa374790.png" alt="fig3.7" /></p>

<p>Figure 3.7: Results from Schuman and Presser (1979). Responses are quite different depending on whether the question is asked in closed or open form.</p>

<p>Although open and closed questions can yield quite different information and both were popular in the early days of survey research, closed questions have come to dominate the field. This domination is not because closed questions have been proven to provide better measurement, rather it is because they are much easier to use; the process of coding open-ended questions is complicated and expensive. The move away from open questions is unfortunate because it is precisely the information that the researcher did not know ahead of time that can be the most valuable.</p>

<p>In some research I’ve done with Karen Levy, we tried to create a new kind of survey question that combines the best features of both open and closed questions (Salganik and Levy 2015). That is, it enables researchers to learn new information as in an open question, and it yields easy to analyze data as in a closed question. Inspired by online systems driven by user-generated content, of which Wikipedia is an exemplar, we called our system a wiki survey. By combing the characteristics of Wikipedia and a traditional survey, we hope to create a new way of asking questions.</p>

<p>The data collection process in a wiki survey is illustrated by a project we did with the New York City Mayor’s Office in order to integrate residents’ ideas into PlaNYC 2030, New York’s citywide sustainability plan. To begin the process, the Mayor’s Office generated a list of 25 ideas based on their previous outreach (e.g., “Require all big buildings to make certain energy efficiency upgrades,” “Teach kids about green issues as part of school curriculum”). Using these 25 ideas as “seeds,” the Mayor’s Office asked the question “Which do you think is a better idea for creating a greener, greater New York City?” Respondents were presented with a pair of ideas (e.g., “Open schoolyards across the city as public playgrounds” and “Increase targeted tree plantings in neighborhoods with high asthma rates”), and asked to choose between them (Figure 3.8). After choosing, respondents were immediately presented with another randomly selected pair of ideas. Respondents were able to continue contributing information about their preferences for as long as they wished by either voting or choosing “I can’t decide.” Crucially, at any point, respondents were able to contribute their own ideas, which—pending approval by the Mayor’s Office—became part of the pool of ideas to be presented to others. Thus, the questions that participants receive is both open and closed simultaneously.</p>

<p><img src="http://oaf2qt3yk.bkt.clouddn.com/e106dd49688ff745fa1d114a8b5bca0c.png" alt="fig3.8" /></p>

<p>Figure 3.8: Interface for a wiki survey (Salganik and Levy 2015).</p>

<p>The Mayor’s Office launched its wiki survey in October 2010 in conjunction with a series of community meetings to obtain resident feedback. Over about four months, 1,436 respondents contributed 31,893 responses and 464 new ideas. Critically, 8 of the top 10 scoring ideas were uploaded by participants rather than part of the set of seed ideas from the Mayor’s Office. And, as we describe in our paper, this is a pattern in many wiki surveys. In other words, by being open to new information, researchers are able to learn things that would have been missed using more closed approaches to asking.</p>

<h3 id="3-5-3-gamification">3.5.3 Gamification</h3>

<blockquote>
<p>Standard surveys are boring for participants, but that can and must change.</p>
</blockquote>

<p>So far, I’ve told you about new approaches to asking that are facilitated by computer-administered interviews. However, one downside of computer-administered interviews is that there is no human interviewer to help induce participation. This is a problem because surveys are both time-consuming and boring. Therefore, in the future, survey designers are going to have to design around their participants and make the process of answering questions more enjoyable and game-like. This process is sometimes called gamification.</p>

<p>To illustrate what a fun survey might look like, let’s consider Friendsense, a survey that was packaged as a game on Facebook. Sharad Goel, Winter Mason, and Duncan Watts (2010) wanted to estimate how much people think they are like their friends and how much they are actually like their friends. This question about real and perceived attitude similarity gets directly at people’s ability to accurately perceive their social environment and has implications for political polarization and the dynamics of social change. Conceptually, real and perceived attitude similarity is an easy thing to measure. The researchers could have just asked lots of people about their opinions and then asked their friends about their opinions (this allows for measurement of real attitude agreement) and they could have asked lots of people to guess their friends’ attitudes (this allows for measurement of perceived attitude agreement). Unfortunately, it is logistically very difficult to interview both a respondent and her friend. Therefore, Goel and colleagues turned their survey into a Facebook app that was fun to play.</p>

<p>After a participant consented to be in a research study, the app selected a friend from the respondent’s Facebook account and asked a question about the attitude of that friend (Figure 3.9). Intermixed with questions about randomly selected friends, the respondent also answered questions about herself. After answering a question about a friend, the respondent was told whether her answer was correct or, if her friend had not answered, the respondent was able to encourage her friend to participate. Thus, the survey spread in part through viral recruitment.</p>

<p><img src="http://oaf2qt3yk.bkt.clouddn.com/d927d36b179209e6790a59c04934fe41.png" alt="fig3.9" /></p>

<p>Figure 3.9: Interface from the Friendsense study (Goel, Mason, and Watts 2010). The researchers turned a standard attitude survey into a fun, gamelike experience. The app asked participants both serious questions and more lighthearted questions, such as the one shown in this image. Image used with permission from Sharad Goel.</p>

<p>The attitude questions were adapted from the General Social Survey. For example, “Does [your friend] sympathize with the Israelis more than the Palestinians in the Middle East situation?” and “Would [your friend] pay higher taxes for the government to provide universal health care?” On top of these serious questions, the researchers mixed in more lighthearted questions: “Would [your friend] rather drink wine over beer?” and “Would [your friend] rather have the power to read minds, instead of the power to fly?” These lighthearted questions made the process more enjoyable to participants and also enabled an interesting comparison: would attitude agreement be similar for serious political questions as lighthearted questions about drinking and superpowers?</p>

<p>There were three main results from the study. First, friends were more likely to give the same answer than strangers, but even close friends still disagree on about 30% of the questions. Second, respondents over-estimate their agreement with their friends. In other words, most of the diversity of opinions that exists between friends is not noticed. Finally, participants were as likely to be aware of disagreements with their friends on serious matters of politics than lighthearted issues about drinking and superpowers.</p>

<p>Although the app is no longer available to play (unfortunately), it was a nice example of how researchers can a standard attitude survey into something enjoyable. More generally, with some creativity and design work, it is possible to improve the user experience for survey participants. So, next time you are designing a survey, take a moment to think about what you could do to make the experience better for your participants. Some may fear that these steps toward gamification could hurt data quality, but I think that bored participants are a far greater risk to data quality.</p>

<p>The work of Goel and colleagues also illustrates the theme of the next section: linking surveys to other data sources. In this case by linking the survey with Facebook the researchers automatically had access to a list of the participants’ friends. In the next section, we will consider the linkages between surveys and other data sources in greater detail.</p>

<h2 id="3-6-surveys-linked-to-other-data">3.6 Surveys linked to other data</h2>

<blockquote>
<p>Linking surveys to other data sources enables you to produce estimates that would impossible otherwise.</p>
</blockquote>

<p>Most surveys are stand-alone, self-contained efforts. They don’t build on each other and they don’t take advantage of all of the other data that exists in the world. This will change. There is just too much to be gained by linking survey data to other data sources, such as the digital trace data discussed in Chapter 2. This ability to link surveys to other data highlights the fact that asking and observing are complements and not substitutes.</p>

<p>In particular, I’ll distinguish between two different ways of linking asking and observing: amplified asking and enriched asking (Figure 3.10). In amplified asking, the digital traces are not of any direct interest other than their ability to help extract more value from the survey data. In enriched asking, on the other hand, the digital trace actually has a core measure of interest and the survey data builds the necessary context around it.</p>

<p><img src="http://oaf2qt3yk.bkt.clouddn.com/80df17cb34b263b59e725a42bbf92cad.png" alt="fig3.10" /></p>

<p>Figure 3.10: Two main ways to combine digital traces and survey data. In amplified asking (Section 3.6.1) the digital traces are used to amplify the survey data. In enriched asking (Section 3.6.2) the digital traces actually have a core measure of interest and the survey data builds the necessary context around it.</p>

<h3 id="3-6-1-amplified-asking">3.6.1 Amplified asking</h3>

<blockquote>
<p>Linking your survey to digital traces can be like asking everyone your questions at all times.</p>
</blockquote>

<p>Asking generally comes in two main categories: sample surveys and censuses. Sample surveys, where you access a small number of people, can be flexible, timely, and relatively cheap. However, sample surveys, because they are based on a sample, are often limited in their resolution; with a sample survey, it is often hard to make estimates about specific geographic regions or for specific demographic groups. Censuses, on the other, attempt to interview everyone in the population. They have great resolution, but they are generally expensive, narrow in focus (they only include a small number of questions), and not timely (they happen on a fixed schedule, such as every 10 years) (Kish 1979). Now imagine if researchers could combine the best characteristics of sample surveys and censuses; imagine if researchers could ask every question to everyone every day.</p>

<p>Obviously, this continual, ubiquitous, always-on survey is a kind of social science fantasy. But, it appears that we can begin to approximate this by combining survey questions from a small number of people with digital traces from many people. I call this type of combination amplified asking. If done well, it could help us provides estimate that are more local (for smaller geographic areas), more granular (for specific demographic groups), and more timely.</p>

<p>One example of amplified asking comes from the work of Joshua Blumenstock, who wanted to collect data that would help guide development in poor countries. More specifically, Blumenstock wanted to create a system to measure wealth and well-being that combined the completeness of a census with the flexibility and frequency of a survey (Blumenstock 2014; Blumenstock, Cadamuro, and On 2015). In fact, I’ve already described Blumenstock’s work briefly in Chapter 1.</p>

<p>To start, Blumenstock partnered with the largest mobile phone provider in Rwanda. The company provided him anonymized transaction records from about 1.5 million customers covering behavior from 2005 and 2009. The logs contain information about each call and text message such as the start time, duration, and approximate geographic location of the caller and receiver. Before we start talking about the statistical issues, it is worth pointing out that this first step may be one of the hardest. As described in Chapter 2, most digital trace data is inaccessible to researchers. And, many companies are justifiably hesitant to share their data because it is private; that is their customers probably didn’t expect that their records will be shared—in bulk—with researchers. In this case, the researchers took careful steps to anonymize the data and their work was overseen by a third-party (i.e., their IRB). But, despite these efforts, these data are probably still identifiable and they likely contain sensitive information (Mayer, Mutchler, and Mitchell 2016; Landau 2016). I’ll return to these ethical question in Chapter 6.</p>

<p>Recall that Blumenstock was interested in measuring wealth and well-being. But, these traits are not directly in the call records. In other words, these call records are incomplete for this research, a common feature of digital traces that was discussed in detail in Chapter 2. But, it seems likely that the call records probably have some information about wealth and well-being. So, one way of asking Blumenstock’s question could be: is it possible to predict how someone will respond to a survey based on their digital trace data? If so, then by asking a few people we can guess the answers of everyone else.</p>

<p>To assess this empirically, Blumenstock and research assistants from Kigali Institute of Science and Technology called a sample of about a thousand mobile phone customers. The researchers explained the goals of the project to the participants, asked for their consent to link the survey responses to the call records, and then asked them a series of questions to measure their wealth and well-being, such as “Do you own a radio?” and “Do you own a bicycle?” (see Figure 3.11 for a partial list). All participants in the survey were compensated financially.</p>

<p>Next, Blumenstock used a two-step procedure common in data science: feature engineering followed by supervised learning. First, in the feature engineering step, for everyone that was interviewed, Blumenstock converted the call records into a set of characteristics about each person; data scientists might call these characteristics “features” and social scientists would call them “variables.” For example, for each person, Blumenstock calculated total number of days with activity, the number of distinct people a person has been in contact with, the amount of money spent on airtime, and so on. Critically, good feature engineering requires knowledge of the research setting. For example, if it is important to distinguish between domestic and international calls (we might expect people who call internationally to be wealthier), then this must be done at the feature engineering step. A researcher with little understanding of Rwanda might not include this feature, and then the predictive performance of the model will suffer.</p>

<p>Next, in the supervised learning step, Blumenstock built a statistical model to predict the survey response for each person based on their features. In this case, Blumenstock used logistic regression with 10-fold cross-validation, but he could have used a variety of other statistical or machine learning approaches.</p>

<p>So how well did it work? Was Blumenstock able to predict answers to survey questions like “Do you own a radio?” and “Do you own a bicycle?” using features derived from call records? Sort of. The accuracy of the predictions were high for some traits (Figure 3.11). But, it is always important to compare a complex prediction method against a simple alternative. In this case, a simple alternative is to predict that everyone will give the most common answer. For example, 97.3% reported owning a radio so if Blumenstock had predicted that everyone would report owning a radio he would have had an accuracy of 97.3%, which is surprisingly similar to the performance of his more complex procedure (97.6% accuracy). In other words, all the fancy data and modeling increased the accuracy of the prediction from 97.3% to 97.6%. However, for other questions, such as “Do you own a bicycle?”, the predictions improved from 54.4% to 67.6%. More generally, Figure 3.12 shows for some traits Blumenstock did not improve much beyond just making the simple baseline prediction, but that for other traits there was some improvement.</p>

<p><img src="http://oaf2qt3yk.bkt.clouddn.com/179dfd9b6498529cef6071910525b5d9.png" alt="fig3.11" /></p>

<p>Figure 3.11: Predictive accuracy for statistical model trained with call records. Results from Table 2 of Blumenstock (2014).</p>

<p><img src="http://oaf2qt3yk.bkt.clouddn.com/e1b8b367faf3a73279a91b17423396e9.png" alt="fig3.12" /></p>

<p>Figure 3.12: Comparison of predictive accuracy for statistical model trained with call records to simple baseline prediction. Points are slightly jittered to avoid overlap; see Table 2 of Blumenstock (2014) for exact values.</p>

<p>At this point you might be thinking that these results are a bit disappointing, but just one year later, Blumenstock and two colleagues—Gabriel Cadamuro and Robert On—published a paper in Science with substantially better results (Blumenstock, Cadamuro, and On 2015). There were two main technical reasons for the improvement: 1) they used more sophisticated methods (i.e., a new approach to feature engineering and a more sophisticated machine learning model) and 2) rather than attempting to infer responses to individual survey questions (e.g., “Do you own a radio?”), they attempted to infer a composite wealth index.</p>

<p>Blumenstock and colleagues demonstrated the performance of their approach in two ways. First, they found that for the people in their sample, they could do a pretty good job of predicting their wealth from call records (Figure 3.14). Second, and ever more importantly, Blumenstock and colleagues showed that their procedure could produce high-quality estimates of the geographic distribution of wealth in Rwanda. More specifically, they used their machine learning model, which was trained on their sample of about 1,000 people, to predict the wealth of all 1.5 million people in the call records. Further, with the geospatial data embedded in the call data (recall that the call data includes the location of the nearest cell tower for each call), the researchers were able to estimate the approximate place of residence of each person. Putting these two estimates together, the research produced an estimate of the geographic distribution of subscriber wealth at extremely fine spatial granularity. For example, they could estimate the average wealth in each of Rwanda’s 2148 cells (the smallest administrative unit in the country). These predicted wealth values were so granular they were difficult to check. So, the researchers aggregated their results to produce estimates of the average wealth of Rwanda’s 30 districts. These district-level estimates were strongly related to the estimates from a gold standard traditional survey, the Rwandan Demographic and Health Survey (Figure 3.14). Although the estimates from the two sources were similar, the estimates from Blumenstock and colleagues were about 50 times cheaper and 10 times faster (when cost in measured in terms of variable costs). This dramatic decrease in cost means that rather than being run every few years—as is standard for Demographic and Health Surveys—the hybrid of small survey combined with big digital trace data could be run every month.</p>

<p><img src="http://oaf2qt3yk.bkt.clouddn.com/8e86aaf3bca7fccbfb579f7d15c17148.png" alt="fig3.13" /></p>

<p>Figure 3.13: Schematic of Blumenstock, Cadamuro, and On (2015). Call data from the phone company was converted to a matrix with one row for each person and one column for each feature (i.e., variable). Next, the researchers built a supervised learning model to predict the survey responses from the person by feature matrix. Then, the supervised learning model was used to impute the survey responses for everyone. In essence, the researchers used the responses of about one thousand people to impute the wealth of about one million people. Also, the researchers estimated the approximate place of residence for all 1.5 million people based on the locations of their calls. When these two estimates were combined—the estimated wealth and the estimated place of residence—the results were similar to estimates from the Demographic and Health Survey, a gold-standard traditional survey (Figure 3.14).</p>

<p><img src="http://oaf2qt3yk.bkt.clouddn.com/bda2690df7d0803562a8d7c05cc2c44c.png" alt="fig3.14" /></p>

<p>Figure 3.14: Results from Blumenstock, Cadamuro, and On (2015). At the individual-level, the researchers were able to do a reasonable job at predicting someone’s wealth from their call records. The estimates of district-level wealth—which were based on individual-level estimates of wealth and place of residence—the results were similar to results from the Demographic and Health Survey, a gold-standard traditional survey.</p>

<p>In conclusion, Blumenstock’s amplified asking approach combined survey data with digital trace data to produce estimates comparable with gold-standard survey estimates. This particular example also clarifies some of the trade-offs between amplified asking and traditional survey methods. First, the amplified asking estimates were more timely, substantially cheaper, and more granular. But, on the other hand, at this time, there is not a strong theoretical basis for this kind of amplified asking. That is, this one example does not show when it will work and when it won’t. Further, the amplified asking approach does not yet have good ways to quantify uncertainty around its estimates. However, amplified asking has deep connections to three large areas in statistics—model-based post-stratification (Little 1993), imputation (Rubin 2004), and small-area estimation (Rao and Molina 2015)—and so I expect that progress will be rapid.</p>

<p>Amplified asking follows a basic recipe that can be tailored to your particular situation. There are two ingredients and two steps. The two ingredients are 1) a digital trace dataset that is wide but thin (that is, it has many people but not the information that you need about each persons) and 2) a survey that is narrow but thick (that is, it has only a few people, but it has the information that you need about those people). Then, there are two steps. First, for the people in both data sources, build a machine learning model that uses digital trace data to predict survey answers. Next, use that machine learning model to impute the survey answers of everyone in the digital trace data. Thus, if there is some question that you want to ask to lots of people, look for digital trace data from those people that might be used to predict their answer.</p>

<p>Comparing Blumenstock’s first and second attempt at the problem also illustrates an important lesson about the transition from second era to third era approaches to survey research: the beginning is not the end. That is, many times, the first approach will not be the best, but if researchers continuing working, things can get better. More generally, when evaluating new approaches to social research in the digital age, it is important to make two distinct evaluations: 1) how well does this work now and 2) how well do you think this might work in the future as the data landscape changes and as researchers devote more attention to the problem. Although, researchers are trained to make the first kind of evaluation (how good is this particular piece of research), the second is often more important.</p>

<h3 id="3-6-2-enriched-asking">3.6.2 Enriched asking</h3>

<blockquote>
<p>Even though it can be messy, enriched asking can be powerful.</p>
</blockquote>

<p>A different approach to dealing with the incompleteness of digital trace data is to enrich it directly with survey data, a process that I’ll call enriched asking. One example of enriched asking is the study of Burke and Kraut (2014), which I described earlier in the chapter (Section 3.2), about whether interacting on Facebook increases friendship strength. In that case, Burke and Kraut combined survey data with Facebook log data.</p>

<p>The setting that Burke and Kraut were working in, however, meant that they didn’t have to deal with two big problems that researchers doing enriched asking face. First, actually linking together the data sets—a process called record linkage, the matching of a record in one dataset with the appropriate record in the other dataset—can be difficult and error-prone (we’ll see an example of this problem below). The second main problem of enriched asking is that the quality of the digital traces will frequently be difficult for researchers to assess. For example, sometimes the process through which it is collected is proprietary and could be susceptible to many of the problems described in Chapter 2. In other words, enriched asking will frequently involve error-prone linking of surveys to black-box data sources of unknown quality. Despite the concerns that these two problems introduce, it is possible to conduct important research with this strategy as was demonstrated by Stephen Ansolabehere and Eitan Hersh (2012) in their research on voting patterns in the US . It is worthwhile to go over this study in some detail because many of the strategies that Ansolabehere and Hersh developed will be useful in other applications of enriched asking.</p>

<p>Voter turnout has been the subject of extensive research in political science, and in the past, researchers’ understanding of who votes and why has generally been based on the analysis of survey data. Voting in the US, however, is an unusual behavior in that the government records whether each citizen has voted (of course, the government does not record who each citizen votes for). For many years, these governmental voting records were available on paper forms, scattered in various local government offices around the country. This made it difficult, but not impossible, for political scientists to have a complete picture of the electorate and to compare what people say in surveys about voting to their actual voting behavior (Ansolabehere and Hersh 2012).</p>

<p>But, now these voting records have been digitized, and a number of private companies have systematically collected and merged these voting records to produce comprehensive master voting files that record the voting behavior of all Americans. Ansolabehere and Hersh partnered with one of these companies—Catalist LCC—in order to use their master voting file to help develop a better picture of the electorate. Further, because it relied on digital records collected and curated by a company, it offered a number of advantages over previous efforts by researchers that had been done without the aid of companies and using analog records.</p>

<p>Like many of the digital trace sources in Chapter 2, the Catalist master file did not include much of the demographic, attitudinal, and behavioral information that Ansolabehere and Hersh needed. In addition to this information, Ansolabehere and Hersh were particularly interested in comparing reported voting behavior to validated voting behavior (i.e., the information in the Catalist database). So, the researchers collected the data that they wanted as part of the Cooperative Congressional Election Study (CCES), a large social survey. Next, the researchers gave this data to Catalist, and Catalist gave the researchers back a merged data file that included validated voting behavior (from Catalist), the self-reported voting behavior (from CCES) and the demographics and attitudes of respondents (from CCES). In other words, Ansolabehere and Hersh enriched the voting data with survey data, and the resulting merged file enables them to do something that neither file enabled individually.</p>

<p>By enriching the Catalist master data file with survey data, Ansolabehere and Hersh came to three important conclusions. First, over-reporting of voting is rampant: almost half of the non-voters reported voting. Or, another way of looking at it is if someone reported voting, there is only an 80% chance that they actually voted. Second, over-reporting is not random; over-reporting is more common among high-income, well-educated, partisans who are engaged in public affairs. In other words, the people who are most likely to vote are also most likely to lie about voting. Third, and most critically, because of the systematic nature of over-reporting, the actual differences between voters and non-voters are smaller than they appear just from surveys. For example, those with a bachelors degree are about 22 percentage points more likely to report voting, but are only 10 percentage points more likely to actual vote. Further, existing resource-based theories of voting are much better at predicting who will report voting than who actually votes, an empirical finding that calls for new theories to understand and predict voting.</p>

<p>But, how much should we trust these results? Remember these results depend on error-prone linking to black-box data with unknown amounts of error. More specifically, the results hinge on two key steps: 1) the ability of Catalist to combine many disparate data sources to produce an accurate master datafile and 2) the ability of Catalist to link the survey data to its master datafile. Each of these steps is quite difficult and errors at either step could lead researchers to the wrong conclusions. However, both data processing and matching are critical to the continued existence of Catalist as a company so it can invest resources in solving these problems, often at a scale that no individual academic researcher or group of researchers can match. In the further reading at the end of the chapter, I describe these problems in more detail and how Ansolabehere and Hersh build confidence in their results. Although these details are specific to this study, issues similar to these will arise for other researchers wishing to link to black-box digital trace data sources.</p>

<p>What are the general lessons researchers can draw from this study? First, there is tremendous value from enriching digital traces with survey data. Second, even though these aggregated, commercial data sources should not be considered “ground truth”, in some cases they can be useful. In fact, it is best to compare these data sources not to absolute Truth (from which they will always fall short). Rather, it is better to compare them to other available data sources, which invariably have errors as well.</p>

<h2 id="3-7-conclusion">3.7 Conclusion</h2>

<p>Big data sources do not mean the end of survey research. In fact, it is the opposite. Big data sources and surveys are complements not substitutes so as the amount of big data increases, I expect that the value of surveys will increases as well.</p>

<p>Researchers are now in the process of creating the third era of survey research that will most likely be characterized by 1) non-probability sampling, 2) computer-administrated interviews, and 3) linking survey data to other data. Each of these three areas are still in active development, but as the data environment changes and researchers invest more time to addresses these problems, I expect major improvements. Just as researchers took advantage of the widespread diffusion of landline phones in the 1960s and 1970s in order to modernize survey research, I expect that researchers will modernize survey research again in response to the changes created by the digital age.</p>

<h2 id="technical-appendix-1">Technical appendix</h2>

<p>This section will take a mathematical approach to sampling and estimation in probability and non-probability samples. It will draw on Särndal, Swensson, and Wretman (2003) and Särndal and Lundström (2005).</p>

<h3 id="further-commentary-1">Further commentary</h3>

<blockquote>
<p>This section is designed to be used as a reference, rather than to be read as a narrative.</p>
</blockquote>

<p><strong>Introduction (Section 3.1)</strong></p>

<p>Many of the themes in this chapter have also been echoed in recent Presidential Addresses at the American Association of Public Opinion Research (AAPOR), such as Dillman (2002), Newport (2011), Santos (2014), and Link (2015).</p>

<p>For more historical background about the development of survey research, see Smith (1976) and Converse (1987). For more on the idea of three eras of survey research, see Groves (2011) and Dillman, Smyth, and Christian (2008) (which breaks up the three eras slightly differently).</p>

<p>A peak inside the transition from the first to the second era in survey research is Groves and Kahn (1979), which does a detailed head-to-head comparison between a face-to-face and telephone survey. Brick and Tucker (2007) looks back at the historical development of random digit dialing sampling methods.</p>

<p>For more how survey research has changed in the past in response to changes in society, see Tourangeau (2004), Mitofsky (1989), and Couper (2011).</p>

<p><strong>Asking vs. observing (Section 3.2)</strong></p>

<p>Learning about internal states by asking questions can be problematic because sometimes the respondents themselves are not aware of their internal states. For example, Nisbett and Wilson (1977) have a wonderful paper with the evocative title: “Telling more than we can know: Verbal reports on mental processes.” In the paper the authors conclude: “subjects are sometimes (a) unaware of the existence of a stimulus that importantly influenced a response, (b) unaware of the existence of the response, and &copy; unaware that the stimulus has affected the response.”</p>

<p>For arguments that researchers should prefer observed behavior to reported behavior or attitudes, see Baumeister, Vohs, and Funder (2007) (psychology) and Jerolmack and Khan (2014) and responses (Maynard 2014; Cerulo 2014; Vaisey 2014; Jerolmack and Khan 2014) (sociology). The difference between asking and observing also arises in economics, where researchers talk about stated and revealed preferences. For example, a researcher could ask respondents whether they prefer eating ice cream or going to gym (stated preferences) or the research could observe how often people eat ice cream and go to the gym (revealed preferences). There is deep skepticism of certain types of stated preferences data in economics (Hausman 2012).</p>

<p>A main theme from these debates is that reported behavior is not always accurate. But, automatically recorded behavior may not be accurate, may not be collected on a sample of interest, and may not be accessible to researchers. Thus, in some situations, I think that reported behavior can be useful. Further, a second main theme from these debates is that reports about emotions, knowledge, expectations, and opinions are not always accurate. But, if information about these internal states are needed by researchers—either to help explain some behavior or as the thing to be explained—then asking may be appropriate.</p>

<p><strong>Total survey error (Section 3.3)</strong></p>

<p>For book length treatments on total survey error, see Groves et al. (2009) or Weisberg (2005). For a history of the development of total survey error, see Groves and Lyberg (2010).</p>

<p>In terms of representation, a great introduction to the issues of non-response and non-response bias is the National Research Council report on Nonresponse in Social Science Surveys: A Research Agenda (2013). Another useful overview is provided by (Groves 2006). Also, entire special issues of the Journal of Official Statistics, Public Opinion Quarterly, and The Annals of the American Academy of Political and Social Science have been published on the topic of non-response. Finally, there are actually many different ways of calculating the response rate; these approaches are described in detail in a report by the American Association of Public Opinion Researchers (AAPOR) (Public Opinion Researchers} 2015).</p>

<p>The 1936 Literary Digest poll has been studied in detail (Bryson 1976; Squire 1988; Cahalan 1989; Lusinchi 2012). It has also been used as a parable to warn against haphazard data collection (Gayo-Avello 2011). In 1936, George Gallup used a more sophisticated form of sampling, and was able to produce more accurate estimates with a much smaller sample. Gallup’s success over the Literary Digest was a milestone the development of survey research (Converse 1987, Ch 3; Ohmer 2006, Ch 4; Igo 2008, Ch 3).</p>

<p>In terms of measurement, a great first resource for designing questionnaires is Bradburn, Sudman, and Wansink (2004). For a more advanced treatment focused specifically on attitude questions, see Schuman and Presser (1996). More on pre-testing questions is available in Presser and Blair (1994), Presser et al. (2004), and Chapter 8 of Groves et al. (2009).</p>

<p>The classic, book-length treatment of the trade-off between survey costs and survey errors is Groves (2004).</p>

<p><strong>Who to ask (Section 3.4)</strong></p>

<p>Classic book-length treatment of standard probability sampling and estimation are Lohr (2009) (more introductory) and Särndal, Swensson, and Wretman (2003) (more advanced). A classic book-length treatment of post-stratification and related methods is Särndal and Lundström (2005). In some digital age settings, researchers know quite a bit about non-respondents, which was not often true in the past. Different forms of non-response adjustment are possible when researchers have information about non-respondents (Kalton and Flores-Cervantes 2003; Smith 2011).</p>

<p>The Xbox study of Wang et al. (2015) uses a technique called multilevel regression and post-stratification (MRP, sometimes called “Mister P”) that allows researchers to estimate cell means even when there are many, many cells. Although there is some debate about the quality of the estimates from this technique, it seems like a promising area to explore. The technique was first used in Park, Gelman, and Bafumi (2004), and there has been subsequent use and debate (Gelman 2007; Lax and Phillips 2009; Pacheco 2011; Buttice and Highton 2013; Toshkov 2015). For more on the connection between individual weights and cell-based weights see Gelman (2007).</p>

<p>For other approaches to weighting web surveys, see Schonlau et al. (2009), Valliant and Dever (2011), and Bethlehem (2010).</p>

<p>Sample matching was proposed by Rivers (2007). Bethlehem (2015) argues that the performance of sample matching will actually be similar to other sampling approaches (e.g., stratified sampling) and other adjustment approaches (e.g., post-stratification). For more on online panels, see Callegaro et al. (2014).</p>

<p>Sometimes researchers have found that probability samples and non-probability samples yield estimates of similar quality (Ansolabehere and Schaffner 2014), but other comparisons have found that non-probability samples do worse (Malhotra and Krosnick 2007; Yeager et al. 2011). One possible reason for these differences is that non-probability samples have improved over time. For a more pessimistic view of non-probability sampling methods see the the AAPOR Task Force on Non-probability Sampling (Baker et al. 2013), and I also recommend reading the commentary that follows the summary report.</p>

<p>For a meta-analysis on the effect of weighting to reduce bias in non-probability samples, see Table 2.4 in Tourangeau, Conrad, and Couper (2013), which leads the authors to conclude “adjustments seem to be useful but fallible corrections &hellip;”</p>

<p><strong>How to ask (Section 3.5)</strong></p>

<p>Conrad and Schober (2008) provides an edited volume titled Envisioning the Survey Interview of the Future, and it addresses many of the themes in this section. Couper (2011) addresses similar themes, and Schober et al. (2015) offers a nice example of how data collection methods that are tailored to a new setting can result in higher quality data.</p>

<p>For another interesting example of using Facebook apps for social science surveys, see Bail (2015).</p>

<p>For more advice on making surveys an enjoyable and valuable experience for participants, see work on the Tailored Design Method (Dillman, Smyth, and Christian 2014).</p>

<p>Stone et al. (2007) offers a book length treatment of ecological momentary assessment and related methods.</p>

<p><strong>Surveys linked to other data (Section 3.6)</strong></p>

<p>Judson (2007) described the process of combining surveys and administrative data as “information integration,” discusses some advantages of this approach, and offers some examples.</p>

<p>Another way that researchers can use digital traces and administrative data is a sampling frame for people with specific characteristics. However, access these records to be used a sampling frame can also create questions related to privacy (Beskow, Sandler, and Weinberger 2006).</p>

<p>Regarding amplified asking, this approach is not as new as it might appear from how I’ve described it. This approach has deep connections to three large areas in statistics—model-based post-stratification (Little 1993), imputation (Rubin 2004), and small area estimation (Rao and Molina 2015). It is also related to the use of surrogate variables in medical research (Pepe 1992).</p>

<p>In addition to the ethical issues regarding accessing the digital trace data, amplified asking could also be used to infer sensitive traits that people might not choose to reveal in a survey (Kosinski, Stillwell, and Graepel 2013).</p>

<p>The cost and time estimates in Blumenstock, Cadamuro, and On (2015) refer more to variable cost—the cost of one additional survey—and do not include fixed costs such as the cost to clean and process the call data. In general, amplified asking will probably have high fixed costs and low variable costs similar to digital experiments (see Chapter 4). More details on the data used in Blumenstock, Cadamuro, and On (2015) paper are in Blumenstock and Eagle (2010) and Blumenstock and Eagle (2012). Approaches from multiple imputuation (Rubin 2004) might help capture uncertainty in estimates from amplified asking. If researchers doing amplified asking only care about aggregate counts, rather than individual-level traits, then the approaches in King and Lu (2008) and Hopkins and King (2010) may be useful. For more about the machine learning approaches in Blumenstock, Cadamuro, and On (2015), see James et al. (2013) (more introductory) or Hastie, Tibshirani, and Friedman (2009) (more advanced). Another popular machine learning textbook is Murphy (2012).</p>

<p>Regarding enriched asking, the results in Ansolabehere and Hersh (2012) hinge on two key steps: 1) the ability of Catalist to combine many disparate data sources to produce an accurate master datafile and 2) the ability of Catalist to link the survey data to its master datafile. Therefore, Ansolabehere and Hersh check each of these steps carefully.</p>

<p>To create the master datafile, Catalist combines and harmonizes information from many different sources including: multiple voting records snapshots from each state, data from the Post Office’s National Change of Address Registry, and data from other unspecified commercial providers. The gory details about how all this cleaning and merging happens are beyond the scope of this book, but this process, no matter how careful, will propagate errors in the original data sources and will introduce errors. Although Catalist was willing to discuss its data processing and provide some of its raw data, it was simply impossible for researchers to review the entire Catalist data pipeline. Rather, the researchers were in a situation where the Catalist data file had some unknown, and perhaps unknowable, amount of error. This is a serious concern because a critic might speculate that the large differences between the survey reports on the CCES and the behavior in the Catalist master data file were caused by errors in the master data file, not by misreporting by respondents.</p>

<p>Ansolabehere and Hersh took two different approaches to addressing the data quality concern. First, in addition to comparing self-reported voting to voting in the Catalist master file, the researchers also compared self-reported party, race, voter registration status (e.g., registered or not registered) and voting method (e.g., in person, absentee ballot, etc.) to those values found in the Catalist databases. For these four demographic variables, the researchers found much higher levels of agreement between survey report and data in the Catalist master file than for voting. Thus, the Catalist master data file appears to have high quality information for traits other than voting, suggesting that it is not of poor overall quality. Second, in part using data from Catalist, Ansolabehere and Hersh developed three different measures of quality of county voting records, and they found that the estimated rate of over-reporting of voting was essentially unrelated to any of these data quality measures, a finding that suggest that the high rates of over-reporting are not being driven by counties with unusually low data quality.</p>

<p>Given the creation of this master voting file, the second source of potential errors is linking the survey records to it. For example, if this linkage is done incorrectly it could lead to an over-estimate of the difference between reported and validated voting behavior (Neter, Maynes, and Ramanathan 1965). If every person had a stable, unique identifier that was in both data sources, then linkage would be trivial. In the US and most other countries, however, there is no universal identifier. Further, even if there were such an identifier people would probably be hesitant to provide it to survey researchers! Thus, Catalist had to do the linkage using imperfect identifiers, in this case four pieces of information about each respondent: name, gender, birth year, and home address. For example, Catalist had to decide if the Homie J Simpson in the CCES was the same person as the Homer Jay Simpson in their master data file. In practice, matching is a difficult and messy process, and, to make matters worse for the researchers, Catalist considered its matching technique to be proprietary.</p>

<p>In order to validate the matching algorithms, they relied on two challenges. First, Catalist participated in a matching competition that was run by an independent, third-party: the MITRE Corporation. MITRE provided all participants two noisy data files to be matched, and different teams competed to return to MITRE the best matching. Because MITRE itself knew the correct matching they were able to score the teams. Of the 40 companies that competed, Catalist came in second place. This kind of independent, third-party evaluation of proprietary technology is quite rare and incredibly valuable; it should give us confidence that Catalist’s matching procedures are essentially at the state-of-the-art. But is the state-of-the-art good enough? In addition to this matching competition, Ansolabehere and Hersh created their own matching challenge for Catalist. From an earlier project, Ansolabehere and Hersh had collected voter records from Florida. They provided some of these records with some of their fields redacted to Catalist and then compared Catalist’s reports of these fields to their actual values. Fortunately, Catalist’s reports were close to the withheld values, indicating that Catalist could match partial voter records onto their master data file. These two challenges, one by a third-party and one by Ansolabehere and Hersh, give us more confidence in the Catalist matching algorithms, even though we cannot review their exact implementation ourselves.</p>

<p>There have been many previous attempts to validate voting. For an overview of that literature, see Belli et al. (1999), Berent, Krosnick, and Lupia (2011), Ansolabehere and Hersh (2012), and Hanmer, Banks, and White (2014).</p>

<p>It is important to note that although in this case researchers were encouraged by the quality of data from Catalist, other evaluations of commercial vendors have been less enthusiastic. Researchers have found poor quality when data from a survey to a consumer-file from Marketing Systems Group (which itself merged together data from three providers: Acxiom, Experian, and InfoUSA) (Pasek et al. 2014). That is, the data file did not match survey responses that researchers expected to be correct, the datafile had missing data for a large number of questions, and the missing data pattern was correlated to reported survey value (in other words the missing data was systematic, not random).</p>

<p>For more on record linkage between surveys and administrative data, see Sakshaug and Kreuter (2012) and Schnell (2013). For more on record linkage in general, see Dunn (1946) and Fellegi and Sunter (1969) (historical) and Larsen and Winkler (2014) (modern). Similar approaches have also been developed in computer science under the names such as data deduplication, instance identification, name matching, duplicate detection, and duplicate record detection (Elmagarmid, Ipeirotis, and Verykios 2007). There are also privacy preserving approaches to record linkage which do not require the transmission of personally identifying information (Schnell 2013). Researchers at Facebook developed a procedure to probabilisticsly link their records to voting behavior (Jones et al. 2013); this linkage was done to evaluate an experiment that I’ll tell you about in Chapter 4 (Bond et al. 2012).</p>

<p>Another example of linking a large-scale social survey to government administrative records comes from the Health and Retirement Survey and the Social Security Administration. For more on that study, including information about the consent procedure, see Olson (1996) and Olson (1999).</p>

<p>The process of combining many sources of administrative records into a master datafile—the process that Catalist employees—is common in the statistical offices of some national governments. Two researchers from Statistics Sweden have written a detailed book on the topic (Wallgren and Wallgren 2007). For an example of this approach in a single county in the United States (Olmstead County, Minnesota; home of the Mayo Clinic), see Sauver et al. (2011). For more on errors that can appear in administrative records, see Groen (2012).</p>

<h2 id="activities-1">Activities</h2>

<p>Key:</p>

<ul>
<li>degree of difficulty: easy easy, medium medium, hard hard, very hard very hard</li>
<li>requires math (requires math)</li>
<li>requires coding (requires coding)</li>
<li>data collection (data collection)</li>
<li>my favorites (my favorite)</li>
</ul>

<p>[hard, requires math] In the chapter, I was very positive about post-stratification. However, it does not always improve the quality of estimates. Construct a situation where can post-stratification can decrease the quality of estimates. (For a hint, see Thomsen (1973)).</p>

<p>[hard, data collection, requires coding] Design and conduct a non-probability survey on Amazon MTurk to ask about gun ownership (“Do you, or does anyone in your household, own a gun, rifle or pistol? Is that you or someone else in your household?”) and attitudes towards gun control (“What do you think is more important–to protect the right of Americans to own guns, or to control gun ownership?”).</p>

<ol>
<li>How long does your survey take? How much does it cost? How do the demographics of your sample compare to the demographics of the U.S. population?</li>
<li>What is the raw estimate of gun ownership using your sample?</li>
<li>Correct for the non-representativeness of your sample using post-stratification or some other technique. Now what is the estimate of gun ownership?</li>
<li>How do your estimates compare to the latest estimate from Pew Research Center? What do you think explain the discrepancies, if there is any?</li>
<li>Repeat the exercise 2-5 for attitudes toward gun control. How do your findings differ?</li>
</ol>

<p>[very hard, data collection, requires coding] Goel and colleagues (2016) administered a non-probability-based survey consisting of 49 multiple-choice attitudinal questions drawn from the General Social Survey (GSS) and select surveys by the Pew Research Center on Amazon MTurk. They then adjust for the non-representativeness of data using model-based post-stratification (Mr. P), and compare the adjusted estimates with those estimated using probability-based GSS/Pew surveys. Conduct the same survey on MTurk and try to replicate Figure 2a and Figure 2b by comparing your adjusted estimates with the estimates from the most recent rounds of GSS/Pew (See Appendix Table A2 for the list of 49 questions).</p>

<ol>
<li>Compare and contrast your results to the results from Pew and GSS.</li>
<li>Compare and contrast your results to the results from the MTurk survey in Goel, Obeng, and Rothschild (2016).</li>
</ol>

<p>[medium, data collection, requires coding] Many studies use self-report measures of mobile phone activity data. This is an interesting setting where researchers can compare self-reported behavior with logged behavior (see e.g., Boase and Ling (2013)). Two common behaviors to ask about are calling and texting, and two common time frames are “yesterday” and “in the past week.”</p>

<ol>
<li>Before collecting any data, which of the self-report measures do you think is more accurate? Why?</li>
<li>Recruit 5 of your friends to be in your survey. 3. Please briefly summarize how these 5 friends were sampled. Might this sampling procedure induce specific biases in your estimates?</li>
<li>Please ask them the following micro-survey:

<ul>
<li>“How many times did you use mobile phone to call others yesterday?”</li>
<li>“How many text messages did you send yesterday?”</li>
<li>“How many times did you use your mobile phone to call others in the last seven days?”</li>
<li>“How many times did you use your mobile phone to send or receive text messages/SMS in the last seven days?” Once the survey is complete, ask to check their usage data as logged by their phone or service provider.</li>
</ul></li>
<li>How does self-report usage compare to log data? Which is most accurate, which is least accurate?</li>
<li>Now combine the data that you have collected with the data from other people in your class (if you are doing this activity for a class). With this larger dataset, repeat part (d).</li>
</ol>

<p>[medium, data collection] Schuman and Presser (1996) argue that question orders would matter for two types of relations between questions: part-part questions where two questions are at the same level of specificity (e.g. ratings of two presidential candidates); and part-whole questions where a general question follows a more specific question (e.g. asking “How satisfied are you with your work?” followed by “How satisfied are you with your life?”).</p>

<p>They further characterize two types of question order effect: consistency effects occur when responses to a later question are brought closer (than they would otherwise be) to those given to an earlier question; contrast effects occur when there are greater differences between responses to two questions.</p>

<ol>
<li>Create a pair of part-part questions that you think will have a large question order effect, a pair of part-whole questions that you think will have a large order effect, and another pair of questions whose order you think would not matter. Run a survey experiment on MTurk to test your questions.</li>
<li>How large was the part-part effect were you able to create? Was it a consistency or contrast effect?</li>
<li>How large was the part-whole effect were you able to create? Was it a consistency or contrast effect?</li>
<li>Was there a question order effect in your pair where you did not think the order would matter?</li>
</ol>

<p>[medium, data collection] Building on the work of Schuman and Presser, Moore (2002) describes a separate dimension of question order effect: additive and subtractive. While contrast and consistency effects are produced as a consequence of respondents’ evaluations of the two items in relation to each other, additive and subtractive effects are produced when respondents are made more sensitive to the larger framework within which the questions are posed. Read Moore (2002), then design and run a survey experiment on MTurk to demonstrate additive or subtractive effects.</p>

<p>[hard, data collection] Christopher Antoun and colleagues (2015) conducted a study comparing the convenience samples obtained from four different online recruiting sources: MTurk, Craigslist, Google AdWords and Facebook. Design a simple survey and recruit participants through at least two different online recruiting sources (they can be different sources from the four sources used in Antoun et al. (2015)).</p>

<ol>
<li>Compare the cost per recruit, in terms of money and time, between different sources.</li>
<li>Compare the composition of the samples obtained from different sources.</li>
<li>Compare the quality of data between the samples. For ideas about how to measure data quality from respondents, see Schober et al. (2015).</li>
<li>What is your preferred source? Why?</li>
</ol>

<p>[medium] YouGov, an internet-based market research firm, conducted online polls of a panel of about 800,000 respondents in the UK and used Mr. P. to predict the result of EU Referendum (i.e., Brexit) where the UK voters vote either to remain in or leave the European Union.</p>

<p>A detailed description of YouGov’s statistical model is here (<a href="https://yougov.co.uk/news/2016/06/21/yougov-referendum-model/" target="_blank">https://yougov.co.uk/news/2016/06/21/yougov-referendum-model/</a>). Roughly speaking, YouGov partitions voters into types based on 2015 general election vote choice, age, qualifications, gender, date of interview, as well as the constituency they live in. First, they used data collected from the YouGov panelists to estimate, among those who vote, the proportion of people of each voter type who intend to vote Leave. They estimate turnout of each voter type by using the 2015 British Election Study (BES) post-election face-to-face survey, which validated turnout from the electoral rolls. Finally, they estimate how many people there are of each voter type in the electorate based on latest Census and Annual Population Survey (with some addition information from the BES, YouGov survey data from around the general election, and information on how many people voted for each party in each constituency).</p>

<p>Three days before the vote, YouGov showed a two point lead for Leave. On the eve of voting, the poll showed too close to call (49-51 Remain). The final on-the-day study predicted <sup>48</sup>&frasl;<sub>52</sub> in favor of Remain (<a href="https://yougov.co.uk/news/2016/06/23/yougov-day-poll/" target="_blank">https://yougov.co.uk/news/2016/06/23/yougov-day-poll/</a>). In fact, this estimate missed the final result (52-48 Leave) by four percentage points.</p>

<ol>
<li>Use the total survey error framework discussed in this chapter to assess what could have gone wrong.</li>
<li>YouGov’s response after the election (<a href="https://yougov.co.uk/news/2016/06/24/brexit-follows-close-run-campaign/" target="_blank">https://yougov.co.uk/news/2016/06/24/brexit-follows-close-run-campaign/</a>) explained: “This seems in a large part due to turnout – something that we have said all along would be crucial to the outcome of such a finely balanced race. Our turnout model was based, in part, on whether respondents had voted at the last general election and a turnout level above that of general elections upset the model, particularly in the North.” Does this change your answer to part (a)?</li>
</ol>

<p>[medium, requires coding] Write a simulation to illustrate each of the representation errors in Figure 3.1.</p>

<ol>
<li>Create a situation where these errors actually cancel out.</li>
<li>Create a situation where the errors compound each other.</li>
</ol>

<p>[very hard, requires coding] The research of Blumenstock and colleagues (2015) involved building a machine learning model that could use digital trace data to predict survey responses. Now, you are going to try the same thing with a different dataset. Kosinski, Stillwell, and Graepel (2013) found that Facebook likes can predict individual traits and attributes. Surprisingly, these predictions can be even more accurate than those of friends and colleagues (Youyou, Kosinski, and Stillwell 2015).</p>

<ol>
<li>Read Kosinski, Stillwell, and Graepel (2013), and replicate Figure 2. Their data are available here: <a href="http://mypersonality.org/" target="_blank">http://mypersonality.org/</a></li>
<li>Now, replicate Figure 3.</li>
<li>Finally, try their model on your own Facebook data: <a href="http://applymagicsauce.com/" target="_blank">http://applymagicsauce.com/</a>. How well does it work for you?</li>
</ol>

<p>[medium] Toole et al. (2015) use call detail records (CDRs) from mobile phones to predict aggregate unemployment trends.</p>

<ol>
<li>Compare and contrast the design of Toole et al. (2015) with Blumenstock, Cadamuro, and On (2015).</li>
<li>Do you think CDRs should replace traditional surveys, complement them or not be used at all for government policymakers to track unemployment? Why?</li>
<li>What evidence would convince you that CDRs can completely replace traditional measures of the unemployment rate?</li>
</ol>

<h1 id="4-running-experiments">4 Running experiments</h1>

<p>4.1 Introduction
4.2 What are experiments?
4.3 Two dimensions of experiments: lab-field and analog-digital
4.4 Moving beyond simple experiments
4.4.1 Validity
4.4.2 Heterogeneity of treatment effects
4.4.3 Mechanisms
4.5 Making it happen
4.5.1 Just do it yourself
4.5.1.1 Use existing environments
4.5.1.2 Build your own experiment
4.5.1.3 Build your own product
4.5.2 Partner with the powerful
4.6 Advice
4.6.1 Create zero variable cost data
4.6.2 Replace, Refine, and Reduce
4.7 Conclusion
- Technical appendix
- Further commentary
- Activities</p>

<h2 id="4-1-introduction">4.1 Introduction</h2>

<p>In the approaches covered so far in this book—observing behavior (Chapter 2) and asking questions (Chapter 3)—researchers collect data about what is naturally occurring in the world. The approach covered in this chapter—running experiments—is fundamentally different. When researchers run experiments, they systematically intervene in the world to create data that is ideally suited to answering questions about cause-and-effect relationships.</p>

<p>Cause-and-effect questions are very common in social research, and examples includes questions such as Does increasing teachers’ salaries increase student learning? What is the effect of minimum wage on employment rates? How does a job applicant’s race effect her chance of getting a job? In addition to these explicitly causal questions, sometimes cause-and-effect questions are implicit in more general questions about maximization of some performance metric. For example, the question “What color button will maximize donations on an NGO website site?” is really lots of questions about the effect of different button colors on donations.</p>

<p>One way to answer cause-and-effect questions is to look for patterns in existing data. For example, using data from thousands of schools, you might calculate that students learn more in schools that offer high teacher salaries. But, does this correlation show that higher salaries cause students to learn more? Of course not. Schools where teachers earn more might be different in many ways. For example, students in schools with high teacher salaries might come from wealthier families. Thus, what looks like an effect of teachers could just come from comparing different types of students. These unmeasured differences between students are called confounders, and in general, the possibility of confounders wreaks havoc on researchers ability to answer cause-and-effect questions by looking for patterns in existing data.</p>

<p>One solution to the problem of confounders is to try to make fair comparisons by adjusting for observable differences between the groups. For example, you might be able to download property tax data from a number of government websites. Then, you could compare student performance in schools where home prices are similar but teacher salaries are different, and you still might find that students learn more in schools with higher teacher pay. But, there are still many possible confounders. Maybe the parents of these student differ in their level of education or maybe the schools differ in their closeness to public libraries or maybe the schools with higher teacher pay also have higher pay for principals and principal pay, not teacher pay, is really what is increasing student learning. You could try to measure these other factors as well, but the list of possible confounders is essentially endless. In many situations, you just cannot measure and adjust for all the possible confounders. This approach can only take you so far.</p>

<p>A better solution to the problem of confounders is running experiments. Experiments enable researchers to move beyond the correlations in naturally occurring data in order to reliably answer cause-and-effect question. In the analog age, experiments were often logistically difficult and expensive. Now, in the digital age, logistical constraints are gradually fading away. Not only is it easier to do experiments like those researchers have done in the past, it is now possible to run new kinds of experiments.</p>

<p>In what I’ve written so far I’ve been a bit loose in my language, but it is important to distinguish between two things: experiments and randomized controlled experiments. In an experiment a researcher intervenes in the world and then measures an outcome. I’ve heard this approach described as “perturb and observe.” This strategy is very effective in the natural sciences, but in medical and social sciences, there is another approach that works better. In a randomized controlled experiment a researcher intervenes for some people and not for others, and, critically, the researcher decides which people receive the intervention by randomization (e.g., flipping a coin). This procedure ensures that randomized controlled experiments create fair comparisons between two groups: one that has received the intervention and one that has not. In other words, randomized controlled experiments are a solution to the problems of confounders. Despite the important differences between experiments and randomized controlled experiments, social researchers often use these terms interchangeably. I’ll follow this convention, but, at certain points, I’ll break the convention to emphasize the value of randomized controlled experiments over experiments without randomization and a control group.</p>

<p>Randomized controlled experiments have proven to be a powerful way to learn about the social world, and in this chapter, I’ll teach you more about how to use them in your research. In Section 4.2, I’ll illustrate the basic logic of experimentation with an example of an experiment on Wikipedia. Then, in Section 4.3, I’ll describe the difference between lab experiments and field experiments and the differences between analog experiments and digital experiments. Further, I’ll argue that digital field experiments can offer the best features of analog lab experiments (tight control) and analog field experiments (realism), all at a scale that was not possible previously. Next, in Section 4.4, I’ll describe three concepts—validity, heterogeneity of treatment effects, and mechanisms—that are critical for designing rich experiments. With that background, I’ll describe the trade-offs involved in the two main strategies for conducting digital experiments: doing it yourself (Section 4.5.1) or partnering with the powerful (Section 4.5.2). Finally, I’ll conclude with some design advice about how you can take advantage of the real power of digital experiments (Section 4.6.1) and describe some of responsibility that comes with that power (Section 4.6.2). The chapter will be presented with a minimum of mathematical notation and formal language; readers interested in a more formal, mathematical approach to experiments should also read the Technical Appendix at the end of the chapter.</p>

<h2 id="4-2-what-are-experiments">4.2 What are experiments?</h2>

<blockquote>
<p>Randomized controlled experiments have four main ingredients: recruitment of participants, randomization of treatment, delivery of treatment, and measurement of outcomes.</p>
</blockquote>

<p>Randomized controlled experiments can take many forms and can be used to study many types of behavior. But, at their core, randomized controlled experiments have four main ingredients: recruitment of participants, randomization of treatment, delivery of treatment, and measurement of outcomes. The digital age does not change the fundamental nature of experimentation, but it does make them easier logistically. For example, in the past it might have been difficult to measure the behavior of millions of people, but that is now routinely happening in many digital systems. Researchers who can figure out how to harness these new opportunities will be able to run experiments that were impossible previously.</p>

<p>To make this all a bit more concrete—both what has stayed the same and what has changed—let’s consider Michael Restivo and Arnout van de Rijt’s (2012). The researchers wanted to understand the effect of informal peer rewards on editorial contributions to Wikipedia. In particular, they studied the effects of barnstars, an award that any Wikipedian can give to any other Wikipedian to acknowledge hard work and due diligence. Restivo and van de Rijt gave barnstars to 100 deserving Wikipedians. Then, Restivo and van de Rijt tracked the recipients’ subsequent contributions to Wikipedia over the next 90 days. Much to their surprise, the people to whom they awarded barnstars tended to make fewer edits after receiving one. In other words, the barnstars seemed to be discouraging rather than encouraging contribution.</p>

<p>Fortunately, Restivo and van de Rijt were not running a “perturb and observe” experiment; they were running a randomized controlled experiment. So, in addition to choosing 100 top contributors to receive a barnstar, they also picked 100 top contributors to whom they did not give a barnstar. These hundred served as a control group, and who got a barnstar and who didn’t was determined randomly. When Restivo and van de Rijt looked at the control group they found that it had a steep drop in contributions too. Finally, when the researchers compared people in the treatment group (i.e., received barnstars) and people in the control group, they found that the barnstar caused editors to contribute about 60% more. But, this increase in contribution was taking place as part of an overall decline in both groups.</p>

<p>As this study illustrates, the control group in experiments is critical in a way that is somewhat paradoxical. In order to precisely measure the effect of barnstars, Restivo and van der Rijt needed to observe people that did not receive barnstars. Many times researchers who are not familiar with experiments fail to appreciate the incredible value of the control group. If Restivo and van de Rijt didn’t have a control group, they would have drawn exactly the wrong conclusion. Control groups are so important that the CEO of a major casino company has said that there are only three ways that employees can be fired from his company: theft, sexual harassment, and running an experiment without a control group (Schrage 2011).</p>

<p>Restivo and van de Rijt’s study illustrates the four main ingredients of an experiment: recruitment, randomization, intervention, and outcomes. Together, these four ingredients allow scientists to move beyond correlations and measure the causal effect of treatments. Specifically, randomization means that when you compare outcomes for the treatment and control groups you get an estimate of the causal effect of that intervention for that set of participants. In other words, with a randomized controlled experiment you can be sure that any differences in outcomes are caused by the intervention and not a confounder, a claim that I make precise in the Technical Appendix using the potential outcomes framework.</p>

<p>In addition to being a nice illustration of the mechanics of experiments, Restivo and van de Rijt’s study also shows that the logistics of digital experiments can be completely different from analog experiments. In Restivo and van de Rijt’s experiment, it was easy to give the barnstar to anyone in the world and it was easy to track the outcome—number of edits—over an extended period of time (because edit history is automatically recorded by Wikipedia). This ability to deliver treatments and measure outcomes at no cost is qualitatively unlike experiments in the past. Although this experiment involved 200 people, it could have been run with 2,000 or 20,000 people. The main thing preventing the researchers from scaling up their experiment by a factor of 100 was not cost, it was ethics. That is, Restivo and van de Rijt didn’t want to give barnstars to undeserving editors and they didn’t want their experiment to disrupt the Wikipedia community (Restivo and Rijt 2012; Restivo and Rijt 2014). So, although the experiment of Restivo and van de Rijt is relatively simple, it clearly shows that some things about experiments have stayed the same and some have changed. In particular, the basic logic of experimentation is the same, but the logistics have changed. Next, in order to more clearly isolate the opportunities created by this change, I’ll compare the experiments that researchers can do now to the kinds of experiments that have been done in the past.</p>

<h2 id="4-3-two-dimensions-of-experiments-lab-field-and-analog-digital">4.3 Two dimensions of experiments: lab-field and analog-digital</h2>

<blockquote>
<p>Lab experiments offer control, field experiments offer realism, and digital field experiments combine control and realism at scale.</p>
</blockquote>

<p>Experiments come in many different shapes and sizes. But, despite these differences, researchers have found it helpful to organize experiments along a continuum between lab experiments and field experiments. Now, however, researchers should also organize experiments along a continuum between analog experiments and digital experiments. This two-dimensional design space will help you understand the strengths and weaknesses of different approaches and suggest areas of greatest opportunity (Figure 4.1).</p>

<p><img src="http://oaf2qt3yk.bkt.clouddn.com/258535053919dd099aea28e121d1c280.png" alt="fig4.1" /></p>

<p>Figure 4.1: Schematic of design space for experiments. In the past, experiments varied along the lab-field dimension. Now, they also vary on the analog-digital dimension. In my opinion, the area of greatest opportunity is digital field experiments.</p>

<p>In the past, the main way that researchers organized experiments was along the lab-field dimension. The majority of experiments in the social sciences are lab experiments where undergraduate students perform strange tasks in a lab for course credit. This type of experiment dominates research in psychology because it enables researchers to create very specific treatments designed to test very specific theories about social behavior. For certain problems, however, something feels a bit strange about drawing strong conclusions about human behavior from such unusual people performing such unusual tasks in such an unusual setting. These concerns have led to a movement toward field experiments. Field experiments combine the strong design of randomized control experiments with more representative groups of participants, performing more common tasks, in more natural settings.</p>

<p>Although some people think of lab and field experiments as competing methods, it is best to think of them as complementary methods with different strengths and weaknesses. For example, Correll, Benard, and Paik (2007) used both a lab experiment and a field experiment in an attempt to find the sources of the “motherhood penalty.” In the United States, mothers earn less money than childless women, even when comparing women with similar skills working in similar jobs. There are many possible explanations for this pattern, and one is that employers are biased against mothers. (Interestingly, the opposite seems to be true for fathers: they tend to earn more than comparable childless men). In order to assess possible bias against mothers, Correll and colleagues ran two experiments: one in the lab and one in the field.</p>

<p>First, in a lab experiment Correll and colleagues told participants, who were college undergraduates, that a California-based start-up communications company was conducting an employment search for a person to lead its new East Coast marketing department. Students were told that the company wanted their help in the hiring process and they were asked to review resumes of several potential candidates and to rate the candidates on a number of dimensions such as their intelligence, warmth, and commitment to work. Further, the students were asked if they would recommend hiring the applicant and what they would recommend as a starting salary. Unbeknownst to the students, however, the resumes were specifically constructed to be similar except for one thing: some of the resumes signaled motherhood (by listing involvement in a parent-teacher association) and some did not. Correll found that students were less likely to recommend hiring the mothers and offered them lower starting salary. Further, through a statistical analysis of both the ratings and the hiring-related decisions, Correll found that mothers’ disadvantages were largely explained by the fact that mothers were rated lower in terms of competence and commitment. In other words, Correll argues that these traits are the mechanism through which mothers are disadvantaged. Thus, this lab experiment allowed Correll and colleagues to measure a causal effect and provide a possible explanation for that effect.</p>

<p>Of course, one might be skeptical about drawing conclusions about the entire US labor market based on the decisions of a few hundred undergraduates who have probably never had a full time job, let alone hired people. Therefore, Correll and colleagues also conducted a complementary field experiment. The researchers responded to hundreds of advertised job openings by sending in fake cover letters and resumes. Similar to the materials shown to the undergraduates, some resumes signaled motherhood and some did not. Correll and colleagues found that mothers were less likely to get called back for interviews than equally qualified childless women. In other words, real employers making consequential decisions in a natural setting behaved much like the undergraduates. Did they make similar decisions for the same reason? Unfortunately, we don’t know. The researchers were not able to ask the employers to rate the candidates or explain their decisions.</p>

<p>This pair of experiments reveals a lot about lab and field experiments in general. Lab experiments offer researchers near total control of the environment in which participants are making decisions. So, for example, in the lab experiment, Correll was able to ensure that all the resumes were read in a quiet setting; in the field experiment, some of the resumes might not have even been read. Further, because participants in the lab setting know that they are being studied, researchers are often able to collect additional data that can help them understand why participants are making their decisions. For example, Correll asked participants in the lab experiment to rate the candidates on different dimensions. This kind of process data could help researchers understand the mechanisms behind differences in how participants treat the resumes.</p>

<p>On the other hand, these exact same characteristics that I just described as advantages are also sometimes considered disadvantages. Researchers who prefer field experiments argue that participants in lab experiments could act very differently when they are being closely observed. For example, in the lab experiment participants might have guessed the goal of the research and altered their behavior so as not to appear biased. Further, researchers who prefer field experiments might argue that small differences on resumes can only stand out in a very clean, sterile lab environment, and thus the lab experiment will over-estimate the effect of motherhood on real hiring decisions. Finally, many proponents of field experiments criticize lab experiments reliance on WEIRD participants: mainly students from Western, Educated, Industrialized, Rich, and Democratic countries (Henrich, Heine, and Norenzayan 2010). The experiments by Correll and colleagues (2007) illustrate the two extremes on the lab-field continuum. In between these two extremes there are a variety of hybrid designs including approaches such as bringing non-students into a lab or going into the field but still having participants perform an unusual task.</p>

<p>In addition to the lab-field dimension that has existed in the past, the digital age means that researchers now have a second major dimension along which experiments can vary: analog-digital. Just as there are pure lab experiments, pure field experiments, and a variety of hybrids in between, there are pure analog experiments, pure digital experiments, and a variety of hybrids. It is tricky to offer a formal definition of this dimension, but a useful working definition is that fully digital experiments are experiments that make use of digital infrastructure to recruit participants, randomize, deliver treatments, and measure outcomes. For example, Restivo and van de Rijt’s (2012) study of barnstars and Wikipedia was a fully digital experiment because it used digital systems for all four of these steps. Likewise fully analog experiments are experiments that do not make use of digital infrastructure for any of these four steps. Many of the classic experiments in psychology are analog experiments. In between these two extremes there are partially digital experiments that use a combination of analog and digital systems for the four steps.</p>

<p>Critically, the opportunities to run digital experiments are not just online. Researchers can run partially digital experiments by using digital devices in the physical world in order to deliver treatments or measure outcomes. For example, researchers could use smart phones to deliver treatments or sensors in the built environment to measure outcomes. In fact, as we will see later in this chapter, researchers have already used home power meters to measure outcomes in experiments about social norms and energy consumption involving 8.5 million of households (Allcott 2015). As digital devices become increasingly integrated into people’s lives and sensors become integrated into the built environment, these opportunities to run partially digital experiments in the physical world will increase dramatically. In other words, digital experiments are not just online experiments.</p>

<p>Digital systems create new possibilities for experiments everywhere along the lab-field continuum. In pure lab experiments, for example, researchers can use digital systems for finer measurement of participants’ behavior; one example of this type of improved measurement is eye-tracking equipment which provides precise and continuous measures of gaze location. The digital age also creates the possibility to run lab-like experiments online. For example, researchers have rapidly adopted Amazon Mechanical Turk (MTurk) to recruit participants for online experiments (Figure 4.2). MTurk matches “employers” who have tasks that need to be completed with “workers” who wish to complete those tasks for money. Unlike traditional labor markets, however, the tasks involved usually only require a few minutes to complete and the entire interaction between employer and worker is virtual. Because MTurk mimics aspects of traditional lab experiments—paying people to complete tasks that they would not do for free—it is naturally suited for certain types of experiments. Essentially, MTurk has created the infrastructure for managing a pool of participants—recruiting and paying people—and researchers have taken advantage of that infrastructure to tap into an always available pool of participants.</p>

<p><img src="http://oaf2qt3yk.bkt.clouddn.com/d765780c3a5f82e192770ad25edacf25.png" alt="fig4.2" /></p>

<p>Figure 4.2: Papers published using data from Amazon Mechanical Turk (MTurk) (Bohannon 2016). MTurk and other online labor markets offer researchers a convenient way to recruit participants for experiments.</p>

<p>Digital experiments create even more possibilities for field-like experiments. Digital field experiments can offer tight control and process data to understand possible mechanisms (like lab experiments) and more diverse participants making real decisions in a natural environment (like field experiments). In addition to this combination of good characteristics of earlier experiments, digital field experiments also offer three opportunities that were difficult in analog lab and field experiments.</p>

<p>First, whereas most analog lab and field experiments have hundreds of participants, digital field experiments can have millions of participants. This change in scale is because some digital experiments can produce data at zero variable cost. That is, once researchers have created an experimental infrastructure, increasing the number of participants typically does not increase the cost. Increasing the number of participants by a factor of 100 or more is not just a quantitative change, it is a qualitative change, because it enables researchers to learn different things from experiments (e.g., heterogeneity of treatment effects) and run entirely different experimental designs (e.g., large group experiments). This point is so important, I’ll return to it towards the end of the chapter when I offer advice about creating digital experiments.</p>

<p>Second, whereas most analog lab and field experiments treat participants as indistinguishable widgets, digital field experiments often use background information about participants in the design and analysis stages of the research. This background information, which is called pre-treatment information, is often available in digital experiments because they take place in fully measured environments. For example, a researcher at Facebook has much more pre-treatment information than a researcher designing a standard lab experiment with undergraduates. This pre-treatment information enables researchers to move beyond treating participants as indistinguishable widgets. More specifically, pre-treatment information enables more efficient experimental designs—such as blocking (Higgins, Sävje, and Sekhon 2016) and targeted recruitment of participants (Eckles, Kizilcec, and Bakshy 2016)—and more insightful analysis—such as estimation of heterogeneity of treatment effects (Athey and Imbens 2016a) and covariate adjustment for improved precision (Bloniarz et al. 2016).</p>

<p>Third, whereas many analog lab and field experiments deliver treatments and measure outcomes in a relatively compressed amount of time, some digital field experiments involve treatments that can be delivered over time and the effects can also be measured over time. For example, Restivo and van de Rijt’s experiment has the outcome measured daily for 90 days, and one of the experiments I’ll tell you about later in the chapter (Ferraro, Miranda, and Price 2011) tracks outcomes over 3 years at basically no cost. These three opportunities—size, pre-treatment information, and longitudinal treatment and outcome data—are most common when experiments are run on top of always-on measurements systems (see Chapter 2 for more on always-on measurement systems).</p>

<p>While digital field experiments offer many possibilities, they also share some weaknesses with both analog lab and field experiments. For example, experiments cannot be used to study the past, and they can only estimate the effects of treatments that can be manipulated. Also, although experiments are undoubtedly useful to guide policy, the exact guidance they can offer is somewhat limited because of complications such as environmental dependence, compliance problems, and equilibrium effects (Banerjee and Duflo 2009; Deaton 2010). Finally, digital field experiments magnify the ethical concerns created by field experiments. Proponents of field experiments trumpet their ability to unobtrusively and randomly intervene into consequential decisions made by millions of people. These features offer certain scientific advantages, but they can also make field experiments ethically complex (think about it as researchers treating people like “lab rats” on a massive scale). Further, in addition to possible harms to participants, digital field experiments, because of their scale, can also raise concerns about the disruption of working social systems (e.g., concerns about disrupting Wikipedia’s reward system if Restivo and van der Rijt gave too many barnstars).</p>

<h2 id="4-4-moving-beyond-simple-experiments">4.4 Moving beyond simple experiments</h2>

<blockquote>
<p>Let’s move beyond simple experiments. Three concepts are useful for rich experiments: validity, heterogeneity of treatment effects, and mechanism.</p>
</blockquote>

<p>Researchers who are new to experiments often focus on a very specific, narrow question: does this treatment “work”? For example, does a phone call from a volunteer encourage someone to vote? Does changing a website button from blue to green increase click-through rate? Unfortunately, loose phrasing about what “works” obscures the fact that narrowly focused experiments don’t really tell you whether a treatment “works” in a general sense. Rather, narrowly focused experiments answer a much more specific question: what is the average effect of this specific treatment with this specific implementation for this population of participants at this time? I’ll call experiments that focus on this narrow question simple experiments.</p>

<p>Simple experiments can provide valuable information, but they fail to answer many questions that are both important and interesting such as: are there some people for whom the treatment had a larger or smaller effect?; is there another treatment that would be more effective?; and how does this experiment relate to broader social theories?</p>

<p>In order to show the value of moving beyond simple experiments, let’s consider one of my favorite analog field experiments, a study by P. Wesley Schultz and colleagues on the relationship between social norms and energy consumption (Schultz et al. 2007). Schultz and colleagues hung doorhangers on 300 households in San Marcos, California, and these doorhangers delivered different messages designed to encourage energy conservation. Then, Schultz and colleagues measured the effect of these messages on electricity consumption, both after one week and three weeks; see Figure 4.3 for a more detailed description of the experimental design.</p>

<p><img src="http://oaf2qt3yk.bkt.clouddn.com/92b8eda01a164d1181f85f1a65ab0da1.png" alt="fig4.3" /></p>

<p>Figure 4.3: Schematic of design from Schultz et al. (2007). The field experiment involved visiting about 300 households in San Marcos, California five times over an eight week period. On each visit the researchers manually took a reading from the house’s power meter. On two of the visits the researchers placed doorhangers on the house providing some information about their energy usage. The research question was how the content of these messages would impact energy use.</p>

<p>The experiment had two conditions. In the first condition, households received general energy saving tips (e.g., use fans instead of air conditioners) and information about their household’s energy usage compared to the average of the energy usage in their neighborhood. Schultz and colleagues called this the descriptive normative condition because the information about the energy use in their neighborhood provided information about typical behavior (i.e., a descriptive norm). When Schultz and colleagues looked at the resulting energy usage in this group, the treatment appeared to have no effect, either in the short-term or the long-term; in other words, the treatment didn’t seem to “work” (Figure 4.4).</p>

<p>But, fortunately, Schultz et al. (2007) did not settle for this simplistic analysis. Before the experiment began they reasoned that heavy users of electricity—people above the mean—might reduce their consumption, and that light users of electricity—people below the mean—might actually increase their consumption. When they looked at the data, that’s exactly what they found (Figure 4.4). Thus, what looked like a treatment that was having no effect was actually a treatment that had two offsetting effects. The researchers called this counter-productive increase among the light users a boomerang effect.</p>

<p><img src="http://oaf2qt3yk.bkt.clouddn.com/f69d15b861dd45c4ec08f4e6819a2c3f.png" alt="fig4.4" /></p>

<p>Figure 4.4: Results from Schultz et al. (2007). The first panel shows that the descriptive norm treatment has an estimated zero average treatment effect. However, the second panel shows that this average treatment effect is actually composed of two offsetting effects. For heavy users, the treatment decreased usage but for light users, the treatment increased usage. Finally, the third panel shows that the second treatment, which used descriptive and injunctive norms, had roughly the same effect on heavy users but mitigated the boomerang effect on light users.</p>

<p>Further, Schultz and colleagues anticipated this possibility, and in the second condition they deployed a slightly different treatment, one explicitly designed to eliminate the boomerang effect. The households in the second condition received the exact same treatment—general energy saving tips and information about their household’s energy usage compared to the their neighborhood—with one tiny addition: for people with below-average consumption, the researchers added a :) and for people with above-average consumption they added a :(. These emoticons were designed to trigger what the researchers called injunctive norms. Injunctive norms refer to perceptions of what is commonly approved (and disapproved) whereas descriptive norms refer to perceptions of what is commonly done (Reno, Cialdini, and Kallgren 1993).</p>

<p>By adding this one tiny emoticon, the researchers dramatically reduced the boomerang effect (Figure 4.4). Thus, by making this one simple change—a change that was motivated by an abstract social psychological theory (Cialdini, Kallgren, and Reno 1991)—the researchers were able to turn a program from one that didn’t seem to work into one that worked, and, simultaneously, they were able to contribute to the general understanding of how social norms affect human behavior.</p>

<p>At this point, however, you might notice that something is a bit different about this experiment. In particular, the experiment of Schultz and colleagues doesn’t really have a control group in the same way that randomized controlled experiments do. The comparison between this design and the design of Restivo and van de Rijt illustrates the differences between two major designs used by researchers. In between-subjects designs, such as Restivo and van de Rijt, there is a treatment group and a control group, and in within-subjects designs the behavior of participants is compared before and after the treatment (Greenwald 1976; Charness, Gneezy, and Kuhn 2012). In a within-subject experiment it is as if each participant acts as her own control group. The strength of between-subjects designs is that it provides protection against confounders (as I described earlier), and the strength of within-subjects experiments is increased precision in estimates. When each participant acts as their own control, between-participant variation is eliminated (see Technical Appendix). To foreshadow an that will come later when I offer advice about designing digital experiments, there is a final design, called a mixed design, that combines the improved precision of within-subjects designs and the protection against confounding of between-subjects designs.</p>

<p><img src="http://oaf2qt3yk.bkt.clouddn.com/4c860a773a714864fc96c6fcfc63bdcd.png" alt="fig4.5" /></p>

<p>Figure 4.5: Three experimental designs. Standard randomized controlled experiments use between-subjects designs. An example of a between-subjects design is Restivo and van de Rijt’s (2012) experiment on barnstars and contributions to Wikipedia: researchers randomly divided participants into treatment and control groups, gave participants in the treatment group a barnstar, and compared outcomes for the two groups. A second type of design is a within-subjects design. The two experiments in Schultz and colleague’s (2007) study on social norms and energy use illustrate a within-subjects design: researchers compared the electricity use of participants before and after receiving the treatment. Within-subjects designs offer improved statistical precision by eliminating between subject variance (see Technical Appendix), but they are open to possible confounders (e.g., changes in weather between the pre-treatment and treatment period) (Greenwald 1976; Charness, Gneezy, and Kuhn 2012). Within-subjects designs are also sometimes called repeated measures designs. Finally, mixed designs combine the improved precision of within-subjects designs and the protection against confounding of between-subjects designs. In a mixed design, a researcher compares the change in outcomes for people in the treatment and control groups. When researchers already have pre-treatment information, as is the case in many digital experiments, mixed designs are preferable to between-subjects designs because of gains in precision (see Technical Appendix).</p>

<p>Overall, the design and results of Schultz et al. (2007) show the value of moving beyond simple experiments. Fortunately, you don’t need to be a genius to create experiments like this. Social scientists have developed three concepts that will guide you toward richer and more creative experiments: 1) validity, 2) heterogeneity of treatment effects, and 3) mechanisms. That is, if you keep these three ideas in mind while you are designing your experiment, you will naturally create more interesting and useful experiments. In order to illustrate these three concepts in action, I’ll describe a number of follow-up partially digital field experiments that built on the elegant design and exciting results in Schultz et al. (2007). As you will see, through more careful design, implementation, analysis, and interpretation, you too can move beyond simple experiments.</p>

<h3 id="4-4-1-validity">4.4.1 Validity</h3>

<blockquote>
<p>Validity refers to how much the results of an experiment support a more general conclusion.</p>
</blockquote>

<p>No experiment is perfect, and researchers have developed an extensive vocabulary to describe possible problems. Validity refers to the extent to which the results of a particular experiment support some more general conclusion. Social scientists have found it helpful to split validity into four main types: statistical conclusion validity, internal validity, construct validity, and external validity (Shadish, Cook, and Campbell 2001, Ch 2). Mastering these concepts will provide you a mental checklist for critiquing and improving the design and analysis of an experiment, and it will help you communicate with other researchers.</p>

<p>Statistical conclusion validity centers around whether the statistical analysis of the experiment was done correctly. In the context of Schultz et al. (2007) such question might center on whether they computed their p-values correctly. Statistical analysis is beyond the scope of this book, but I can say that the statistical principles needed to design and analyze experiments have not changed in the digital age. However, the different data environment in digital experiments does create new statistical opportunities (e.g., using machine learning methods to estimate heterogeneity of treatment effects (Imai and Ratkovic 2013)) and new computational challenges (e.g., blocking in massive experiments (Higgins, Sävje, and Sekhon 2016)).</p>

<p>Internal validity centers around whether the experimental procedures were performed correctly. Returning to the experiment of Schultz et al. (2007), questions about internal validity could center around the randomization, delivery of the treatment, and measurement of outcomes. For example, you might be concerned that the research assistants did not read the electric meters reliably. In fact, Schultz and colleagues were worried about this problem and they had a sample of meters read twice; fortunately, the results were essentially identical. In general, Schultz and colleagues’ experiment appears to have high internal validity, but this is not always the case; complex field and online experiments often run into problems actually delivering the right treatment to the right people and measuring the outcomes for everyone. Fortunately, the digital age can help reduce concerns about internal validity because it makes it easier to ensure that the treatment is delivered as designed to those who are supposed to receive it and to measure outcomes for all participants.</p>

<p>Construct validity centers around the match between the data and the theoretical constructs. As discussed in Chapter 2, constructs are abstract concepts that social scientists reason about. Unfortunately, these abstract concepts don’t always have clear definitions and measurements. Returning to Schultz et al. (2007), the claim that injunctive social norms can lower electricity use requires researchers to design a treatment that would manipulate “injunctive social norms” (e.g., an emoticon) and to measure “electricity use”. In analog experiments, many researchers designed their own treatments and measured their own outcomes. This approach ensures that, as much as possible, the experiments match the abstract constructs being studied. In digital experiments where researchers partner with companies or governments to deliver treatments and use always-on data systems to measure outcomes, the match between the experiment and the theoretical constructs may be less tight. Thus, I expect that construct validity will tend to be a bigger concern in digital experiments than analog experiments.</p>

<p>Finally, external validity centers around whether the results of this experiment would generalize to other situations. Returning to Schultz et al. (2007), one could ask, will this same idea—providing people information about their energy usage in relationship to their peers and a signal of injunctive norms (e.g., an emoticon)—reduce energy usage if it was done in a different way in a different setting? For most well-designed and well-run experiments, concerns about external validity are the hardest to address. In the past, these debates about external validity were frequently just a bunch of people sitting in a room trying to imagine what would have happened if the procedures were done in a different way, or in a different place, or with different people. Fortunately, the digital age enables researchers to move beyond these data-free speculations and assess external validity empirically.</p>

<p>Because the results from Schultz et al. (2007) were so exciting, a company named Opower partnered with utilities in the United States to deploy the treatment more widely. Based on the design of Schultz et al. (2007), Opower created customized Home Energy Reports that had two main modules, one showing a household’s electricity usage relative to its neighbors with an emoticon and one providing tips for lowering energy usage (Figure 4.6). Then, in partnership with researchers, Opower ran randomized controlled experiments to assess the impact of the Home Energy Reports. Even though the treatments in these experiments were typically delivered physically—usually through old fashioned snail mail—the outcome was measured using digital devices in the physical world (e.g., power meters). Rather than manually collecting this information with research assistants visiting each house, the Opower experiments were all done in partnership with power companies enabling the researchers to access the power readings. Thus, these partially digital field experiments were run at a massive scale at low variable cost.</p>

<p><img src="http://oaf2qt3yk.bkt.clouddn.com/5915ed8848b985c8933707d3c5820467.png" alt="fig4.6" /></p>

<p>Figure 4.6: The Home Energy Reports in Allcott (2011) had a Social Comparison Module and an Action Steps Module.</p>

<p>In a first set of experiments involving 600,000 households served by 10 utility companies around the United States, Allcott (2011) found the Home Energy Report lowered electricity consumption by 1.7%. In other words, the results from the much larger, more geographically diverse study were qualitatively similar to the results from Schultz et al. (2007). But, the effect size was smaller: in Schultz et al. (2007) the households in the descriptive and injective norms condition (the one with the emoticon) reduced their electricity usage by 5%. The precise reason for this difference is unknown, but Allcott (2011) speculated that receiving a handwritten emoticon as part of a study sponsored by a university might have a larger effect on behavior than receiving a printed emoticon as part of a mass produced report from a power company.</p>

<p>Further, in subsequent research, Allcott (2015) reported on an additional 101 experiments involving an additional 8 million households. In these next 101 experiments the Home Energy Report continued to cause people to lower their electricity consumption, but the effects were even smaller. The precise reason for this decline is not known, but Allcott (2015) speculated that the effectiveness of the report appeared to be declining over time because it was actually being applied to different types of participants. More specifically, utilities in more environmentalist areas were more likely adopt the program earlier and their customers were more responsive to the treatment. As utilities with less environmental customers adopted the program, its effectiveness appeared to decline. Thus, just as randomization in experiments ensures that the treatment and control group are similar, randomization in research sites ensures that the estimates can be generalized from a one group of participants to a more general population (think back to Chapter 3 about sampling). If research sites are not sampled randomly, then generalization—even from a perfectly designed and conducted experiment—can be problematic.</p>

<p>Together, these 111 experiments—10 in Allcott (2011) and 101 in Allcott (2015)—involved about 8.5 million households from all over the United States. They consistently show that Home Energy Reports reduce average electricity consumption, a result that supports the original findings of Schultz and colleagues from 300 homes in California. Beyond just replicating these original results, the follow-up experiments also show that the size of the effect varies by location. This set of experiments also illustrates two more general points about partially digital field experiments. First, researchers will be able to empirically address concerns about external validity when the cost of running experiments is low, and this can occur if the outcome is already being measured by an always-on data system. Therefore, it suggests that research should be on the look-out for other interesting and important behaviors that are already being recorded, and then design experiments on top of this existing measuring infrastructure. Second, this set of experiments reminds us that digital field experiments are not just online; increasingly I expect that they will be everywhere with many outcomes measured by sensors in the built environment.</p>

<p>The four types of validity—statistical conclusion validity, internal validity, construct validity, external validity—provide a mental checklist to help researchers assess whether the results from a particular experiment support a more general conclusion. Compared to analog age experiments, in digital age experiments it should be easier to address external validity empirically and it should be easier to ensure internal validity. On the other hand, issues of construct validity will probably be more challenging in digital age experiments (although that was not the case with the Opower experiments).</p>

<h3 id="4-4-2-heterogeneity-of-treatment-effects">4.4.2 Heterogeneity of treatment effects</h3>

<blockquote>
<p>Experiments normally measure the average effect, but the effect can be different for different people.</p>
</blockquote>

<p>The second key idea for moving beyond simple experiments is heterogeneity of treatment effects. The experiment of Schultz et al. (2007) powerfully illustrates how the same treatment can have different effects on different kinds of people (Figure 4.4), but this analysis of heterogeneity is actually quite unusual for an analog age experiment. Most analog age experiments involve a small number of participants that are treated as interchangeable “widgets” because little about them is known pre-treatment. In digital experiments, however, these data constraints are less common because researchers tend to have more participants and know more about them. In this different data environment, we can estimate heterogeneity of treatment effects in order to provide clues about how the treatment works, how it can be improved, and how it can be targeted to those mostly likely to benefit.</p>

<p>Two examples of heterogeneity of treatment effects in the context of social norms and energy use come from additional research on the Home Energy Reports. First, Allcott (2011) used the large sample size (600,000 households) to further split the sample and estimate the effect of the Home Energy Report by decile of pre-treatment energy usage. While Schultz et al. (2007) found differences between heavy and light users, Allcott (2011) found that there were also differences within the heavy and light user group. For example, the heaviest users (those in the top decile) reduced their energy usage twice as much as someone in the middle of the heavy user group (Figure 4.7). Further, estimating the effect by pre-treatment behavior also revealed that there was not a boomerang effect even for the lightest users (Figure 4.7).</p>

<p><img src="http://oaf2qt3yk.bkt.clouddn.com/c424b8e630762465e1641b2d1d9bc0c7.png" alt="fig4.7" /></p>

<p>Figure 4.7: Heterogeneity of treatment effects in Allcott (2011). The decrease in energy use was different for people in different deciles of baseline usage.</p>

<p>In a related study, Costa and Kahn (2013) speculated that the effectiveness of the Home Energy Report could vary based on a participant’s political ideology and that the treatment might actually cause people with certain ideologies to increase their electricity use. In other words, they speculated that the Home Energy Reports might be creating a boomerang effect for some types of people. To assess this possibility, Costa and Kahn merged the Opower data with data purchased from a third-party aggregator that included information such as political party registration, donations to environment organizations, and household participation in renewable energy programs. With this merged dataset, Costa and Kahn found that the Home Energy Reports produced broadly similar effects for participants with different ideologies; there was no evidence that any group exhibited boomerang effects (Figure 4.8).</p>

<p><img src="http://oaf2qt3yk.bkt.clouddn.com/a7fa530346919085bdb5760654cdb917.png" alt="fig4.8" /></p>

<p>Figure 4.8: Heterogeneity of treatment effects in Costa and Kahn (2013). The estimated average treatment effect for the entire sample is -2.1% [-1.5%, -2.7%]. By combining information from the experiment with information about the households, Costa and Kahn (2013) used a series of statistical models to estimate the treatment effect for very specific groups of people. Two estimates are presented for each group because the estimates depend on the covariates they included in their statistical models (see model 4 and model 6 in Table 3 and Table 4 in Costa and Kahn (2013)). As this example illustrates, treatment effects can be different for different people and estimates of treatment effects that come from statistical models can depend on the details of those models (Grimmer, Messing, and Westwood 2014).</p>

<p>As these two examples illustrate, in the digital age, we can move from estimating average treatment effects to estimating the heterogeneity of treatment effects because we can have many more participants and we know more about those participants. Learning about heterogeneity of treatment effects can enable targeting of a treatment where it is most effective, provide facts that stimulate new theory development, and provide hints about a possible mechanism, the topic to which I now turn.</p>

<h3 id="4-4-3-mechanisms">4.4.3 Mechanisms</h3>

<blockquote>
<p>Experiments measure what happened. Mechanisms explain why and how it happened.</p>
</blockquote>

<p>The third key idea for moving beyond simple experiments is mechanisms. Mechanisms tell us why or how a treatment caused an effect. The process of searching for mechanisms is also sometimes called looking for intervening variables or mediating variables. Although experiments are good for estimating causal effects, they are often not designed to reveal mechanisms. Digital age experiments can help us identify mechanisms in two ways: 1) they enable us to collect more process data and 2) they enable us to test many related treatments.</p>

<p>Because mechanisms are tricky to define formally (Hedström and Ylikoski 2010), I’m going to start with a simple example: limes and scurvy (Gerber and Green 2012). In the 18th century doctors had a pretty good sense that when sailors ate limes they did not get scurvy. Scurvy is a terrible disease so this was powerful information. But, these doctors did not know why limes prevented scurvy. It was not until 1932, almost 200 years later, that scientists could reliably show that vitamin C was the reason that lime prevented scurvy (Carpenter 1988, p 191). In this case, vitamin C is the mechanism through which limes prevent scurvy (Figure 4.9). Of course, identifying the mechanism is very important scientifically—lots of science is about understanding why things happen. Identifying mechanisms is very important practically. Once we understand why a treatment works, we can potentially develop new treatments that work even better.</p>

<p><img src="http://oaf2qt3yk.bkt.clouddn.com/4a6b19bb8d02275a273a866cc102bd5c.png" alt="fig4.9" /></p>

<p>Figure 4.9: Limes prevent scurvy and the mechanism is Vitamin C.</p>

<p>Unfortunately, isolating mechanisms is very difficult. Unlike limes and scurvy, in many social settings, treatments probably operate through many interrelated pathways, which makes isolation of mechanisms extremely difficult. However, in the case of social norms and energy use, researchers have tried to isolate mechanisms by collecting process data and testing related treatments.</p>

<p>One way to test possible mechanisms is by collecting process data about how the treatment impacted possible mechanisms. For example, recall that Allcott (2011) showed that Home Energy Reports caused people to lower their electricity usage. But, how do these reports lower electricity usage? What were the mechanisms? In a follow-up study, Allcott and Rogers (2014) partnered with a power company that, through a rebate program, had acquired information on which consumers upgraded their appliances to more energy efficient models. Allcott and Rogers (2014) found that slightly more people receiving the Home Energy Reports upgraded their appliances. But, this difference was so small that it could only account for 2% of the decrease in energy use in the treated households. In other words, appliance upgrades were not the dominant mechanism through which the Home Energy Report decreased electricity consumption.</p>

<p>A second way to study mechanisms is to run experiments with slightly different versions of the treatment. For example, in the experiment of Schultz et al. (2007) and all the subsequent Home Energy Report experiments, participants were provided with a treatment that has two main parts 1) tips about energy savings and 2) information about their energy use relative to their peers (Figure 4.6). Thus, it is possible that the energy saving tips are what caused the change, not the peer information. To assess the possibility that the tips alone might have been sufficient, Ferraro, Miranda, and Price (2011) partnered with a water company near Atlanta, GA, and ran a related experiment on water conservation involving about 100,000 households. There were four conditions:</p>

<ul>
<li>a group that received tips on saving water.</li>
<li>a group that received tips on saving water + a moral appeal to save water.</li>
<li>a group that received tips on saving water + a moral appeal to save water + information about their water use relative to their peers.</li>
<li>a control group.</li>
</ul>

<p>The researchers found that the tips only treatment had no effect on water usage in the short (one year), medium (two year), and long (three year) term. The tips + appeal treatment caused participants to decrease water usage, but only in the short-term. Finally, the tips + appeal + peer information treatment caused decreased usage in the short, medium, and long term (Figure 4.10). These kinds of experiments with unbundled treatments are a good way to figure out which part of the treatment—or which parts together—are the ones that are causing the effect (Gerber and Green 2012, Sec. 10.6). For example, the experiment of Ferraro and colleagues shows us that water saving tips alone are not enough to decrease water usage.</p>

<p><img src="http://oaf2qt3yk.bkt.clouddn.com/67e268bade13a3707d4ad62584b7bb7e.png" alt="fig4.10" /></p>

<p>Figure 4.10: Results from Ferraro, Miranda, and Price (2011). Treatments were sent May 21, 2007, and effects were measured during the summers of 2007, 2008, and 2009. By unbundling the treatment the researchers hoped to develop a better sense of the mechanisms. The tips only treatment had essentially no effect in the short (one year), medium (two years), and long (three years) term. The tips + appeal treatment caused participants to decrease water usage, but only in the short-term. The advice + appeal + peer information treatment caused participants to decrease water usage in the short, medium, and long term. Vertical bars are estimated confidence intervals. See Bernedo, Ferraro, and Price (2014) for actual study materials.</p>

<p>Ideally, one would move beyond the layering of components (tips; tips + appeal; tips + appeal + peer information) to a full factorial design—also sometimes called a $2^k$ factorial design—where each possible combination of the three elements is tested (Table 4.1). By testing every possible combination of components, researchers can fully assess the effect of each component in isolation and in combination. For example, the experiment of Ferraro and colleagues does not reveal whether peer comparison alone would have been sufficient to lead to long term changes in behavior. In the past, these full factorial designs have been difficult to run because they require a large number of participants and they require researchers to be able to precisely control and deliver a large number of treatments. But, the digital age removes these logistical constraints in some situations.</p>

<p>Table 4.1: Example of treatments in a full factorial design with 3 elements: tips, appeal, and peer information. The actual design of Ferraro, Miranda, and Price (2011) was a fractional factorial design that included three treatments: tips; tips + appeal; and tips + appeal + peer information (Figure 4.10).</p>

<p>|Treatment  |Characteristics|
|&ndash;|:&ndash;|
|1  |control|
|2| tips|
|3  |appeal|
|4  |peer information|
|5  |tips + appeal|
|6  |tips + peer information|
|7  |appeal + peer information|
|8  |tips + appeal + peer information|</p>

<p>In summary, mechanisms—the pathways through which a treatment has an effect—are incredibly important. Digital age experiments can help researchers learn about mechanisms by 1) collecting process data and 2) enabling full factorial designs. The mechanisms suggested by these approaches can then by tested directly by experiments specifically designed to test mechanisms (Ludwig, Kling, and Mullainathan 2011; Imai, Tingley, and Yamamoto 2013; Pirlott and MacKinnon 2016).</p>

<p>In total, these three concepts—validity; heterogeneity of treatment effects; and mechanisms—provide a powerful set of ideas for designing and interpreting experiments. These concepts help researchers move beyond simple experiments about what “works” to richer experiments that have tighter links to theory, that reveal where and why treatments work, and might even help researchers design more effective treatments. Given this conceptual background about experiments, I’ll now turn to how you can actually make your experiments happen.</p>

<h2 id="4-5-making-it-happen">4.5 Making it happen</h2>

<blockquote>
<p>Even if you don’t work at a big tech company you can run digital experiments. You can either do it yourself or partner with someone who can help you (and who you can help).</p>
</blockquote>

<p>By this point, I hope that you are excited about the possibilities of doing your own digital experiments. If you work at a big tech company you might already be doing these experiments all the time. But, if you don’t work at a tech company you might think that you can’t run digital experiments. Fortunately, that’s wrong; with a little creativity and hard work, everyone can run a digital experiment.</p>

<p>As a first step, it is helpful to distinguish between two main approaches: doing it yourself or partnering with the powerful. And, there are even a couple of different ways that you can do it yourself; you can experiment in existing environments, build your own experiment, or build your own product for repeated experimentation. I’ll illustrate these approaches with lots of examples below, and while you are learning about them you should notice how each approach offers trade-offs along four main dimensions: cost, control, realism, and ethics (Figure 4.11). No approach is the best in all situations.</p>

<p><img src="http://oaf2qt3yk.bkt.clouddn.com/962240be37a2a0748a7950178e3fde23.png" alt="fig4.11" /></p>

<p>Figure 4.11: Summary of trade-offs for different ways that you can make your experiment happen. By cost I mean cost to the researcher in terms of time and money. By control I mean the ability to do what you want in terms of recruiting participants, randomization, delivering treatments, and measuring outcomes. By realism I mean the extent to which the decision environment matches those encountered in everyday life; note that high realism is not always important for testing theories (Falk and Heckman 2009). By ethics I mean the ability of well-intentioned researchers to manage ethical challenges that might arise.</p>

<h3 id="4-5-1-just-do-it-yourself">4.5.1 Just do it yourself</h3>

<p>4.5.1.1 Use existing environments
4.5.1.2 Build your own experiment
4.5.1.3 Build your own product</p>

<h4 id="4-5-1-1-use-existing-environments">4.5.1.1 Use existing environments</h4>

<blockquote>
<p>You can run experiments inside existing environments, often without any coding or partnership.</p>
</blockquote>

<p>Logistically, the easiest way to do digital experiments is to overlay your experiment on top of an existing environment, enabling you to run a digital field experiment. These experiments can be run at a reasonably large scale and don’t require partnership with a company or extensive software development.</p>

<p>For example, Jennifer Doleac and Luke Stein (2013) took advantage of an online marketplace (e.g., craigslist) to run an experiment that measured racial discrimination. Doleac and Stein advertised thousands of iPods, and by systematically varying the characteristics of the seller, they were able to study the effect of race on economic transactions. Further, Doleac and Stein used the scale of their experiment to estimate when the effect is bigger (heterogeneity of treatment effects) and offer some ideas about why the effect might occur (mechanisms).</p>

<p>Prior to the study of Doleac and Stein, there had been two main approaches to experimentally measuring discrimination. In correspondence studies researchers create resumes of fictional people of different races and use these resumes to, for example, apply for different jobs. Bertrand and Mullainathan’s (2004) paper with the memorable title “Are Emily and Greg More Employable Than Lakisha and Jamal? A Field Experiment on Labor Market Discrimination” is a wonderful illustration of a correspondence study. Correspondence studies have relatively low cost per observation, which enables a single researcher to collect thousands of observations in a typical study. But, correspondence studies of racial discrimination have been questioned because names potentially signal many things in addition to the race of the applicant. That is, names such as Greg, Emily, Lakisha, and Jamal may signal social class in addition to race. Thus, any difference in treatment for resumes of Greg’s and Jamal’s might be due to more than presumed race differences of the applicants. Audit studies, on the other hand, involve hiring actors of different races to apply in person for jobs. Even though audit studies provide a clear signal of applicant race, they are extremely expensive per observation, which means that they typically only have hundreds of observations.</p>

<p>In their digital field experiment, Doleac and Stein were able to create an attractive hybrid. They were able to collect data at relatively low cost per observation—resulting in thousands of observations (as in a correspondence study)—and they were able to signal race using photographs—resulting in a clear uncounfounded signal of race (as in an audit study). Thus, the online environment sometimes enables researchers to create new treatments that have properties that are hard to construct otherwise.</p>

<p>The iPod advertisements of Doleac and Stein varied along three main dimensions. First, they varied the characteristics of the seller, which was signaled by the hand photographed holding the iPod <a href="Figure 4.12" target="_blank">white, black, white with tattoo</a>. Second, they varied the asking price [$90, $110, $130]. Third, they varied the quality of the ad text [high-quality and low-quality (e.g., cApitalization errors and spelin errors)]. Thus, the authors had a 3 X 3 X 2 design which was deployed across more than 300 local markets ranging from towns (e.g., Kokomo, IN and North Platte, NE) to mega-cities (e.g., New York and Los Angeles).</p>

<p><img src="http://oaf2qt3yk.bkt.clouddn.com/4206cc43876039218a5002242d34b20e.png" alt="4.12" /></p>

<p>Figure 4.12: Hands used in the experiment of Doleac and Stein (2013). iPods were sold by sellers with different characteristics to measure discrimination in an online marketplace.</p>

<p>Averaged across all conditions, the outcomes were better for the white seller than the black seller, with the tattooed seller having intermediate results. For example, white sellers received more offers and had higher final sale prices. Beyond these average effects, Doleac and Stein estimated the heterogeneity of effects. For example, one prediction from earlier theory is that discrimination would be less in markets that are more competitive. Using the number of offers received as a proxy for market competition, the authors found that black sellers do indeed receive worse offers in markets with a low degree of competition. Further, by comparing outcomes for the ads with high-quality and low-quality text, Doleac and Stein found that ad quality does not impact the disadvantage faced by black and tattooed sellers. Finally, taking advantage of the fact that advertisements were placed in more than 300 markets, the authors find that black sellers are more disadvantaged in cities with high crime rates and high residential segregation. None of these results give us a precise understanding of exactly why black sellers had worse outcomes, but, when combined with the results of other studies, they can begin to inform theories about the causes of racial discrimination in different types of economic transactions.</p>

<p>Another example that shows the ability of researchers to conduct digital field experiments in existing systems is the research by Arnout van de Rijt and colleagues (2014) on the keys to success. In many aspects of life, seemingly similar people end up with very different outcomes. One possible explanation for this pattern is that small—and essentially random—advantages can lock-in and grow over time, a process that researchers call cumulative advantage. In order to determine whether small initial successes lock-in or fade away, van de Rijt and colleagues (2014) intervened into four different systems bestowing success on randomly selected participants, and then measured the long-term impacts of this arbitrary success.</p>

<p>More specifically, van de Rijt and colleagues 1) pledged money to randomly selected projects on kickstarter.com, a crowdfunding website; 2) positively rated randomly selected reviews on the website epinions; 3) gave awards to randomly chosen contributors to Wikipedia; and 4) signed randomly selected petitions on change.org. The researchers found very similar results across all four systems: in each case, participants that were randomly given some early success went on to have more subsequent success than their otherwise completely indistinguishable peers (Figure 4.13). The fact that the same pattern appeared in many systems increases the external validity of these results because it reduces the chance that this pattern is an artifact of any particular system.</p>

<p><img src="http://oaf2qt3yk.bkt.clouddn.com/3fd0949079f480fd26586955866926bb.png" alt="4.13" /></p>

<p>Figure 4.13: Long-term effects of randomly bestowed success in four different social systems. Arnout van de Rijt and colleagues (2014) 1) pledged money to randomly selected projects on kickstarter.com, a crowdfunding website; 2) positively rated randomly selected reviews on the website epinions; 3) gave awards to randomly chosen contributors to Wikipedia; and 4) signed randomly selected petitions on change.org.</p>

<p>Together, these two examples show that researchers can conduct digital field experiments without the need to partner with companies or the need to build complex digital systems. Further, Table 4.2 provides even more examples that show the range of what is possible when researchers use the infrastructure of existing systems to deliver treatment and/or measure outcomes. These experiments are relatively cheap for researchers and they offer a high degree of realism. But, these experiments offer researchers limited control over the participants, treatments, and outcomes to be measured. Further, for experiments taking place in only one system, researchers need to be concerned that the effects could be driven by system-specific dynamics (e.g., the way that Kickstarter ranks projects or the way that change.org ranks petitions; for more information, see the discussion about algorithmic confounding in Chapter 2). Finally, when researchers intervene in working systems, tricky ethical questions emerge about possible harm to participants, non-participants, and systems. We will consider these ethical question in more detail in Chapter 6, and there is an excellent discussion of them in the appendix of van de Rijt (2014). The trade-offs that come with working in an existing system are not ideal for every project, and for that reason some researchers build their own experimental system, the topic of the next section.</p>

<p>Table 4.2: Examples of experiments in existing systems. These experiments seem to fall into three main categories, and this categorization may help you notice additional opportunities for your own research. First, there are experiments that involve selling or buying something (e.g., Doleac and Stein (2013)). Second, there are experiments that involve delivering a treatment to specific participants (e.g., Restivo and Rijt (2012)). Finally, there are experiments that involve delivering treatments to specific objects such as petitions (e.g., Vaillant et al. (2015)).</p>

<p>|Topic| Citation|
|&ndash;|:&ndash;|
|Effect of barnstars on contributions to Wikipedia| Restivo and Rijt (2012); Restivo and Rijt (2014); Rijt et al. (2014)|
|Effect of anti-harassment message on racist tweets|    Munger (2016)|
|Effect of auction method on sale price|    Lucking-Reiley (1999)|
|Effect of reputation on price in online auctions|  Resnick et al. (2006)|
|Effect of race of seller on sale of baseball cards on eBay |Ayres, Banaji, and Jolls (2015)|
|Effect of race of seller on sale of iPods  |Doleac and Stein (2013)|
|Effect of race of guest on Airbnb rentals  |Edelman, Luca, and Svirsky (2016)|
|Effect of donations on the success of projects on Kickstarter| Rijt et al. (2014)|
|Effect of race and ethnicity on housing rentals|   Hogan and Berry (2011)|
|Effect of positive rating on future ratings on epinions|   Rijt et al. (2014)|
|Effect of signatures on the success of petitions|  Vaillant et al. (2015); Rijt et al. (2014)|</p>

<h3 id="4-5-1-2-build-your-own-experiment">4.5.1.2 Build your own experiment</h3>

<blockquote>
<p>Building your own experiment might be costly, but it will enable you to create the experiment that you want.</p>
</blockquote>

<p>In addition to overlaying experiments on top of existing environments, you can also build your own experiment. The main advantage of this approach is control; if you are building the experiment, you can create the environment and treatments that you want. These bespoke experimental environments can create opportunities to test theories that are impossible to test in naturally occurring environments. The main drawbacks of building your own experiment are that it can be expensive and that the environment that you are able to create might not have the realism of a naturally occurring system. Researchers building their own experiment also must have a strategy for recruiting participants. When working in existing systems, researchers are essentially bringing the experiments to their participants. But, when researchers build their own experiment, they need to bring participants to it. Fortunately, services such as Amazon Mechanical Turk (MTurk) can provide researchers a convenient way to bring participants to their experiments.</p>

<p>One example that illustrates the virtues of bespoke environments for testing abstract theories is the digital lab experiment by Gregory Huber, Seth Hill, and Gabriel Lenz (2012). The experiment explores a possible practical limitation to the functioning of democratic governance. Earlier non-experimental studies of actual elections suggest that voters are not able to accurately assess the performance of incumbent politicians. In particular, voters appear to suffer from three biases: 1) focused on recent rather than cumulative performance; 2) manipulatable by rhetoric, framing, and marketing; and 3) influenced by events unrelated to incumbent performance, such as the success of local sports team and the weather. In these earlier studies, however, it was hard to isolate any of these factors from all the other stuff that happens in real, messy elections. Therefore, Huber and colleagues created a highly simplified voting environment in order to isolate, and then experimentally study, each of these three possible biases.</p>

<p>As I describe the experimental set-up below it is going to sound very artificial, but remember that realism is not a goal in lab-style experiments. Rather, the goal is to clearly isolate the process that you are trying to study, and this tight isolation is sometimes not possible in studies with more realism (Falk and Heckman 2009). Further, in this particular case, the researchers argued that if voters cannot effectively evaluate performance in this highly simplified setting, then they are not going to be able to do it in a more realistic, more complex setting.</p>

<p>Huber and colleagues used Amazon Mechanical Turk (MTurk) to recruit participants. Once a participant provided informed consent and passed a short test, she was told that she was participating in a 32 round game to earn tokens that could be converted into real money. At the beginning of the game, each participant was told that she had been assigned an “allocator” that would give her free tokens each round and that some allocators were more generous than others. Further, each participant was also told that she would have a chance to either keep her allocator or be assigned a new one after 16 rounds of the game. Given what you know about Huber and colleagues’ research goals, you can see that the allocator represents a government and this choice represents an election, but participants were not aware of the general goals of the research. In total, Huber and colleagues recruited about 4,000 participants who were paid about $1.25 for a task that took about 8 minutes.</p>

<p>Recall that one of the findings from earlier research was that voters reward and punish incumbents for outcomes that are clearly beyond their control, such as the success of local sports teams and the weather. To assess whether participants voting decisions could be influenced by purely random events in their setting, Huber and colleagues added a lottery to their experimental system. At either the 8th round or the 16th round (i.e., right before the chance to replace the allocator) participants were randomly placed in a lottery where some won 5000 points, some won 0 points, and some lost 5000 points. This lottery was intended to mimic good or bad news that is independent of the performance of the politician. Even though participants were explicitly told that the lottery was unrelated to the performance of their allocator, the outcome of the lottery still impacted participants’ decisions. Participants that benefited from the lottery were more likely to keep their allocator, and this effect was stronger when the lottery happened in round 16—right before the replacement decision—than when it happened in round 8 (Figure 4.14). These results, along with the results of several other experiments in the paper, led Huber and colleagues to conclude that even in a simplified setting, voters have difficulty making wise decisions, a result that impacted future research about voter decision making (Healy and Malhotra 2013). The experiment of Huber and colleagues shows that MTurk can be used to recruit participants for lab-style experiments to precisely test very specific theories. It also shows the value of building your own experimental environment: it is hard to imagine how these same processes could have been isolated so cleanly in any other setting.</p>

<p><img src="http://oaf2qt3yk.bkt.clouddn.com/60b16912591752506ed1748f279ba448.png" alt="fig4.14" /></p>

<p>Figure 4.14: Results from Huber, Hill, and Lenz (2012). Participants that benefited from the lottery were more likely to retain their allocator, and this effect was stronger when the lottery happened in round 16—right before the replacement decision—than when it happened in round 8.</p>

<p>In addition to building lab-like experiments, researchers can also build experiments that are more field-like. For example, Centola (2010) built a digital field experiment to study the effect of social network structure on the spread of behavior. His research question required him to observe the same behavior spreading in populations that had different social network structures but were otherwise indistinguishable. The only way to do this was with a bespoke, custom-built experiment. In this case, Centola built a web-based health community.</p>

<p>Centola recruited about 1,500 participants with advertising on health websites. When participants arrived at the online community—which was called the Healthy Lifestyle Network—they provided informed consent and then were assigned “health buddies.” Because of the way Centola assigned these health buddies he was able to knit together different social network structures in different groups. Some groups were built to have random networks (where everyone was equally likely to be connected) and other groups were built to have clustered networks (where connections are more locally dense). Then, Centola introduced a new behavior into each network, the chance to register for a new website with additional health information. Whenever anyone signed up for this new website, all of her health buddies received an email announcing this behavior. Centola found that this behavior—signing-up for the new website—spread further and faster in the clustered network than the random network, a finding that was contrary to some existing theories.</p>

<p>Overall, building your own experiment gives you much more control; it enables you to construct the best possible environment to isolate what you want to study. It is hard to imagine how either of these experiments could have been performed in an already existing environment. Further, building your own system decreases ethical concerns around experimenting in existing systems. When you build your own experiment, however, you run into many of the problems that are encountered in lab experiments: recruiting participants and concerns about realism. A final downside is that building your own experiment can be costly and time-consuming, although as these examples show, the experiments can range from relatively simple environments (such as the study of voting by Huber, Hill, and Lenz (2012)) to relatively complex environments (such as the study of networks and contagion by Centola (2010)).</p>

<h4 id="4-5-1-3-build-your-own-product">4.5.1.3 Build your own product</h4>

<blockquote>
<p>Building your own product is high-risk, high-reward. But, if it works, you can benefit from a positive feedback loop that enables distinctive research.</p>
</blockquote>

<p>Taking the approach of building your own experiment one step further, some researchers actually build their own products. These products attract participants, and then serve as platforms for experiments and other kinds of research. For example, a group of researchers at the University of Minnesota created MovieLens, which provides free, non-commercial personalized movie recommendations. MovieLens has operated continuously since 1997, and during this time 250,000 registered users have provided more than 20 million ratings about more than 30,000 movies (Harper and Konstan 2015). MovieLens has used the active community of users to conduct wonderful research ranging from testing social science theories about contributions to public goods (Beenen et al. 2004; Cosley et al. 2005; Chen et al. 2010; Ren et al. 2012) to addressing algorithmic challenges in recommendation systems (Rashid et al. 2002; Drenner et al. 2006; Harper, Sen, and Frankowski 2007; Ekstrand et al. 2015); for a full review see Harper and Konstan (2015). Many of these experiments would not have been possible without researchers having complete control over a real working product.</p>

<p>Unfortunately, building your own product is incredibly difficult, and you should think of it like creating a start-up company: high-risk, high-reward. If it is successful, this approach offers much of the control that comes from building your own experiment with the realism and participants that come from working in existing systems. Further, this approach is potentially able to create a positive feedback loop where more research leads to a better product which leads to more users which leads to more researchers and so on (Figure 4.15). In other words, once a positive feedback loop kicks in, research should get easier and easier. Despite the potential upside to this approach, I can’t find any other examples of success, which shows just how difficult it is to execute successfully. But, my hope is that this strategy will become more practical as technology improves. These difficulties with creating your own product mean that researchers who want to control a product are much more likely to partner with a company, the topic I’ll address next.</p>

<p><img src="http://oaf2qt3yk.bkt.clouddn.com/f23e89e2f607e76e4168659e5aca1246.png" alt="fig4.15" /></p>

<p>Figure 4.15: If you can successfully build your own product, you can benefit from a positive feedback loop: research leads to a better product which leads to more users which leads to even more research. These kinds of positive feedback loops are incredibly difficult to create, but they can enable research that would not be possible otherwise. MovieLens is an example of a research project that has succeed in creating a positive feedback loop (Harper and Konstan 2015).</p>

<h3 id="4-5-2-partner-with-the-powerful">4.5.2 Partner with the powerful</h3>

<p>Partnering can reduce costs and increase scale, but it can alter the kinds of participants, treatments, and outcomes that you can use.
The alternative to doing it yourself is partnering with a powerful organization such as a company, government, or NGO. The advantage of working with a partner is that they can enable you to run experiments that you just can’t do by yourself. For example, one of the experiments that I’ll tell you about below involved 61 million participants; no individual researcher could achieve that scale. At the same time that partnering increases what you can do, it also, simultaneously, constrains you. For example, most companies will not allow you to run an experiment that could harm their business or their reputation. Working with partners also means that when it comes time to publish, you may come under pressure to “re-frame” your results, and some partners might even try to block the publication of your work if it makes them look bad. Finally, partnering also comes with costs related to developing and maintaining these collaborations.</p>

<p>The core challenge that has to be solved to make these partnerships successful is finding a way to balance the interests of both parties, and a helpful way to think about that balance is Pasteur’s Quadrant (Stokes 1997). Many researchers think that if they are working on something practical—something that might be of interest to a partner—then they cannot be doing real science. This mindset will make it very difficult to create successful partnerships, and it also happens to be completely wrong. The problem with this way of thinking is wonderfully illustrated by the path-breaking research of biologist Louis Pasteur. While working on a commercial fermentation project to convert beet juice into alcohol, Pasteur discovered a new class of microorganism that eventually led to the germ theory of disease. This discovery solved a very practical problem—it helped improve the process of fermentation—and it lead to a major scientific advance. Thus, rather than thinking about research with practical applications as being in conflict with true scientific research, it is better to think of these as two separate dimensions. Research can be motivated by use (or not) and research can seek fundamental understanding (or not). Critically, some research—like Pasteur’s—can be motivated by use and seeking fundamental understanding (Figure 4.16). Research in Pasteur’s Quadrant—research that inherently advances two goals—is ideal for collaborations between researchers and partners. Given that background, I’ll describe two experimental studies with partnerships: one with a company and one with an NGO.</p>

<p><img src="http://oaf2qt3yk.bkt.clouddn.com/a49ee82f2be8d2d24ab93dbcfe34562e.png" alt="fig4.16" /></p>

<p>Figure 4.16: Pasteur’s Quadrant (based on Fig 3.5 from Stokes (1997)). Rather than thinking of research as either “basic” or “applied” it is better to think of research as motivated by use (or not) and seeking fundamental understanding (or not). An example of research that both is motivated by use and seeks fundamental understanding is Pasteur’s work on converting beet juice into alcohol which lead to the germ theory of disease. This is the kind of work that is best suited for partnerships with the powerful. Examples of work motivated by use but that does not seek fundamental understanding come from Thomas Edison, and examples of work that is not motivated by use but which seeks understanding come from Niels Bohr. See Stokes (1997) for a more thorough discussion of this framework and each of these cases.</p>

<p>Large companies, particularly tech companies, have developed incredibly sophisticated infrastructure for running complex experiments. In the tech industry, these experiments are often called A/B tests (because they test the effectiveness of two treatments: A and B). These experiments are frequently run for things like increasing click-through rates on ads, but the same experimental infrastructure can also be used for research that advances scientific understanding. An example that illustrates the potential of this kind of research is a study conducted by a partnership between researchers at Facebook and the University of California, San Diego, on the effects of different messages on voter turnout (Bond et al. 2012).</p>

<p>On November 2, 2010—the day of the US congressional elections—all 61 million Facebook users who live in the US and are over 18 took part in the experiment about voting. Upon visiting Facebook, users were randomly assigned into one of three groups, which determined what banner (if any) was placed at the top of their News Feed (Figure 4.17):</p>

<ul>
<li>a control group.</li>
<li>an informational message about voting with a clickable “I Voted” button and a counter (info).</li>
<li>an informational message about voting with a clickable “I Voted” button and a counter + names and pictures of their friends who had already clicked the “I Voted” (info + social).</li>
</ul>

<p>Bond and colleagues studied two main outcomes: reported voting behavior and actual voting behavior. First, they found that people in the info + social group were about 2 percentage points more likely than people in the info group to click “I Voted” (about 20% vs 18%). Further, after the researchers merged their data with publicly available voting records for about 6 million people they found that people in the info + social group were 0.39 percentage points more likely to actually vote than people in the control condition and that people in the info group just as likely to vote as people in the control condition (Figure 4.17).</p>

<p><img src="http://oaf2qt3yk.bkt.clouddn.com/9969345f5b2085457dfe855371021a3c.png" alt="fig4.17" /></p>

<p>Figure 4.17: Results from a get-out-the-vote experiment on Facebook (Bond et al. 2012). Participants in the info group voted at the same rate as people in the control condition, but people in the info + social group voted at a slightly higher rate. Bars represent estimated 95% confidence intervals. Results in the graph include about 6 million participants for whom researchers could match to voting records.</p>

<p>This experiment shows that some online get-out-the-vote messages are more effective than others, and it shows that researcher’s estimate of the effectiveness of a treatment can depend on whether they study reported or actual behavior. This experiment unfortunately does not offer any clues about the mechanisms through which the social information—which some researchers have playfully called a “face pile”—increased voting. It could be that the social information increased the probability that someone noticed the banner or that it increased the probability that someone who noticed the banner actually voted or both. Thus, this experiment provides an interesting finding that further researcher will likely explore (see e.g., Bakshy, Eckles, et al. (2012)).</p>

<p>In addition to advancing the goals of the researchers, this experiment also advanced the goal of the partner organization (Facebook). If you change the behavior studied from voting to buying soap, then you can see that the study has the exact same structure as an experiment to measure the effect of online ads (see e.g., Lewis and Rao (2015)). These ad effectiveness studies frequently measure the effect of exposure to online ads—the treatments in Bond et al. (2012) are basically ads for voting—on offline behavior. Thus, this study could advance Facebook’s ability to study the effectiveness of online ads and could help Facebook convince potential advertisers that Facebook ads are effective.</p>

<p>Even though the interests of the researchers and partners were mostly aligned in this study, they were also partially in tension. In particular, the allocation of participants to the three conditions—control, info, and info + social—was tremendously imbalanced: 98% of the sample was assigned to info + social. This imbalanced allocation is inefficient statistically, and a much better allocation for the researchers would have have been <sup>1</sup>&frasl;<sub>3</sub> of the participants in each group. But, the imbalanced allocation happened because Facebook wanted everyone to receive the info + social treatment. Fortunately, the researchers convinced them to hold back 1% for a related treatment and 1% of participants for a control group. Without the control group it would have been basically impossible to measure the effect of the info + social treatment because it would have been a “perturb and observe” experiment rather than a randomized controlled experiment. This example provides a valuable practical lesson for working with partners: sometimes you create an experiment by convincing someone to deliver a treatment and sometimes you create an experiment by convincing someone not to deliver a treatment (i.e., to create a control group).</p>

<p>Partnership does not always need to involve tech companies and A/B tests with millions of participants. For example, Alexander Coppock, Andrew Guess, and John Ternovski (2016) partnered with an environmental NGO (League of Conservation Voters) to run experiments testing different strategies for promoting social mobilization. The researchers used the NGO’s Twitter account to send out both public tweets and private direct messages that attempted to prime different types of identities. The researchers then measured which of these messages were most effective for encouraging people to sign a petition and retweet information about a petition.</p>

<p>Table 4.3: Examples of research that comes through partnership between researchers and organizations. In some cases, the researchers work at the organizations.
|Topic  |Citation|
|&ndash;|:&ndash;:|
|Effect of Facebook News Feed on information sharing|   Bakshy, Rosenn, et al. (2012)|
|Effect of partial anonymity on behavior on online dating website|  Bapna et al. (2016)|
|Effect of Home Energy Reports on electricity usage|    Allcott (2011); Allcott and Rogers (2014); Allcott (2015); Costa and Kahn (2013); Ayres, Raseman, and Shih (2013)|
|Effect of app design on viral spread   |Aral and Walker (2011)|
|Effect of spreading mechanism on diffusion|    Taylor, Bakshy, and Aral (2013)|
|Effect of social information in advertisements |Bakshy, Eckles, et al. (2012)|
|Effect of catalog frequency on sales through catalog and online for different types of customers   |Simester et al. (2009)|
|Effect of popularity information on potential job applications|    Gee (2015)|
|Effect of initial ratings on popularity|   Muchnik, Aral, and Taylor (2013)|
|Effect of message content on political mobilization|   Coppock, Guess, and Ternovski (2016)|</p>

<p>Overall, partnering with the powerful enables to you operate at a scale that is hard to do otherwise, and Table 4.3 provides other examples of partnerships between researchers and organizations. Partnering can be much easier than building your own experiment. But, these advantages come with disadvantages: partnerships can limit the kinds of participants, treatments, and outcomes that you can study. Further, these partnerships can lead to ethical challenges. The best way to spot an opportunity for a partnership is to notice a real problem that you can solve while you are doing interesting science. If you are not used to this way of looking at the world, it can be hard to spot problems in Pasteur’s Quadrant, but with practice, you’ll start to notice them more and more.</p>

<h2 id="4-6-advice">4.6 Advice</h2>

<p>Whether you are doing it yourself or working with a partner, I’d like to offer two pieces of advice that I’ve found particularly helpful in my own work. First, think as much as possible before any data has been collected. This advice probably seems obvious to researchers accustomed to running experiments, but it is very important for researchers accustomed to working with big data sources (see Chapter 2). With big data sources most of the work happens after you have the data, but experiments are the opposite; most of the work should happen before you collect data. One of the best ways to force yourself to think carefully about your design and analysis is to create and register an analysis plan for your experiment. Fortunately, many of the best-practices for the analysis of experimental data have been formalized into reporting guidelines, and these guidelines are a great place to start when creating your analysis plan (Schulz et al. 2010; Gerber et al. 2014; Simmons, Nelson, and Simonsohn 2011).</p>

<p>The second piece of advice is that no one experiment is going to be perfect, and because of that, you should try to design a series of experiments that reinforce each other. I’ve even heard this described as the armada strategy; rather than trying to build one massive battleship, you might be better building lots of smaller ships with complementary strengths. These kinds of multi-experiment studies are routine in psychology, but they are rare elsewhere. Fortunately, the low cost of some digital experiments makes these kind of multi-experiment studies easier.</p>

<p>Also, I’d like to offer two pieces of advice that are less common now but are particularly important for designing digital age experiments: create zero marginal cost data and build ethics into your design.</p>

<h3 id="4-6-1-create-zero-variable-cost-data">4.6.1 Create zero variable cost data</h3>

<p>The key to running large experiments is driving your variable cost to zero. The best ways to do this are automation and designing enjoyable experiments.
Digital experiments can have dramatically different cost structures and this enables researchers to run experiments that were impossible in the past. More specifically, experiments generally have two main types of costs: fixed costs and variable costs. Fixed costs are costs that don’t change depending on how many participants you have. For example, in a lab experiment, fixed costs might be the cost of renting the space and buying furniture. Variable costs, on the other hand, change depending on how many participants you have. For example, in a lab experiment, variable costs might come from paying staff and participants. In general, analog experiments have low fixed costs and high variable costs, and digital experiments have high fixed costs and low variable costs (Figure 4.18). With appropriate design, you can drive the variable cost of your experiment all the way to zero, and this can create exciting research opportunities.</p>

<p><img src="http://oaf2qt3yk.bkt.clouddn.com/0beabea46f340524c6cdc5b0121035c9.png" alt="fig4.18" /></p>

<p>Figure 4.18: Schematic of cost structures in analog and digital experiments. In general, analog experiments have low fixed costs and high variable costs whereas digital experiments have high fixed costs and low variable costs. The different cost structures mean that digital experiments can run at a scale that is not possible with analog experiments.</p>

<p>There are two main elements of variable cost—payments to staff and payments to participants—and each of these can be driven to zero using different strategies. Payments to staff stem from the work that research assistants do recruiting participants, delivering treatments, and measuring outcomes. For example, the analog field experiment of Schultz and colleagues (2007) on social norms and electricity usage required research assistants to travel to each home to deliver the treatment and read the electric meter (Figure 4.3). All of this effort by research assistants meant that adding a new household to the study would have added to the cost. On the other hand, for the digital field experiment of Restivo and van de Rijt (2012) on rewards in Wikipedia, researchers could add more participants at virtually no cost. A general strategy for reducing variable administrative costs is to replace human work (which is expensive) with computer work (which is cheap). Roughly, you can ask yourself: can this experiment run while everyone on my research team is sleeping? If the answer is yes, you’ve done a great job of automation.</p>

<p>The second main type of variable cost is payments to participants. Some researchers have used Amazon Mechanical Turk and other online labor markets to decrease the payments that are needed for participants. To drive variable costs all the way to zero, however, a different approach is needed. For a long time, researchers have designed experiments that are so boring they have to pay people to participate. But, what if you could create an experiment that people want to be in? This may sound far fetched, but I’ll give you an example below from my own work, and there are more examples in Table 4.4. Note that this approach to designing enjoyable experiments echoes some of the themes in Chapter 3 regarding designing more enjoyable surveys and in Chapter 5 regarding the design of mass collaboration. Thus, I think that participant enjoyment—what might also be called user experience—will be an increasingly important part of research design in the digital age.</p>

<p>Table 4.4: Examples of experiments with zero variable cost that compensated participants with a valuable service or an enjoyable experience.</p>

<p>|Compensation|  Citation|
|&ndash;|:&ndash;|
|Website with health information|   Centola (2010)|
|Exercise program|  Centola (2011)|
|Free music |Salganik, Dodds, and Watts (2006); Salganik and Watts (2008); Salganik and Watts (2009b)|
|Fun game|  Kohli et al. (2012)|
|Movie recommendations  |Harper and Konstan (2015)|</p>

<p>If you want to create zero variable costs experiments you’ll want to ensure that everything is fully automated and that participants don’t require any payments. In order to show how this is possible, I’ll describe my dissertation research on the success and failure of cultural products. This example also shows that zero variable cost data is not just about doing things cheaper. Rather, it is about enabling experiments that would not be possible otherwise.</p>

<p>My dissertation was motivated by the puzzling nature of success for cultural products. Hit songs, best selling books, and blockbuster movies are much, much more successful than average. Because of this, the markets for these products are often called “winner-take-all” markets. Yet, at the same time, which particular song, book, or movie will become successful is incredibly unpredictable. The screenwriter William Goldman (1989) elegantly summed up lots of academic research by saying that, when it comes to predicting success, “nobody knows anything.” The unpredictability of winner-take-all markets made me wonder how much of success is a result of quality and how much is just luck. Or, expressed slightly differently, if we could create parallel worlds and have them all evolve independently, would the same songs become popular in each world? And, if not, what might be a mechanism that causes these differences?</p>

<p>In order to answer these questions, we—Peter Dodds, Duncan Watts (my dissertation advisor), and I—ran a series of online field experiments. In particular, we built a website called MusicLab where people could discover new music, and we used it for a series of experiments. We recruited participants by running banner ads on a teen-interest website (Figure 4.19) and through mentions in the media. Participants arriving at our website provided informed consent, completed a short background questionnaire, and were randomly assigned to one of two experimental conditions—independent and social influence. In the independent condition, participants made decisions about which songs to listen to, given only the names of the bands and the songs. While listening to a song, participants were asked to rate it after which they had the opportunity (but not the obligation) to download the song. In the social influence condition, participants had the same experience, except they could also see how many times each song had been downloaded by previous participants. Furthermore, participants in the social influence condition were randomly assigned to one of eight parallel worlds each of which evolved independently (Figure 4.20). Using this design, we ran two related experiments. In the first, we presented participants the songs in an unsorted grid, which provided them a weak signal of popularity. In the second experiment, we presented the songs in a ranked list, which provided a much stronger signal of popularity (Figure 4.21).</p>

<p><img src="http://oaf2qt3yk.bkt.clouddn.com/5f71d72d7ae7e652ad7afd0734cebc0b.png" alt="" /></p>

<p>Figure 4.19: An example of banner ad that my colleagues and I used to recruit participants for the MusicLab experiments (Salganik, Dodds, and Watts 2006).</p>

<p><img src="http://oaf2qt3yk.bkt.clouddn.com/3771e0db83e0a618bf25b63b43c23c44.png" alt="4.20" /></p>

<p>Figure 4.20: Experimental design for the MusicLab experiments (Salganik, Dodds, and Watts 2006). Participants were randomly assigned into one of two conditions: independent and social influence. Participants in the independent condition made their choices without any information about what other people had done. Participants in the social influence condition were randomly assigned into one of eight parallel worlds, where they could see the popularity—as measured by downloads of previous participants—of each song in their world, but they could not see any information, nor did they even know about the existence of, any of the other worlds.</p>

<p>We found that the popularity of the songs differed across the worlds suggesting an important role of luck. For example, in one world the song “Lockdown” by 52Metro came in 1st, and in another world it came in 40th out of 48 songs. This was exactly the same song competing against all the same songs, but in one world it got lucky and in the others it did not. Further, by comparing results across the two experiments we found that social influence leads to more unequal success, which perhaps creates the appearance of predictability. But, looking across the worlds (which can’t be done outside of this kind of parallel worlds experiment), we found that social influence actually increased the unpredictability. Further, surprisingly, it was the songs of highest appeal that have the most unpredictable outcomes (Figure 4.22).</p>

<p><img src="http://oaf2qt3yk.bkt.clouddn.com/e94ce5d114bdd8c5211c9f999fa72e72.png" alt="fig4.21" /></p>

<p>Figure 4.21: Screenshots from the social influence conditions in the MusicLab experiments (Salganik, Dodds, and Watts 2006). In the social influence condition in experiment 1, the songs, along with the number of previous downloads, were presented to the participants arranged in a 16 X 3 rectangular grid, where the positions of the songs were randomly assigned for each participant. In experiment 2, participants in the social influence condition were shown the songs, with download counts, presented in one column in descending order of current popularity.</p>

<p><img src="http://oaf2qt3yk.bkt.clouddn.com/886b5f6274254b6bd0d4bb6a4449fc1e.png" alt="fig4.22" /></p>

<p>Figure 4.22: Results from the MusicLab experiments showing the relationship between appeal and success (Salganik, Dodds, and Watts 2006). The x-axis is the market share of the song in the independent world, which serves as a measure of the appeal of the song, and the y-axis is the market share of the same song in the 8 social influence worlds, which serves as a measure of the success of the songs. We found that increasing the social influence that participants experienced—specifically, the change in layout from experiment 1 to experiment 2 (Figure 4.21)—caused success to become more unpredictable, especially for the highest appeal songs.</p>

<p>MusicLab was able to run at essentially zero variable cost because of the way that it was designed. First, everything was fully automated so it was able to run while I was sleeping. Second, the compensation was free music so there was no variable participant compensation cost. The use of music as compensation also illustrates how there is sometimes a trade-off between fixed costs and variable costs. Using music increased the fixed costs because I had to spend time securing permission from the bands and preparing reports for the bands about participants’ reaction to their music. But, in this case, increasing fixed costs in order to decrease variables costs was the right thing to do; that’s what enabled us to run an experiment that was about 100 times larger than a standard lab experiment.</p>

<p>Further, the MusicLab experiments show that zero variable cost does not have to be an end in itself; rather, it can be a means to running a new kind of experiment. Notice that we did not use all of our participants to run a standard social influence lab experiment 100 times. Instead, we did something different, which you could think of as switching from a psychological experiment to a sociological experiment (Hedström 2006). Rather than focusing on individual decision-making, we focused our experiment on popularity, a collective outcome. This switch to a collective outcome meant that we required about 700 participants to produce a single data point (there were 700 people in each of the parallel worlds). That scale was only possible because of the cost structure of the experiment. In general, if researchers want to study how collective outcomes arise from individual decisions, group experiments such as MusicLab are very exciting. In the past, they have been logistically difficult, but those difficulties are fading because of the possibility of zero variable cost data.</p>

<p>In addition to illustrating the benefits of zero variable cost data, the MusicLab experiments also show a challenge with this approach: high fixed costs. In my case, I was extremely lucky to be able to work with a talented web developer named Peter Hausel for about six months to construct the experiment. This was only possible because my advisor, Duncan Watts, had received a number of grants to support this kind of research. Technology has improved since we built MusicLab in 2004, and it would be much easier to build an experiment like this now. But, high fixed cost strategies are really only possible for researchers who can somehow cover those costs.</p>

<p>In conclusion, digital experiments can have dramatically different cost structures than analog experiments. If you want to run really large experiments, you should try to decrease your variable cost as much as possible and ideally all the way to 0. You can do this by automating the mechanics of your experiment (e.g., replacing human time with computer time) and designing experiments that people want to be in. Researchers who can design experiments with these features will be able to run new kinds of experiments that were not possible in the past.</p>

    </div>



  </div>


</article>

<div class="container">
  <nav>
  <ul class="pager">
    
    <li class="previous"><a href="https://chengjunwang.com/en/note/2017-07-02-health-care/"><span
      aria-hidden="true">&larr;</span> 学校医保</a></li>
    

    
  </ul>
</nav>

</div>

<div class="article-container">
  
<section id="comments">
  <div id="disqus_thread">
    <div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'ChengjunWang';
    var disqus_identifier = 'https:\/\/chengjunwang.com\/en\/note\/2017-8-15-bitbybit\/';
    var disqus_title = ' Bit by Bit: Social Research in the Digital Age';
    var disqus_url = 'https:\/\/chengjunwang.com\/en\/note\/2017-8-15-bitbybit\/';

    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
  </div>
</section>



</div>

<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2016 Cheng-Jun Wang &middot; 

      Powered by the <a href="https://github.com/gcushen/hugo-academic" target="_blank">Academic
      theme</a> for <a href="http://gohugo.io" target="_blank">Hugo</a> and <a href="http://github.com/" target="_blank">Github</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>

    <script src="//cdnjs.cloudflare.com/ajax/libs/gsap/1.18.4/TweenMax.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/gsap/latest/plugins/ScrollToPlugin.min.js"></script>
    <script src="/js/jquery-1.12.3.min.js"></script>
    <script src="/js/bootstrap.min.js"></script>
    <script src="/js/isotope.pkgd.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.1/imagesloaded.pkgd.min.js"></script>
    <script src="/js/hugo-academic.js"></script>
    

    

    
    
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/highlight.min.js"></script>

      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    
    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] ,
        processEscapes: true
      },
        TeX: {
           equationNumbers: { autoNumber: "AMS" }
        }
       });
    </script>
    <script async src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    

    

  </body>
</html>

