<!DOCTYPE html>
<html lang="en-us">
<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="theme" content="hugo-academic">
  <meta name="generator" content="Hugo 0.18.1" />
  <meta name="author" content="Cheng-Jun Wang">
  <meta name="description" content="Associate Professor">

  
  
  
    
  
  
    
    
    <link rel="stylesheet" href="/css/highlight.min.css">
    
  
  <link rel="stylesheet" href="/css/fontsgoogle.css">
  <link rel="stylesheet" href="/css/bootstrap.min.css">
  <link rel="stylesheet" href="/css/font-awesome.min.css">
  <link rel="stylesheet" href="/css/academicons.min.css">

  <link rel="stylesheet" href="/css/hugo-academic.css">
  

  <link rel="alternate" href="https://chengjunwang.com/index.xml" type="application/rss+xml" title="Academic">
  <link rel="feed" href="https://chengjunwang.com/index.xml" type="application/rss+xml" title="Academic">

  <link rel="icon" type="image/png" href="/img/icon.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/apple-touch-icon.png">

  <link rel="canonical" href="https://chengjunwang.com/en/post/en/2012-04-23-scraping-newyork-times-with-python/">

  

  <title>Scraping New York Times &amp; The Guardian using Python | Academic</title>

</head>
<body id="top" data-spy="scroll" data-target="#navbar-main" data-offset="71">

<nav class="navbar navbar-default navbar-fixed-top" id="navbar-main">
  <div class="container">

    
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse"
              data-target=".navbar-collapse" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="/">Academic</a>
    </div>

    
    <div class="collapse navbar-collapse">

      
      <ul class="nav navbar-nav navbar-right">
        

        

        <li class="nav-item">
          <a href="/en/#about">
            
            <span>Home</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/en/#publications_selected">
            
            <span>Publications</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/en/#projects">
            
            <span>Projects</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/en/#posts">
            
            <span>News</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/en/#teaching">
            
            <span>Teaching</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/en/#contact">
            
            <span>Contact</span>
          </a>
        </li>

        
        

        

        <li class="nav-item">
          <a href="/zh">
            
            <span>中文</span>
          </a>
        </li>

        
        
      </ul>

    </div>
  </div>
</nav>


<article class="article" itemscope itemtype="http://schema.org/Article">



  


  

<div class="col-sm-2 col-sm-offset-1 doc-sidebar">
	<div id="sidebar">
	<div class="sidebar-module">
		<div class="sidebar-toc">
			<h4 class="sidebar-heading">Table of Contents</h4>
			
			
		</div>
	</div>
	
	</div>

</div>



  <div class="article-container">


    <h1 itemprop="name">Scraping New York Times &amp; The Guardian using Python</h1>
    

<div class="article-metadata">

  <span class="article-date">
    <time datetime="0001-01-01 00:00:00 &#43;0000 UTC" itemprop="datePublished">
      Mon, Jan 1, 0001
    </time>
  </span>

  
  
  
  <span class="article-categories">
    <i class="fa fa-folder"></i>
    
    <a href="/en/categories/python">python</a
    >
    
  </span>
  
  

  
  
  
  <span class="article-tags">
    <i class="fa fa-tags"></i>
    
    <a href="/en/tags/news-media">news media</a
    >
    
  </span>
  
  

  
  
<div class="share-box" aria-hidden="true">
  <ul class="share">
    <li>
      <a class="facebook"
         href="https://www.facebook.com/sharer.php?u=https%3a%2f%2fchengjunwang.com%2fen%2fpost%2fen%2f2012-04-23-scraping-newyork-times-with-python%2f"
         target="_blank">
        <i class="fa fa-facebook"></i>
      </a>
    </li>
    <li>
      <a class="twitter"
         href="https://twitter.com/intent/tweet?text=Scraping%20New%20York%20Times%20%26%20The%20Guardian%20using%20Python&amp;url=https%3a%2f%2fchengjunwang.com%2fen%2fpost%2fen%2f2012-04-23-scraping-newyork-times-with-python%2f"
         target="_blank">
        <i class="fa fa-twitter"></i>
      </a>
    </li>
    <li>
      <a class="linkedin"
         href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fchengjunwang.com%2fen%2fpost%2fen%2f2012-04-23-scraping-newyork-times-with-python%2f&amp;title=Scraping%20New%20York%20Times%20%26%20The%20Guardian%20using%20Python"
         target="_blank">
        <i class="fa fa-linkedin"></i>
      </a>
    </li>
    <li>
      <a class="weibo"
         href="http://service.weibo.com/share/share.php?url=https%3a%2f%2fchengjunwang.com%2fen%2fpost%2fen%2f2012-04-23-scraping-newyork-times-with-python%2f&amp;title=Scraping%20New%20York%20Times%20%26%20The%20Guardian%20using%20Python"
         target="_blank">
        <i class="fa fa-weibo"></i>
      </a>
    </li>
    <li>
      <a class="email"
         href="mailto:?subject=Scraping%20New%20York%20Times%20%26%20The%20Guardian%20using%20Python&amp;body=https%3a%2f%2fchengjunwang.com%2fen%2fpost%2fen%2f2012-04-23-scraping-newyork-times-with-python%2f">
        <i class="fa fa-envelope"></i>
      </a>
    </li>
  </ul>
</div>


  

</div>


    <div class="article-style" itemprop="articleBody">
      <p>I have read the blog post about Scraping New York Times Articles with R. It’s great. I want to reproduce the work with python.
First, we should learn about nytimes article search api.</p>

<p><a href="http://developer.nytimes.com/docs/article_search_api/" target="_blank">http://developer.nytimes.com/docs/article_search_api/</a></p>

<p>Second, we need to register and get the key which will be used in python script.</p>

<p><a href="http://developer.nytimes.com/apps/register" target="_blank">http://developer.nytimes.com/apps/register</a></p>

<pre><code># !/usr/bin/env python
# -*- coding: UTF-8  -*-
# Scraping New York Times using python
# 20120421@ Canberra
# chengjun wang

import json
import urllib2

'''
About the api and the key, see the links above.
'''

'''step 1: input query information'''
apiUrl='http://api.nytimes.com/svc/search/v1/article?format=json'
query='query=occupy+wall+street'                            # set the query word here
apiDate='begin_date=20110901&amp;end_date=20120214'             # set the date here
fields='fields=body%2Curl%2Ctitle%2Cdate%2Cdes_facet%2Cdesk_facet%2Cbyline'
offset='offset=0'
key='api-key=c2c5b91680.......2811165'  # input your key here

'''step 2: get the number of offset/pages'''
link=[apiUrl, query, apiDate, fields, offset, key]
ReqUrl='&amp;'.join(link)
jstr = urllib2.urlopen(ReqUrl).read()  # t = jstr.strip('()')
ts = json.loads( jstr )
number=ts['total'] #  the number of queries  # query=ts['tokens'] # result=ts['results']
print number
seq=range(number/9)  # this is not a good way
print seq

'''step 3: crawl the data and dump into csv'''
import csv
addressForSavingData= &quot;D:/Research/Dropbox/tweets/wapor_assessing online opinion/News coverage of ows/nyt.csv&quot;
file = open(addressForSavingData,'wb') # save to csv file
for i in seq:
    nums=str(i)
    offsets=''.join(['offset=', nums]) # I made error here, and print is a good way to test
    links=[apiUrl, query, apiDate, fields, offsets, key]
    ReqUrls='&amp;'.join(links)
    print &quot;*_____________*&quot;, ReqUrls
    jstrs = urllib2.urlopen(ReqUrls).read()
    t = jstrs.strip('()')
    tss= json.loads( t )  # error no joson object
    result = tss['results']
    for ob in result:
        title=ob['title']  # body=ob['body']   # body,url,title,date,des_facet,desk_facet,byline
        print title
        url=ob['url']
        date=ob['date'] # desk_facet=ob['desk_facet']  # byline=ob['byline'] # some author names don't exist
        w = csv.writer(file,delimiter=',',quotechar='|', quoting=csv.QUOTE_MINIMAL)
        w.writerow((date, title, url)) # write it out
file.close()
pass
</code></pre>

<p>see the result:</p>

<p><img src="http://weblab.com.cityu.edu.hk/blog/chengjun/files/2012/04/nyt1.png" alt="" /></p>

<p>Similarly, you can crawl the article data from The Guardian. See the link below.</p>

<p><a href="http://explorer.content.guardianapis.com/#/?format=json&amp;order-by=newest" target="_blank">http://explorer.content.guardianapis.com/#/?format=json&amp;order-by=newest</a></p>

<p>After you have registered you app and got the key, we can work on the python script.</p>

<pre><code># !/usr/bin/env python
# -*- coding: UTF-8 -*-
# Scraping The Guardian using Python
# 20120421@ Canberra
# chengjun wang

import json
import urllib2

'''

http://content.guardianapis.com/search?q=occupy+wall+street&amp;from-date=2011-09-01&amp;to-date=2012-02-14&amp;page=2

&amp;page-size=3&amp;format=json&amp;show-fields=all&amp;use-date=newspaper-edition&amp;api-key=m....g33gzq
'''

'''step 1: input query information'''
apiUrl='http://content.guardianapis.com/search?q=occupy+wall+street' # set the query word here
apiDate='from-date=2011-09-01&amp;to-date=2011-10-14'           # set the date here
apiPage='page=2'   # set the page
apiNum=10       # set the number of articles in one page
apiPageSize=''.join(['page-size=',str(apiNum)])
fields='format=json&amp;show-fields=all&amp;use-date=newspaper-edition'
key='api-key=mudfuj...g33gzq' # input your key here

'''step 2: get the number of offset/pages'''
link=[apiUrl, apiDate, apiPage, apiPageSize, fields, key]
ReqUrl='&amp;'.join(link)
jstr = urllib2.urlopen(ReqUrl).read() # t = jstr.strip('()')
ts = json.loads( jstr )
number=ts['response']['total'] # the number of queries # query=ts['tokens'] # result=ts['results']
print number
seq=range(number/(apiNum-1)) # this is not a good way
print seq

'''step 3: crawl the data and dump into csv'''
import csv
addressForSavingData= &quot;D:/Research/Dropbox/tweets/wapor_assessing online opinion/News coverage of ows/guardian.csv&quot;
file = open(addressForSavingData,'wb') # save to csv file
for i in seq:
    nums=str(i+1)
    apiPages=''.join(['page=', nums]) # I made error here, and print is a good way to test
    links= [apiUrl, apiDate, apiPages, apiPageSize, fields, key]
    ReqUrls='&amp;'.join(links)
    print &quot;*_____________*&quot;, ReqUrls
    jstrs = urllib2.urlopen(ReqUrls).read()
    t = jstrs.strip('()')
    tss= json.loads( t )
    result = tss['response']['results']
    for ob in result:
        title=ob['webTitle'].encode('utf-8') # body=ob['body']  # body,url,title,date,des_facet,desk_facet,byline
        print title
        section=ob[&quot;sectionName&quot;].encode('utf-8')
        url=ob['webUrl']
        date=ob['fields']['newspaperEditionDate'] # date=ob['webPublicationDate'] # byline=ob['fields']['byline']
        w = csv.writer(file,delimiter=',',quotechar='|', quoting=csv.QUOTE_MINIMAL)
        w.writerow((date, title, section, url)) # write it out
file.close()
pass
</code></pre>

    </div>

  </div>


</article>


<div class="container">
  <nav>
  <ul class="pager">
    
    <li class="previous"><a href="https://chengjunwang.com/en/post/en/2014-03-16-scraping-weibo-using-python/"><span
      aria-hidden="true">&larr;</span> Scraping data from Sina Weibo using Python</a></li>
    

    
    <li class="next"><a href="https://chengjunwang.com/en/post/en/2013-08-14-recoding-in-r/">Recoding variables in R <span
      aria-hidden="true">&rarr;</span></a></li>
    
  </ul>
</nav>

</div>

<div class="article-container">
  
<section id="comments">
  <div id="disqus_thread">
    <div id="disqus_thread"></div>
<script type="text/javascript">
    var disqus_shortname = 'ChengjunWang';
    var disqus_identifier = 'https:\/\/chengjunwang.com\/en\/post\/en\/2012-04-23-scraping-newyork-times-with-python\/';
    var disqus_title = 'Scraping New York Times \x26 The Guardian using Python';
    var disqus_url = 'https:\/\/chengjunwang.com\/en\/post\/en\/2012-04-23-scraping-newyork-times-with-python\/';

    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
  </div>
</section>



</div>



<footer class="site-footer">
  <div class="container">
    <p class="powered-by">

      &copy; 2016 Cheng-Jun Wang &middot; 

      Powered by the <a href="https://github.com/gcushen/hugo-academic" target="_blank">Academic
      theme</a> for <a href="http://gohugo.io" target="_blank">Hugo</a> and <a href="http://github.com/" target="_blank">Github</a>.

      <span class="pull-right" aria-hidden="true">
        <a href="#" id="back_to_top">
          <span class="button_icon">
            <i class="fa fa-chevron-up fa-2x"></i>
          </span>
        </a>
      </span>

    </p>
  </div>
</footer>

    <script src="//cdnjs.cloudflare.com/ajax/libs/gsap/1.18.4/TweenMax.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/gsap/latest/plugins/ScrollToPlugin.min.js"></script>
    <script src="/js/jquery-1.12.3.min.js"></script>
    <script src="/js/bootstrap.min.js"></script>
    <script src="/js/isotope.pkgd.min.js"></script>
    <script src="//cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.1/imagesloaded.pkgd.min.js"></script>
    <script src="/js/hugo-academic.js"></script>
    

    

    
    
      
      <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.9.0/highlight.min.js"></script>

      

      

      <script>hljs.initHighlightingOnLoad();</script>
    

    
    

    

  </body>
</html>

